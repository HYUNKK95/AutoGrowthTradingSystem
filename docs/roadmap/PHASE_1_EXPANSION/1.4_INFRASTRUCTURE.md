# ğŸ—ï¸ Phase 1.4: ì¸í”„ë¼ í™•ì¥ ë° ìŠ¤ì¼€ì¼ë§

## ğŸ“‹ **ê°œìš”**

### ğŸ¯ **ëª©í‘œ**
- **ìˆ˜í‰ì  í™•ì¥**: ìë™ ìŠ¤ì¼€ì¼ë§ ë° ë¡œë“œ ë°¸ëŸ°ì‹±
- **ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”**: ì½ê¸°/ì“°ê¸° ë¶„ë¦¬ ë° ìƒ¤ë”©
- **ìºì‹± ì „ëµ**: ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œ êµ¬ì¶•
- **CDN ë° ì—£ì§€ ì»´í“¨íŒ…**: ê¸€ë¡œë²Œ ì„±ëŠ¥ ìµœì í™”

### ğŸ“Š **ì„±ëŠ¥ ëª©í‘œ**
- **ì²˜ë¦¬ëŸ‰**: 10,000 TPS (ê±°ë˜/ì´ˆ)
- **ì‘ë‹µ ì‹œê°„**: < 50ms (95th percentile)
- **ê°€ìš©ì„±**: 99.99% (ì—°ê°„ ë‹¤ìš´íƒ€ì„ < 1ì‹œê°„)
- **í™•ì¥ì„±**: 10ë°° íŠ¸ë˜í”½ ì¦ê°€ ì‹œ ìë™ ìŠ¤ì¼€ì¼ë§

## ğŸ—ï¸ **ì¸í”„ë¼ í™•ì¥ ì•„í‚¤í…ì²˜**

### ğŸ“ **ì¸í”„ë¼ í™•ì¥ êµ¬ì¡°**
```
infrastructure/
â”œâ”€â”€ scaling/                        # ìŠ¤ì¼€ì¼ë§ êµ¬ì„±
â”‚   â”œâ”€â”€ auto_scaling/              # ìë™ ìŠ¤ì¼€ì¼ë§
â”‚   â”œâ”€â”€ load_balancing/            # ë¡œë“œ ë°¸ëŸ°ì‹±
â”‚   â””â”€â”€ capacity_planning/         # ìš©ëŸ‰ ê³„íš
â”œâ”€â”€ database/                      # ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
â”‚   â”œâ”€â”€ read_replicas/             # ì½ê¸° ë³µì œë³¸
â”‚   â”œâ”€â”€ sharding/                  # ìƒ¤ë”© ì „ëµ
â”‚   â””â”€â”€ optimization/              # ì„±ëŠ¥ ìµœì í™”
â”œâ”€â”€ caching/                       # ìºì‹± ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ multi_layer/               # ë‹¤ì¸µ ìºì‹±
â”‚   â”œâ”€â”€ distributed/               # ë¶„ì‚° ìºì‹œ
â”‚   â””â”€â”€ invalidation/              # ìºì‹œ ë¬´íš¨í™”
â””â”€â”€ cdn/                          # CDN ë° ì—£ì§€
    â”œâ”€â”€ edge_computing/            # ì—£ì§€ ì»´í“¨íŒ…
    â”œâ”€â”€ global_distribution/       # ê¸€ë¡œë²Œ ë¶„ì‚°
    â””â”€â”€ performance_optimization/  # ì„±ëŠ¥ ìµœì í™”
```

## ğŸ”„ **ìë™ ìŠ¤ì¼€ì¼ë§ ì‹œìŠ¤í…œ**

### ğŸ“¦ **Auto Scaling êµ¬ì„±**

```yaml
# infrastructure/scaling/auto_scaling/autoscaling.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: trading-app-hpa
  namespace: trading-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: trading-app
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        name: trading-ingress
      target:
        type: Value
        value: 1000
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 5
        periodSeconds: 60
```

### ğŸ”§ **ìŠ¤ì¼€ì¼ë§ ë§¤ë‹ˆì €**

```python
# infrastructure/scaling/auto_scaling/scaling_manager.py
import asyncio
import time
from typing import Dict, List, Optional, Any
from decimal import Decimal
from datetime import datetime, timedelta
from dataclasses import dataclass
import kubernetes
from kubernetes import client, config

@dataclass
class ScalingMetrics:
    """ìŠ¤ì¼€ì¼ë§ ë©”íŠ¸ë¦­"""
    cpu_usage: float
    memory_usage: float
    request_rate: float
    response_time: float
    error_rate: float
    timestamp: datetime

@dataclass
class ScalingDecision:
    """ìŠ¤ì¼€ì¼ë§ ê²°ì •"""
    action: str  # 'scale_up', 'scale_down', 'maintain'
    reason: str
    current_replicas: int
    target_replicas: int
    metrics: ScalingMetrics
    timestamp: datetime

class AutoScalingManager:
    """ìë™ ìŠ¤ì¼€ì¼ë§ ê´€ë¦¬ì"""
    
    def __init__(self, namespace: str = "trading-system"):
        self.namespace = namespace
        self.scaling_history = []
        self.metrics_history = []
        self.is_running = False
        
        # Kubernetes í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        try:
            config.load_incluster_config()
        except:
            config.load_kube_config()
        
        self.v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.autoscaling_v2 = client.AutoscalingV2Api()
        
        # ìŠ¤ì¼€ì¼ë§ ì„ê³„ê°’
        self.scaling_thresholds = {
            'cpu_high': 80.0,
            'cpu_low': 30.0,
            'memory_high': 85.0,
            'memory_low': 40.0,
            'request_rate_high': 1000,
            'request_rate_low': 100,
            'response_time_high': 200,
            'error_rate_high': 5.0
        }
        
        logger.info("Initialized auto scaling manager")
    
    async def start_monitoring(self):
        """ìŠ¤ì¼€ì¼ë§ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.is_running = True
        
        while self.is_running:
            try:
                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics = await self._collect_scaling_metrics()
                self.metrics_history.append(metrics)
                
                # ìŠ¤ì¼€ì¼ë§ ê²°ì •
                decision = await self._make_scaling_decision(metrics)
                
                if decision:
                    # ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰
                    await self._execute_scaling_decision(decision)
                    self.scaling_history.append(decision)
                
                # íˆìŠ¤í† ë¦¬ ì •ë¦¬
                await self._cleanup_history()
                
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ í™•ì¸
                
            except Exception as e:
                logger.error(f"Error in scaling monitoring: {e}")
                await asyncio.sleep(60)
    
    async def _collect_scaling_metrics(self) -> ScalingMetrics:
        """ìŠ¤ì¼€ì¼ë§ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # Kubernetes ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            pods = self.v1.list_namespaced_pod(
                namespace=self.namespace,
                label_selector="app=trading-app"
            )
            
            total_cpu = 0.0
            total_memory = 0.0
            pod_count = len(pods.items)
            
            for pod in pods.items:
                # Pod ë©”íŠ¸ë¦­ ì¡°íšŒ
                pod_metrics = self.v1.read_namespaced_pod(
                    name=pod.metadata.name,
                    namespace=self.namespace
                )
                
                # CPU ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
                if pod_metrics.status.container_statuses:
                    for container in pod_metrics.status.container_statuses:
                        if container.name == 'trading-app':
                            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë©”íŠ¸ë¦­ ì„œë²„ì—ì„œ ì¡°íšŒ
                            total_cpu += 50.0  # ì˜ˆì‹œ ê°’
                            total_memory += 60.0  # ì˜ˆì‹œ ê°’
            
            avg_cpu = total_cpu / pod_count if pod_count > 0 else 0.0
            avg_memory = total_memory / pod_count if pod_count > 0 else 0.0
            
            # ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ì¡°íšŒ
            request_rate = await self._get_request_rate()
            response_time = await self._get_average_response_time()
            error_rate = await self._get_error_rate()
            
            return ScalingMetrics(
                cpu_usage=avg_cpu,
                memory_usage=avg_memory,
                request_rate=request_rate,
                response_time=response_time,
                error_rate=error_rate,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"Failed to collect scaling metrics: {e}")
            return ScalingMetrics(
                cpu_usage=0.0,
                memory_usage=0.0,
                request_rate=0.0,
                response_time=0.0,
                error_rate=0.0,
                timestamp=datetime.now()
            )
    
    async def _make_scaling_decision(self, metrics: ScalingMetrics) -> Optional[ScalingDecision]:
        """ìŠ¤ì¼€ì¼ë§ ê²°ì •"""
        try:
            # í˜„ì¬ ë ˆí”Œë¦¬ì¹´ ìˆ˜ ì¡°íšŒ
            deployment = self.apps_v1.read_namespaced_deployment(
                name="trading-app",
                namespace=self.namespace
            )
            current_replicas = deployment.spec.replicas or 0
            
            # ìŠ¤ì¼€ì¼ë§ ì¡°ê±´ í™•ì¸
            should_scale_up = (
                metrics.cpu_usage > self.scaling_thresholds['cpu_high'] or
                metrics.memory_usage > self.scaling_thresholds['memory_high'] or
                metrics.request_rate > self.scaling_thresholds['request_rate_high'] or
                metrics.response_time > self.scaling_thresholds['response_time_high']
            )
            
            should_scale_down = (
                metrics.cpu_usage < self.scaling_thresholds['cpu_low'] and
                metrics.memory_usage < self.scaling_thresholds['memory_low'] and
                metrics.request_rate < self.scaling_thresholds['request_rate_low'] and
                current_replicas > 3  # ìµœì†Œ ë ˆí”Œë¦¬ì¹´ ìˆ˜
            )
            
            # ìŠ¤ì¼€ì¼ë§ ê²°ì •
            if should_scale_up:
                target_replicas = min(current_replicas + 2, 50)  # ìµœëŒ€ 50ê°œ
                reason = f"High load detected: CPU={metrics.cpu_usage:.1f}%, Memory={metrics.memory_usage:.1f}%"
                
                return ScalingDecision(
                    action='scale_up',
                    reason=reason,
                    current_replicas=current_replicas,
                    target_replicas=target_replicas,
                    metrics=metrics,
                    timestamp=datetime.now()
                )
                
            elif should_scale_down:
                target_replicas = max(current_replicas - 1, 3)  # ìµœì†Œ 3ê°œ
                reason = f"Low load detected: CPU={metrics.cpu_usage:.1f}%, Memory={metrics.memory_usage:.1f}%"
                
                return ScalingDecision(
                    action='scale_down',
                    reason=reason,
                    current_replicas=current_replicas,
                    target_replicas=target_replicas,
                    metrics=metrics,
                    timestamp=datetime.now()
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to make scaling decision: {e}")
            return None
    
    async def _execute_scaling_decision(self, decision: ScalingDecision):
        """ìŠ¤ì¼€ì¼ë§ ê²°ì • ì‹¤í–‰"""
        try:
            # ë°°í¬ ì—…ë°ì´íŠ¸
            deployment = self.apps_v1.read_namespaced_deployment(
                name="trading-app",
                namespace=self.namespace
            )
            
            deployment.spec.replicas = decision.target_replicas
            
            self.apps_v1.patch_namespaced_deployment(
                name="trading-app",
                namespace=self.namespace,
                body=deployment
            )
            
            logger.info(f"Scaling {decision.action}: {decision.current_replicas} -> {decision.target_replicas}")
            logger.info(f"Reason: {decision.reason}")
            
        except Exception as e:
            logger.error(f"Failed to execute scaling decision: {e}")
    
    async def _get_request_rate(self) -> float:
        """ìš”ì²­ë¥  ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë©”íŠ¸ë¦­ ì„œë²„ì—ì„œ ì¡°íšŒ
        return 500.0  # ì˜ˆì‹œ ê°’
    
    async def _get_average_response_time(self) -> float:
        """í‰ê·  ì‘ë‹µ ì‹œê°„ ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë©”íŠ¸ë¦­ ì„œë²„ì—ì„œ ì¡°íšŒ
        return 150.0  # ì˜ˆì‹œ ê°’
    
    async def _get_error_rate(self) -> float:
        """ì˜¤ë¥˜ë¥  ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë©”íŠ¸ë¦­ ì„œë²„ì—ì„œ ì¡°íšŒ
        return 2.0  # ì˜ˆì‹œ ê°’
    
    async def _cleanup_history(self):
        """íˆìŠ¤í† ë¦¬ ì •ë¦¬"""
        try:
            # 24ì‹œê°„ ì´ì „ ë°ì´í„° ì œê±°
            cutoff_time = datetime.now() - timedelta(hours=24)
            
            self.metrics_history = [
                m for m in self.metrics_history
                if m.timestamp > cutoff_time
            ]
            
            self.scaling_history = [
                s for s in self.scaling_history
                if s.timestamp > cutoff_time
            ]
            
        except Exception as e:
            logger.error(f"Failed to cleanup history: {e}")
    
    def get_scaling_summary(self) -> Dict[str, Any]:
        """ìŠ¤ì¼€ì¼ë§ ìš”ì•½"""
        try:
            recent_decisions = [
                d for d in self.scaling_history
                if d.timestamp > datetime.now() - timedelta(hours=1)
            ]
            
            scale_ups = sum(1 for d in recent_decisions if d.action == 'scale_up')
            scale_downs = sum(1 for d in recent_decisions if d.action == 'scale_down')
            
            return {
                'total_decisions_1h': len(recent_decisions),
                'scale_ups_1h': scale_ups,
                'scale_downs_1h': scale_downs,
                'current_replicas': self._get_current_replicas(),
                'scaling_thresholds': self.scaling_thresholds,
                'last_decision': recent_decisions[-1].__dict__ if recent_decisions else None
            }
            
        except Exception as e:
            logger.error(f"Failed to get scaling summary: {e}")
            return {}
    
    def _get_current_replicas(self) -> int:
        """í˜„ì¬ ë ˆí”Œë¦¬ì¹´ ìˆ˜ ì¡°íšŒ"""
        try:
            deployment = self.apps_v1.read_namespaced_deployment(
                name="trading-app",
                namespace=self.namespace
            )
            return deployment.spec.replicas or 0
        except:
            return 0
    
    def stop_monitoring(self):
        """ìŠ¤ì¼€ì¼ë§ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_running = False
        logger.info("Stopped scaling monitoring")
```

## ğŸ—„ï¸ **ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”**

### ğŸ“¦ **ì½ê¸° ë³µì œë³¸ ì„¤ì •**

```yaml
# infrastructure/database/read_replicas/postgres-replicas.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
  namespace: trading-system
data:
  POSTGRES_DB: trading
  POSTGRES_USER: trading_user
  POSTGRES_PASSWORD: trading_password
  MASTER_HOST: postgres-master
  MASTER_PORT: "5432"
  REPLICA_HOSTS: "postgres-replica-1,postgres-replica-2,postgres-replica-3"

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-master
  namespace: trading-system
spec:
  serviceName: postgres-master
  replicas: 1
  selector:
    matchLabels:
      app: postgres-master
  template:
    metadata:
      labels:
        app: postgres-master
    spec:
      containers:
      - name: postgres
        image: postgres:13
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: POSTGRES_DB
        - name: POSTGRES_USER
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: POSTGRES_USER
        - name: POSTGRES_PASSWORD
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: POSTGRES_PASSWORD
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Service
metadata:
  name: postgres-master
  namespace: trading-system
spec:
  selector:
    app: postgres-master
  ports:
  - port: 5432
    targetPort: 5432
  type: ClusterIP

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-replicas
  namespace: trading-system
spec:
  serviceName: postgres-replicas
  replicas: 3
  selector:
    matchLabels:
      app: postgres-replica
  template:
    metadata:
      labels:
        app: postgres-replica
    spec:
      containers:
      - name: postgres
        image: postgres:13
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: POSTGRES_DB
        - name: POSTGRES_USER
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: POSTGRES_USER
        - name: POSTGRES_PASSWORD
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: POSTGRES_PASSWORD
        - name: MASTER_HOST
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: MASTER_HOST
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Service
metadata:
  name: postgres-replicas
  namespace: trading-system
spec:
  selector:
    app: postgres-replica
  ports:
  - port: 5432
    targetPort: 5432
  type: ClusterIP
```

### ğŸ”§ **ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬ì**

```python
# infrastructure/database/read_replicas/db_manager.py
import asyncio
import random
from typing import Dict, List, Optional, Any
from contextlib import asynccontextmanager
import asyncpg
from dataclasses import dataclass

@dataclass
class DatabaseConnection:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´"""
    host: str
    port: int
    database: str
    user: str
    password: str
    is_master: bool
    is_healthy: bool = True
    last_health_check: Optional[datetime] = None

class DatabaseManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.master_connection = None
        self.replica_connections = []
        self.connection_pools = {}
        self.health_check_interval = 30  # 30ì´ˆ
        
        logger.info("Initialized database manager")
    
    async def initialize(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # ë§ˆìŠ¤í„° ì—°ê²° ì„¤ì •
            master_config = self.config['master']
            self.master_connection = DatabaseConnection(
                host=master_config['host'],
                port=master_config['port'],
                database=master_config['database'],
                user=master_config['user'],
                password=master_config['password'],
                is_master=True
            )
            
            # ë§ˆìŠ¤í„° ì—°ê²° í’€ ìƒì„±
            self.connection_pools['master'] = await asyncpg.create_pool(
                host=self.master_connection.host,
                port=self.master_connection.port,
                database=self.master_connection.database,
                user=self.master_connection.user,
                password=self.master_connection.password,
                min_size=5,
                max_size=20
            )
            
            # ë³µì œë³¸ ì—°ê²° ì„¤ì •
            for replica_config in self.config['replicas']:
                replica_connection = DatabaseConnection(
                    host=replica_config['host'],
                    port=replica_config['port'],
                    database=replica_config['database'],
                    user=replica_config['user'],
                    password=replica_config['password'],
                    is_master=False
                )
                
                self.replica_connections.append(replica_connection)
                
                # ë³µì œë³¸ ì—°ê²° í’€ ìƒì„±
                pool_name = f"replica_{replica_connection.host}"
                self.connection_pools[pool_name] = await asyncpg.create_pool(
                    host=replica_connection.host,
                    port=replica_connection.port,
                    database=replica_connection.database,
                    user=replica_connection.user,
                    password=replica_connection.password,
                    min_size=3,
                    max_size=10
                )
            
            logger.info(f"Initialized database manager with {len(self.replica_connections)} replicas")
            
        except Exception as e:
            logger.error(f"Failed to initialize database manager: {e}")
            raise
    
    @asynccontextmanager
    async def get_master_connection(self):
        """ë§ˆìŠ¤í„° ì—°ê²° íšë“"""
        if 'master' not in self.connection_pools:
            raise Exception("Master connection pool not available")
        
        async with self.connection_pools['master'].acquire() as conn:
            yield conn
    
    @asynccontextmanager
    async def get_replica_connection(self):
        """ë³µì œë³¸ ì—°ê²° íšë“ (ë¡œë“œ ë°¸ëŸ°ì‹±)"""
        healthy_replicas = [
            name for name, pool in self.connection_pools.items()
            if name != 'master' and self._is_pool_healthy(name)
        ]
        
        if not healthy_replicas:
            # ë³µì œë³¸ì´ ì—†ìœ¼ë©´ ë§ˆìŠ¤í„° ì‚¬ìš©
            async with self.get_master_connection() as conn:
                yield conn
            return
        
        # ëœë¤ ì„ íƒ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¡œë“œ ë°¸ëŸ°ì‹± ì‚¬ìš©)
        selected_replica = random.choice(healthy_replicas)
        
        async with self.connection_pools[selected_replica].acquire() as conn:
            yield conn
    
    async def execute_write(self, query: str, *args) -> Any:
        """ì“°ê¸° ì‘ì—… ì‹¤í–‰ (ë§ˆìŠ¤í„°)"""
        async with self.get_master_connection() as conn:
            return await conn.execute(query, *args)
    
    async def execute_read(self, query: str, *args) -> List[Any]:
        """ì½ê¸° ì‘ì—… ì‹¤í–‰ (ë³µì œë³¸)"""
        async with self.get_replica_connection() as conn:
            return await conn.fetch(query, *args)
    
    async def execute_transaction(self, queries: List[tuple]) -> List[Any]:
        """íŠ¸ëœì­ì…˜ ì‹¤í–‰ (ë§ˆìŠ¤í„°)"""
        async with self.get_master_connection() as conn:
            async with conn.transaction():
                results = []
                for query, args in queries:
                    result = await conn.execute(query, *args)
                    results.append(result)
                return results
    
    def _is_pool_healthy(self, pool_name: str) -> bool:
        """ì—°ê²° í’€ ê±´ê°• ìƒíƒœ í™•ì¸"""
        try:
            pool = self.connection_pools[pool_name]
            return not pool.is_closed()
        except:
            return False
    
    async def health_check(self):
        """ê±´ê°• ìƒíƒœ í™•ì¸"""
        try:
            # ë§ˆìŠ¤í„° ê±´ê°• ìƒíƒœ í™•ì¸
            async with self.get_master_connection() as conn:
                await conn.execute("SELECT 1")
                self.master_connection.is_healthy = True
                self.master_connection.last_health_check = datetime.now()
            
            # ë³µì œë³¸ ê±´ê°• ìƒíƒœ í™•ì¸
            for replica in self.replica_connections:
                pool_name = f"replica_{replica.host}"
                if pool_name in self.connection_pools:
                    try:
                        async with self.connection_pools[pool_name].acquire() as conn:
                            await conn.execute("SELECT 1")
                            replica.is_healthy = True
                            replica.last_health_check = datetime.now()
                    except Exception as e:
                        replica.is_healthy = False
                        logger.warning(f"Replica {replica.host} is unhealthy: {e}")
                        
        except Exception as e:
            logger.error(f"Database health check failed: {e}")
    
    async def get_database_stats(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        try:
            stats = {
                'master': {
                    'host': self.master_connection.host,
                    'is_healthy': self.master_connection.is_healthy,
                    'last_health_check': self.master_connection.last_health_check.isoformat() if self.master_connection.last_health_check else None
                },
                'replicas': [
                    {
                        'host': replica.host,
                        'is_healthy': replica.is_healthy,
                        'last_health_check': replica.last_health_check.isoformat() if replica.last_health_check else None
                    }
                    for replica in self.replica_connections
                ],
                'connection_pools': {
                    name: {
                        'size': pool.get_size(),
                        'free_size': pool.get_free_size(),
                        'is_closed': pool.is_closed()
                    }
                    for name, pool in self.connection_pools.items()
                }
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get database stats: {e}")
            return {}
    
    async def close(self):
        """ì—°ê²° í’€ ì¢…ë£Œ"""
        try:
            for pool in self.connection_pools.values():
                await pool.close()
            
            logger.info("Closed all database connection pools")
            
        except Exception as e:
            logger.error(f"Failed to close database pools: {e}")
```

## ğŸ—„ï¸ **ìºì‹± ì‹œìŠ¤í…œ**

### ğŸ“¦ **Redis í´ëŸ¬ìŠ¤í„° ì„¤ì •**

```yaml
# infrastructure/caching/redis-cluster.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-config
  namespace: trading-system
data:
  redis.conf: |
    cluster-enabled yes
    cluster-config-file nodes.conf
    cluster-node-timeout 5000
    appendonly yes
    appendfsync everysec
    maxmemory-policy allkeys-lru
    maxmemory 2gb

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
  namespace: trading-system
spec:
  serviceName: redis-cluster
  replicas: 6
  selector:
    matchLabels:
      app: redis-cluster
  template:
    metadata:
      labels:
        app: redis-cluster
    spec:
      containers:
      - name: redis
        image: redis:6-alpine
        ports:
        - containerPort: 6379
        - containerPort: 16379
        command:
        - redis-server
        - /etc/redis/redis.conf
        volumeMounts:
        - name: redis-config
          mountPath: /etc/redis
        - name: redis-data
          mountPath: /data
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: redis-config
        configMap:
          name: redis-config

---
apiVersion: v1
kind: Service
metadata:
  name: redis-cluster
  namespace: trading-system
spec:
  selector:
    app: redis-cluster
  ports:
  - port: 6379
    targetPort: 6379
    name: redis
  - port: 16379
    targetPort: 16379
    name: cluster
  type: ClusterIP
```

### ğŸ”§ **ë‹¤ì¸µ ìºì‹± ë§¤ë‹ˆì €**

```python
# infrastructure/caching/multi_layer/cache_manager.py
import asyncio
import json
import hashlib
from typing import Dict, List, Optional, Any, Union
from datetime import datetime, timedelta
import redis.asyncio as redis
from dataclasses import dataclass

@dataclass
class CacheConfig:
    """ìºì‹œ ì„¤ì •"""
    ttl: int = 300  # 5ë¶„
    max_size: int = 1000
    enable_compression: bool = True
    enable_encryption: bool = False

class MultiLayerCacheManager:
    """ë‹¤ì¸µ ìºì‹± ê´€ë¦¬ì"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.l1_cache = {}  # ë©”ëª¨ë¦¬ ìºì‹œ (L1)
        self.l2_cache = None  # Redis ìºì‹œ (L2)
        self.cache_stats = {
            'l1_hits': 0,
            'l1_misses': 0,
            'l2_hits': 0,
            'l2_misses': 0,
            'writes': 0
        }
        
        logger.info("Initialized multi-layer cache manager")
    
    async def initialize(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        try:
            # Redis ì—°ê²° ì„¤ì •
            self.l2_cache = redis.Redis(
                host=self.config['redis']['host'],
                port=self.config['redis']['port'],
                db=self.config['redis']['db'],
                decode_responses=True
            )
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            await self.l2_cache.ping()
            
            logger.info("Initialized L2 cache (Redis)")
            
        except Exception as e:
            logger.error(f"Failed to initialize L2 cache: {e}")
            self.l2_cache = None
    
    async def get(self, key: str, default: Any = None) -> Any:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        try:
            # L1 ìºì‹œ í™•ì¸
            if key in self.l1_cache:
                cache_entry = self.l1_cache[key]
                if not self._is_expired(cache_entry):
                    self.cache_stats['l1_hits'] += 1
                    return cache_entry['value']
                else:
                    # ë§Œë£Œëœ í•­ëª© ì œê±°
                    del self.l1_cache[key]
            
            self.cache_stats['l1_misses'] += 1
            
            # L2 ìºì‹œ í™•ì¸
            if self.l2_cache:
                try:
                    cached_value = await self.l2_cache.get(key)
                    if cached_value:
                        # L1 ìºì‹œì— ì €ì¥
                        self._set_l1_cache(key, json.loads(cached_value))
                        self.cache_stats['l2_hits'] += 1
                        return json.loads(cached_value)
                    else:
                        self.cache_stats['l2_misses'] += 1
                except Exception as e:
                    logger.error(f"L2 cache get error: {e}")
                    self.cache_stats['l2_misses'] += 1
            
            return default
            
        except Exception as e:
            logger.error(f"Cache get error: {e}")
            return default
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """ìºì‹œì— ê°’ ì €ì¥"""
        try:
            # TTL ì„¤ì •
            if ttl is None:
                ttl = self.config.get('default_ttl', 300)
            
            # L1 ìºì‹œì— ì €ì¥
            self._set_l1_cache(key, value, ttl)
            
            # L2 ìºì‹œì— ì €ì¥
            if self.l2_cache:
                try:
                    serialized_value = json.dumps(value)
                    await self.l2_cache.setex(key, ttl, serialized_value)
                except Exception as e:
                    logger.error(f"L2 cache set error: {e}")
            
            self.cache_stats['writes'] += 1
            return True
            
        except Exception as e:
            logger.error(f"Cache set error: {e}")
            return False
    
    async def delete(self, key: str) -> bool:
        """ìºì‹œì—ì„œ ê°’ ì‚­ì œ"""
        try:
            # L1 ìºì‹œì—ì„œ ì‚­ì œ
            if key in self.l1_cache:
                del self.l1_cache[key]
            
            # L2 ìºì‹œì—ì„œ ì‚­ì œ
            if self.l2_cache:
                try:
                    await self.l2_cache.delete(key)
                except Exception as e:
                    logger.error(f"L2 cache delete error: {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"Cache delete error: {e}")
            return False
    
    async def invalidate_pattern(self, pattern: str) -> int:
        """íŒ¨í„´ì— ë§ëŠ” ìºì‹œ ë¬´íš¨í™”"""
        try:
            deleted_count = 0
            
            # L1 ìºì‹œì—ì„œ íŒ¨í„´ ë§¤ì¹­ í•­ëª© ì‚­ì œ
            keys_to_delete = [
                key for key in self.l1_cache.keys()
                if self._matches_pattern(key, pattern)
            ]
            
            for key in keys_to_delete:
                del self.l1_cache[key]
                deleted_count += 1
            
            # L2 ìºì‹œì—ì„œ íŒ¨í„´ ë§¤ì¹­ í•­ëª© ì‚­ì œ
            if self.l2_cache:
                try:
                    keys = await self.l2_cache.keys(pattern)
                    if keys:
                        await self.l2_cache.delete(*keys)
                        deleted_count += len(keys)
                except Exception as e:
                    logger.error(f"L2 cache pattern delete error: {e}")
            
            return deleted_count
            
        except Exception as e:
            logger.error(f"Cache pattern invalidation error: {e}")
            return 0
    
    def _set_l1_cache(self, key: str, value: Any, ttl: int = 300):
        """L1 ìºì‹œì— ê°’ ì €ì¥"""
        # L1 ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.l1_cache) >= self.config.get('l1_max_size', 1000):
            # LRU ì •ì±…ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = min(self.l1_cache.keys(), key=lambda k: self.l1_cache[k]['timestamp'])
            del self.l1_cache[oldest_key]
        
        self.l1_cache[key] = {
            'value': value,
            'timestamp': datetime.now(),
            'ttl': ttl
        }
    
    def _is_expired(self, cache_entry: Dict[str, Any]) -> bool:
        """ìºì‹œ í•­ëª© ë§Œë£Œ í™•ì¸"""
        expiry_time = cache_entry['timestamp'] + timedelta(seconds=cache_entry['ttl'])
        return datetime.now() > expiry_time
    
    def _matches_pattern(self, key: str, pattern: str) -> bool:
        """íŒ¨í„´ ë§¤ì¹­ í™•ì¸"""
        # ê°„ë‹¨í•œ ì™€ì¼ë“œì¹´ë“œ ë§¤ì¹­
        if '*' in pattern:
            pattern_parts = pattern.split('*')
            key_parts = key.split('*')
            
            if len(pattern_parts) != len(key_parts):
                return False
            
            for p_part, k_part in zip(pattern_parts, key_parts):
                if p_part and p_part not in k_part:
                    return False
            
            return True
        else:
            return key == pattern
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        total_l1_requests = self.cache_stats['l1_hits'] + self.cache_stats['l1_misses']
        total_l2_requests = self.cache_stats['l2_hits'] + self.cache_stats['l2_misses']
        
        return {
            'l1_cache': {
                'size': len(self.l1_cache),
                'hits': self.cache_stats['l1_hits'],
                'misses': self.cache_stats['l1_misses'],
                'hit_rate': (self.cache_stats['l1_hits'] / total_l1_requests * 100) if total_l1_requests > 0 else 0
            },
            'l2_cache': {
                'hits': self.cache_stats['l2_hits'],
                'misses': self.cache_stats['l2_misses'],
                'hit_rate': (self.cache_stats['l2_hits'] / total_l2_requests * 100) if total_l2_requests > 0 else 0
            },
            'writes': self.cache_stats['writes'],
            'total_requests': total_l1_requests
        }
    
    async def clear_all(self):
        """ëª¨ë“  ìºì‹œ í´ë¦¬ì–´"""
        try:
            # L1 ìºì‹œ í´ë¦¬ì–´
            self.l1_cache.clear()
            
            # L2 ìºì‹œ í´ë¦¬ì–´
            if self.l2_cache:
                await self.l2_cache.flushdb()
            
            logger.info("Cleared all caches")
            
        except Exception as e:
            logger.error(f"Failed to clear caches: {e}")
```

## ğŸŒ **CDN ë° ì—£ì§€ ì»´í“¨íŒ…**

### ğŸ“¦ **CloudFront ì„¤ì •**

```yaml
# infrastructure/cdn/cloudfront-distribution.yaml
apiVersion: cloudfront.aws.amazon.com/v1
kind: Distribution
metadata:
  name: trading-cdn
spec:
  enabled: true
  defaultCacheBehavior:
    targetOriginId: trading-origin
    viewerProtocolPolicy: redirect-to-https
    allowedMethods:
      - GET
      - HEAD
      - OPTIONS
    cachedMethods:
      - GET
      - HEAD
    forwardedValues:
      queryString: false
      cookies:
        forward: none
    minTTL: 0
    defaultTTL: 86400  # 24ì‹œê°„
    maxTTL: 31536000   # 1ë…„
    compress: true
    lambdaFunctionAssociations:
      - eventType: viewer-request
        lambdaFunctionARN: arn:aws:lambda:us-east-1:123456789012:function:edge-function
  origins:
    - id: trading-origin
      domainName: trading.example.com
      customOriginConfig:
        httpPort: 80
        httpsPort: 443
        originProtocolPolicy: https-only
  priceClass: PriceClass_100  # US, Canada, Europe
  aliases:
    - cdn.trading.example.com
  viewerCertificate:
    acmCertificateArn: arn:aws:acm:us-east-1:123456789012:certificate/xxx
    sslSupportMethod: sni-only
    minimumProtocolVersion: TLSv1.2_2021
```

### ğŸ”§ **ì—£ì§€ í•¨ìˆ˜**

```javascript
// infrastructure/cdn/edge_computing/edge-function.js
'use strict';

exports.handler = async (event) => {
    const request = event.Records[0].cf.request;
    const headers = request.headers;
    
    // ì‚¬ìš©ì ìœ„ì¹˜ ê¸°ë°˜ ë¼ìš°íŒ…
    const country = headers['cloudfront-viewer-country']?.[0]?.value;
    const region = headers['cloudfront-viewer-country-region']?.[0]?.value;
    
    // ì§€ì—­ë³„ ìµœì í™”ëœ ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¼ìš°íŒ…
    if (country === 'US') {
        request.origin.custom.domainName = 'us-east-1.trading.example.com';
    } else if (country === 'JP') {
        request.origin.custom.domainName = 'ap-northeast-1.trading.example.com';
    } else if (country === 'DE') {
        request.origin.custom.domainName = 'eu-central-1.trading.example.com';
    }
    
    // API ìš”ì²­ì— ëŒ€í•œ ìºì‹± ìµœì í™”
    if (request.uri.startsWith('/api/')) {
        // ë™ì  API ì‘ë‹µì€ ìºì‹±í•˜ì§€ ì•ŠìŒ
        request.headers['cache-control'] = [{
            key: 'Cache-Control',
            value: 'no-cache, no-store, must-revalidate'
        }];
    } else if (request.uri.startsWith('/static/')) {
        // ì •ì  ìì‚°ì€ ì¥ê¸° ìºì‹±
        request.headers['cache-control'] = [{
            key: 'Cache-Control',
            value: 'public, max-age=31536000, immutable'
        }];
    }
    
    // ë³´ì•ˆ í—¤ë” ì¶”ê°€
    request.headers['x-frame-options'] = [{
        key: 'X-Frame-Options',
        value: 'DENY'
    }];
    
    request.headers['x-content-type-options'] = [{
        key: 'X-Content-Type-Options',
        value: 'nosniff'
    }];
    
    request.headers['strict-transport-security'] = [{
        key: 'Strict-Transport-Security',
        value: 'max-age=31536000; includeSubDomains'
    }];
    
    return request;
};
```

## ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„**

### ğŸ“‹ **ì™„ë£Œëœ ì‘ì—…**
- âœ… ìë™ ìŠ¤ì¼€ì¼ë§ ì‹œìŠ¤í…œ
- âœ… ë°ì´í„°ë² ì´ìŠ¤ ì½ê¸° ë³µì œë³¸
- âœ… ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œ
- âœ… CDN ë° ì—£ì§€ ì»´í“¨íŒ…

### ğŸ”„ **ì§„í–‰ ì¤‘ì¸ ì‘ì—…**
- ğŸ”„ ìƒ¤ë”© ì „ëµ êµ¬í˜„
- ğŸ”„ ê¸€ë¡œë²Œ ë¡œë“œ ë°¸ëŸ°ì‹±
- ğŸ”„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### â³ **ë‹¤ìŒ ë‹¨ê³„**
1. **Phase 2 ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤** ì‹œì‘
2. **ê³µí†µ ì»´í¬ë„ŒíŠ¸** ì™„ì„±
3. **í…œí”Œë¦¿ ë° ë„êµ¬** ìƒì„±

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-01-31
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: 2024-02-01 (Phase 2 ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì‹œì‘)
**ì¸í”„ë¼ ëª©í‘œ**: 10,000 TPS, < 50ms ì‘ë‹µì‹œê°„, 99.99% ê°€ìš©ì„±
**í™•ì¥ì„±**: 10ë°° íŠ¸ë˜í”½ ì¦ê°€ ì‹œ ìë™ ìŠ¤ì¼€ì¼ë§ 