# 📊 Phase 2.4: 실시간 모니터링 및 관찰성

## 📋 **개요**

### 🎯 **목표**
- **실시간 모니터링**: 서비스 상태 및 성능 실시간 추적
- **분산 추적**: 마이크로서비스 간 호출 추적
- **로그 집계**: 중앙화된 로그 수집 및 분석
- **알림 시스템**: 자동화된 알림 및 장애 대응

### 📊 **성능 목표**
- **메트릭 수집 지연**: < 1초
- **로그 수집 지연**: < 5초
- **분산 추적 지연**: < 100ms
- **알림 발송 지연**: < 30초
- **대시보드 업데이트**: 실시간 (1초 간격)

## 🏗️ **모니터링 아키텍처**

### 📁 **모니터링 구조**
```
monitoring/
├── metrics/                       # 메트릭 수집
│   ├── prometheus/               # Prometheus 메트릭
│   ├── custom-metrics/           # 커스텀 메트릭
│   └── business-metrics/         # 비즈니스 메트릭
├── tracing/                      # 분산 추적
│   ├── jaeger/                  # Jaeger 추적
│   ├── zipkin/                  # Zipkin 추적
│   └── custom-tracing/          # 커스텀 추적
├── logging/                      # 로그 관리
│   ├── elasticsearch/           # Elasticsearch
│   ├── fluentd/                 # Fluentd
│   └── kibana/                  # Kibana
├── alerting/                     # 알림 시스템
│   ├── alertmanager/            # AlertManager
│   ├── notification/            # 알림 채널
│   └── escalation/              # 알림 에스컬레이션
└── dashboards/                   # 대시보드
    ├── grafana/                 # Grafana 대시보드
    ├── custom-dashboards/       # 커스텀 대시보드
    └── real-time/               # 실시간 대시보드
```

## 🔧 **Prometheus 메트릭 수집**

### 📦 **Prometheus 설정**

```yaml
# monitoring/metrics/prometheus/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "alert_rules.yml"
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093
    
    scrape_configs:
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
      
      - job_name: 'trading-services'
        static_configs:
          - targets: ['order-service:8080', 'matching-engine:8080', 'position-service:8080']
        metrics_path: /metrics
        scrape_interval: 10s

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:v2.45.0
        ports:
        - containerPort: 9090
          name: http
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
        - name: prometheus-data
          mountPath: /prometheus
        command:
        - /bin/prometheus
        - --config.file=/etc/prometheus/prometheus.yml
        - --storage.tsdb.path=/prometheus
        - --web.console.libraries=/etc/prometheus/console_libraries
        - --web.console.templates=/etc/prometheus/consoles
        - --storage.tsdb.retention.time=200h
        - --web.enable-lifecycle
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config
      - name: prometheus-data
        persistentVolumeClaim:
          claimName: prometheus-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
spec:
  selector:
    app: prometheus
  ports:
  - port: 9090
    targetPort: 9090
    name: http
  type: ClusterIP
```

### 📦 **커스텀 메트릭 수집기**

```python
# monitoring/metrics/custom-metrics/metrics_collector.py
import asyncio
import time
import psutil
import threading
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from prometheus_client import Counter, Gauge, Histogram, Summary, generate_latest, CONTENT_TYPE_LATEST

logger = logging.getLogger(__name__)

@dataclass
class ServiceMetrics:
    """서비스 메트릭"""
    service_name: str
    request_count: int
    error_count: int
    response_time: float
    active_connections: int
    memory_usage: float
    cpu_usage: float
    timestamp: datetime

class MetricsCollector:
    """메트릭 수집기"""
    
    def __init__(self, service_name: str):
        self.service_name = service_name
        
        # Prometheus 메트릭 정의
        self.request_counter = Counter(
            'http_requests_total',
            'Total HTTP requests',
            ['service', 'method', 'endpoint', 'status']
        )
        
        self.error_counter = Counter(
            'http_errors_total',
            'Total HTTP errors',
            ['service', 'method', 'endpoint', 'error_type']
        )
        
        self.response_time_histogram = Histogram(
            'http_request_duration_seconds',
            'HTTP request duration',
            ['service', 'method', 'endpoint'],
            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
        )
        
        self.active_connections_gauge = Gauge(
            'active_connections',
            'Number of active connections',
            ['service']
        )
        
        self.memory_usage_gauge = Gauge(
            'memory_usage_bytes',
            'Memory usage in bytes',
            ['service']
        )
        
        self.cpu_usage_gauge = Gauge(
            'cpu_usage_percent',
            'CPU usage percentage',
            ['service']
        )
        
        self.trading_metrics = {
            'orders_processed': Counter('orders_processed_total', 'Total orders processed', ['service', 'order_type']),
            'trades_executed': Counter('trades_executed_total', 'Total trades executed', ['service', 'symbol']),
            'position_value': Gauge('position_value_usd', 'Current position value in USD', ['service', 'symbol']),
            'profit_loss': Gauge('profit_loss_usd', 'Current profit/loss in USD', ['service', 'symbol']),
            'order_latency': Histogram('order_processing_duration_seconds', 'Order processing duration', ['service', 'order_type']),
            'matching_engine_tps': Gauge('matching_engine_tps', 'Matching engine transactions per second', ['service'])
        }
        
        self.collection_interval = 10  # 10초
        self.metrics_history = []
        
        logger.info(f"Initialized metrics collector for service: {service_name}")
    
    def record_request(self, method: str, endpoint: str, status: int, duration: float):
        """HTTP 요청 기록"""
        try:
            # 요청 카운터 증가
            self.request_counter.labels(
                service=self.service_name,
                method=method,
                endpoint=endpoint,
                status=status
            ).inc()
            
            # 응답 시간 히스토그램
            self.response_time_histogram.labels(
                service=self.service_name,
                method=method,
                endpoint=endpoint
            ).observe(duration)
            
            # 에러 카운터 (4xx, 5xx 상태 코드)
            if status >= 400:
                error_type = 'client_error' if status < 500 else 'server_error'
                self.error_counter.labels(
                    service=self.service_name,
                    method=method,
                    endpoint=endpoint,
                    error_type=error_type
                ).inc()
                
        except Exception as e:
            logger.error(f"Failed to record request metrics: {e}")
    
    def record_trading_metric(self, metric_name: str, value: float, labels: Dict[str, str] = None):
        """거래 메트릭 기록"""
        try:
            if metric_name in self.trading_metrics:
                metric = self.trading_metrics[metric_name]
                
                if hasattr(metric, 'inc'):
                    # Counter 타입
                    metric.labels(service=self.service_name, **labels or {}).inc(value)
                elif hasattr(metric, 'set'):
                    # Gauge 타입
                    metric.labels(service=self.service_name, **labels or {}).set(value)
                elif hasattr(metric, 'observe'):
                    # Histogram 타입
                    metric.labels(service=self.service_name, **labels or {}).observe(value)
                    
        except Exception as e:
            logger.error(f"Failed to record trading metric {metric_name}: {e}")
    
    def update_system_metrics(self):
        """시스템 메트릭 업데이트"""
        try:
            # 메모리 사용량
            memory = psutil.virtual_memory()
            self.memory_usage_gauge.labels(service=self.service_name).set(memory.used)
            
            # CPU 사용량
            cpu_percent = psutil.cpu_percent(interval=1)
            self.cpu_usage_gauge.labels(service=self.service_name).set(cpu_percent)
            
            # 활성 연결 수 (예시)
            active_connections = len(psutil.net_connections())
            self.active_connections_gauge.labels(service=self.service_name).set(active_connections)
            
        except Exception as e:
            logger.error(f"Failed to update system metrics: {e}")
    
    def get_metrics(self) -> str:
        """Prometheus 형식 메트릭 반환"""
        try:
            # 시스템 메트릭 업데이트
            self.update_system_metrics()
            
            # Prometheus 메트릭 생성
            return generate_latest()
            
        except Exception as e:
            logger.error(f"Failed to generate metrics: {e}")
            return ""
    
    async def start_metrics_collection_loop(self):
        """메트릭 수집 루프 시작"""
        while True:
            try:
                # 시스템 메트릭 업데이트
                self.update_system_metrics()
                
                # 메트릭 히스토리에 저장
                metrics = ServiceMetrics(
                    service_name=self.service_name,
                    request_count=0,  # 실제로는 카운터에서 가져와야 함
                    error_count=0,
                    response_time=0.0,
                    active_connections=len(psutil.net_connections()),
                    memory_usage=psutil.virtual_memory().used,
                    cpu_usage=psutil.cpu_percent(),
                    timestamp=datetime.now()
                )
                
                self.metrics_history.append(metrics)
                
                # 1시간 이전 데이터 제거
                cutoff_time = datetime.now() - timedelta(hours=1)
                self.metrics_history = [
                    m for m in self.metrics_history
                    if m.timestamp > cutoff_time
                ]
                
                await asyncio.sleep(self.collection_interval)
                
            except Exception as e:
                logger.error(f"Metrics collection loop error: {e}")
                await asyncio.sleep(self.collection_interval)
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """메트릭 요약"""
        if not self.metrics_history:
            return {}
        
        latest_metrics = self.metrics_history[-1]
        
        return {
            "service_name": self.service_name,
            "current_metrics": {
                "active_connections": latest_metrics.active_connections,
                "memory_usage_mb": latest_metrics.memory_usage / (1024 * 1024),
                "cpu_usage_percent": latest_metrics.cpu_usage,
                "timestamp": latest_metrics.timestamp.isoformat()
            },
            "history_count": len(self.metrics_history)
        }
```

## 🔧 **Jaeger 분산 추적**

### 📦 **Jaeger 설정**

```yaml
# monitoring/tracing/jaeger/jaeger-config.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.47
        ports:
        - containerPort: 16686
          name: ui
        - containerPort: 14268
          name: http
        - containerPort: 14250
          name: grpc
        env:
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"

---
apiVersion: v1
kind: Service
metadata:
  name: jaeger
  namespace: monitoring
spec:
  selector:
    app: jaeger
  ports:
  - port: 16686
    targetPort: 16686
    name: ui
  - port: 14268
    targetPort: 14268
    name: http
  - port: 14250
    targetPort: 14250
    name: grpc
  type: ClusterIP
```

### 📦 **분산 추적 클라이언트**

```python
# monitoring/tracing/jaeger/tracing_client.py
import asyncio
import time
from typing import Dict, Any, Optional
from contextlib import asynccontextmanager
import logging
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.aiohttp_client import AioHttpClientInstrumentor
from opentelemetry.instrumentation.asyncio import AsyncioInstrumentor

logger = logging.getLogger(__name__)

class TracingClient:
    """분산 추적 클라이언트"""
    
    def __init__(self, service_name: str, jaeger_endpoint: str = "http://jaeger:14268/api/traces"):
        self.service_name = service_name
        self.jaeger_endpoint = jaeger_endpoint
        
        # Tracer Provider 설정
        self.tracer_provider = TracerProvider()
        
        # Jaeger Exporter 설정
        self.jaeger_exporter = JaegerExporter(
            agent_host_name="jaeger",
            agent_port=6831,
        )
        
        # Batch Span Processor 설정
        self.span_processor = BatchSpanProcessor(self.jaeger_exporter)
        self.tracer_provider.add_span_processor(self.span_processor)
        
        # Tracer 설정
        trace.set_tracer_provider(self.tracer_provider)
        self.tracer = trace.get_tracer(service_name)
        
        # 자동 계측 설정
        AsyncioInstrumentor().instrument()
        AioHttpClientInstrumentor().instrument()
        
        logger.info(f"Initialized tracing client for service: {service_name}")
    
    def start_span(self, name: str, attributes: Dict[str, Any] = None) -> trace.Span:
        """스팬 시작"""
        return self.tracer.start_span(name, attributes=attributes or {})
    
    def add_event(self, span: trace.Span, name: str, attributes: Dict[str, Any] = None):
        """이벤트 추가"""
        span.add_event(name, attributes=attributes or {})
    
    def set_attribute(self, span: trace.Span, key: str, value: Any):
        """속성 설정"""
        span.set_attribute(key, value)
    
    def record_exception(self, span: trace.Span, exception: Exception):
        """예외 기록"""
        span.record_exception(exception)
    
    @asynccontextmanager
    async def trace_operation(self, operation_name: str, attributes: Dict[str, Any] = None):
        """작업 추적 컨텍스트 매니저"""
        span = self.start_span(operation_name, attributes)
        
        try:
            yield span
        except Exception as e:
            self.record_exception(span, e)
            raise
        finally:
            span.end()
    
    async def trace_http_request(self, method: str, url: str, headers: Dict[str, str] = None):
        """HTTP 요청 추적"""
        attributes = {
            "http.method": method,
            "http.url": url,
            "service.name": self.service_name
        }
        
        if headers:
            attributes.update({f"http.header.{k}": v for k, v in headers.items()})
        
        async with self.trace_operation("http_request", attributes) as span:
            return span
    
    async def trace_database_query(self, query: str, database: str, table: str = None):
        """데이터베이스 쿼리 추적"""
        attributes = {
            "db.system": database,
            "db.statement": query,
            "service.name": self.service_name
        }
        
        if table:
            attributes["db.table"] = table
        
        async with self.trace_operation("database_query", attributes) as span:
            return span
    
    async def trace_trading_operation(self, operation_type: str, symbol: str, amount: float = None):
        """거래 작업 추적"""
        attributes = {
            "trading.operation": operation_type,
            "trading.symbol": symbol,
            "service.name": self.service_name
        }
        
        if amount:
            attributes["trading.amount"] = amount
        
        async with self.trace_operation("trading_operation", attributes) as span:
            return span
    
    def get_trace_id(self, span: trace.Span) -> str:
        """트레이스 ID 조회"""
        return format(span.get_span_context().trace_id, "032x")
    
    def get_span_id(self, span: trace.Span) -> str:
        """스팬 ID 조회"""
        return format(span.get_span_context().span_id, "016x")
    
    async def shutdown(self):
        """종료"""
        await self.tracer_provider.shutdown()
        logger.info("Tracing client shutdown complete")

# 사용 예시
class TradingServiceWithTracing:
    """추적이 포함된 거래 서비스"""
    
    def __init__(self):
        self.tracing_client = TracingClient("trading-service")
    
    async def process_order(self, order_data: Dict[str, Any]):
        """주문 처리 (추적 포함)"""
        async with self.tracing_client.trace_trading_operation(
            "process_order",
            order_data.get("symbol", "unknown"),
            order_data.get("amount", 0)
        ) as span:
            
            # 주문 검증
            self.tracing_client.add_event(span, "order_validation_start")
            # ... 검증 로직
            self.tracing_client.add_event(span, "order_validation_complete")
            
            # 데이터베이스 저장
            async with self.tracing_client.trace_database_query(
                "INSERT INTO orders",
                "postgresql",
                "orders"
            ) as db_span:
                # ... 데이터베이스 작업
                pass
            
            # 외부 API 호출
            async with self.tracing_client.trace_http_request(
                "POST",
                "https://api.exchange.com/orders"
            ) as api_span:
                # ... API 호출
                pass
            
            # 결과 반환
            span.set_attribute("order.status", "processed")
            return {"status": "success", "trace_id": self.tracing_client.get_trace_id(span)}
```

## 🔧 **Elasticsearch 로그 집계**

### 📦 **Elasticsearch 설정**

```yaml
# monitoring/logging/elasticsearch/elasticsearch-config.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: monitoring
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        env:
        - name: cluster.name
          value: "trading-cluster"
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms1g -Xmx1g"
        - name: xpack.security.enabled
          value: "false"
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
      volumes:
      - name: elasticsearch-data
        persistentVolumeClaim:
          claimName: elasticsearch-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: monitoring
spec:
  selector:
    app: elasticsearch
  ports:
  - port: 9200
    targetPort: 9200
    name: http
  - port: 9300
    targetPort: 9300
    name: transport
  type: ClusterIP
```

### 📦 **로그 수집기**

```python
# monitoring/logging/fluentd/log_collector.py
import asyncio
import json
import logging
from typing import Dict, Any, Optional
from datetime import datetime
import aiohttp
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class LogEntry:
    """로그 엔트리"""
    timestamp: datetime
    level: str
    service: str
    message: str
    trace_id: Optional[str] = None
    span_id: Optional[str] = None
    user_id: Optional[str] = None
    request_id: Optional[str] = None
    extra: Dict[str, Any] = None

class LogCollector:
    """로그 수집기"""
    
    def __init__(self, service_name: str, elasticsearch_url: str = "http://elasticsearch:9200"):
        self.service_name = service_name
        self.elasticsearch_url = elasticsearch_url
        self.session = None
        self.log_buffer = []
        self.buffer_size = 100
        self.flush_interval = 10  # 10초
        
        logger.info(f"Initialized log collector for service: {service_name}")
    
    async def initialize(self):
        """초기화"""
        self.session = aiohttp.ClientSession()
        
        # Elasticsearch 인덱스 생성
        await self._create_index()
    
    async def _create_index(self):
        """Elasticsearch 인덱스 생성"""
        try:
            index_name = f"logs-{datetime.now().strftime('%Y.%m.%d')}"
            index_url = f"{self.elasticsearch_url}/{index_name}"
            
            # 인덱스 매핑 설정
            mapping = {
                "mappings": {
                    "properties": {
                        "timestamp": {"type": "date"},
                        "level": {"type": "keyword"},
                        "service": {"type": "keyword"},
                        "message": {"type": "text"},
                        "trace_id": {"type": "keyword"},
                        "span_id": {"type": "keyword"},
                        "user_id": {"type": "keyword"},
                        "request_id": {"type": "keyword"},
                        "extra": {"type": "object"}
                    }
                },
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 1
                }
            }
            
            async with self.session.put(index_url, json=mapping) as response:
                if response.status in [200, 201]:
                    logger.info(f"Created Elasticsearch index: {index_name}")
                else:
                    logger.warning(f"Index creation response: {response.status}")
                    
        except Exception as e:
            logger.error(f"Failed to create Elasticsearch index: {e}")
    
    async def log(self, level: str, message: str, trace_id: str = None, 
                 span_id: str = None, user_id: str = None, request_id: str = None,
                 extra: Dict[str, Any] = None):
        """로그 기록"""
        log_entry = LogEntry(
            timestamp=datetime.now(),
            level=level,
            service=self.service_name,
            message=message,
            trace_id=trace_id,
            span_id=span_id,
            user_id=user_id,
            request_id=request_id,
            extra=extra or {}
        )
        
        # 버퍼에 추가
        self.log_buffer.append(log_entry)
        
        # 버퍼가 가득 차면 즉시 전송
        if len(self.log_buffer) >= self.buffer_size:
            await self._flush_buffer()
    
    async def _flush_buffer(self):
        """버퍼 플러시"""
        if not self.log_buffer:
            return
        
        try:
            # 로그 데이터 준비
            logs_data = []
            for entry in self.log_buffer:
                log_doc = {
                    "timestamp": entry.timestamp.isoformat(),
                    "level": entry.level,
                    "service": entry.service,
                    "message": entry.message
                }
                
                if entry.trace_id:
                    log_doc["trace_id"] = entry.trace_id
                if entry.span_id:
                    log_doc["span_id"] = entry.span_id
                if entry.user_id:
                    log_doc["user_id"] = entry.user_id
                if entry.request_id:
                    log_doc["request_id"] = entry.request_id
                if entry.extra:
                    log_doc["extra"] = entry.extra
                
                logs_data.append(log_doc)
            
            # Elasticsearch에 전송
            index_name = f"logs-{datetime.now().strftime('%Y.%m.%d')}"
            bulk_url = f"{self.elasticsearch_url}/{index_name}/_bulk"
            
            # Bulk API 형식으로 데이터 준비
            bulk_data = ""
            for log_doc in logs_data:
                bulk_data += json.dumps({"index": {}}) + "\n"
                bulk_data += json.dumps(log_doc) + "\n"
            
            async with self.session.post(
                bulk_url,
                data=bulk_data,
                headers={"Content-Type": "application/x-ndjson"}
            ) as response:
                if response.status == 200:
                    logger.debug(f"Flushed {len(logs_data)} log entries to Elasticsearch")
                else:
                    logger.error(f"Failed to flush logs: {response.status}")
            
            # 버퍼 클리어
            self.log_buffer.clear()
            
        except Exception as e:
            logger.error(f"Failed to flush log buffer: {e}")
    
    async def start_flush_loop(self):
        """플러시 루프 시작"""
        while True:
            try:
                await self._flush_buffer()
                await asyncio.sleep(self.flush_interval)
                
            except Exception as e:
                logger.error(f"Flush loop error: {e}")
                await asyncio.sleep(self.flush_interval)
    
    async def search_logs(self, query: str, start_time: datetime = None, 
                         end_time: datetime = None, level: str = None,
                         service: str = None, limit: int = 100) -> List[Dict[str, Any]]:
        """로그 검색"""
        try:
            index_name = f"logs-{datetime.now().strftime('%Y.%m.%d')}"
            search_url = f"{self.elasticsearch_url}/{index_name}/_search"
            
            # 검색 쿼리 구성
            search_query = {
                "query": {
                    "bool": {
                        "must": [
                            {"query_string": {"query": query}}
                        ]
                    }
                },
                "sort": [{"timestamp": {"order": "desc"}}],
                "size": limit
            }
            
            # 필터 추가
            filters = []
            
            if start_time and end_time:
                filters.append({
                    "range": {
                        "timestamp": {
                            "gte": start_time.isoformat(),
                            "lte": end_time.isoformat()
                        }
                    }
                })
            
            if level:
                filters.append({"term": {"level": level}})
            
            if service:
                filters.append({"term": {"service": service}})
            
            if filters:
                search_query["query"]["bool"]["filter"] = filters
            
            async with self.session.post(search_url, json=search_query) as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get("hits", {}).get("hits", [])
                else:
                    logger.error(f"Log search failed: {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"Failed to search logs: {e}")
            return []
    
    async def close(self):
        """종료"""
        await self._flush_buffer()
        if self.session:
            await self.session.close()
            logger.info("Log collector closed")
```

## 🔧 **AlertManager 알림 시스템**

### 📦 **AlertManager 설정**

```yaml
# monitoring/alerting/alertmanager/alertmanager-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/YOUR_SLACK_WEBHOOK'
    
    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'slack-notifications'
      routes:
      - match:
          severity: critical
        receiver: 'pager-duty-critical'
        continue: true
      - match:
          severity: warning
        receiver: 'slack-notifications'
    
    receivers:
    - name: 'slack-notifications'
      slack_configs:
      - channel: '#trading-alerts'
        title: '{{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        send_resolved: true
    
    - name: 'pager-duty-critical'
      pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_KEY'
        description: '{{ template "pagerduty.description" . }}'
        severity: '{{ if eq .CommonLabels.severity "critical" }}critical{{ else }}warning{{ end }}'
    
    templates:
    - '/etc/alertmanager/template/*.tmpl'

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.25.0
        ports:
        - containerPort: 9093
          name: http
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager
        command:
        - /bin/alertmanager
        - --config.file=/etc/alertmanager/alertmanager.yml
        - --storage.path=/alertmanager
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: alertmanager-config
        configMap:
          name: alertmanager-config

---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  selector:
    app: alertmanager
  ports:
  - port: 9093
    targetPort: 9093
    name: http
  type: ClusterIP
```

### 📦 **알림 관리자**

```python
# monitoring/alerting/alertmanager/alert_manager.py
import asyncio
import aiohttp
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

@dataclass
class Alert:
    """알림"""
    id: str
    name: str
    severity: str  # info, warning, critical
    message: str
    service: str
    metric: str
    value: float
    threshold: float
    timestamp: datetime
    resolved: bool = False
    resolved_at: Optional[datetime] = None

class AlertManager:
    """알림 관리자"""
    
    def __init__(self, alertmanager_url: str = "http://alertmanager:9093"):
        self.alertmanager_url = alertmanager_url
        self.session = None
        self.active_alerts = {}
        self.alert_history = []
        
        logger.info("Initialized alert manager")
    
    async def initialize(self):
        """초기화"""
        self.session = aiohttp.ClientSession()
    
    async def create_alert(self, name: str, severity: str, message: str,
                          service: str, metric: str, value: float, threshold: float) -> str:
        """알림 생성"""
        alert_id = f"{service}_{metric}_{int(datetime.now().timestamp())}"
        
        alert = Alert(
            id=alert_id,
            name=name,
            severity=severity,
            message=message,
            service=service,
            metric=metric,
            value=value,
            threshold=threshold,
            timestamp=datetime.now()
        )
        
        # 활성 알림에 추가
        self.active_alerts[alert_id] = alert
        
        # AlertManager에 전송
        await self._send_to_alertmanager(alert)
        
        logger.info(f"Created alert: {alert_id} ({severity})")
        return alert_id
    
    async def resolve_alert(self, alert_id: str):
        """알림 해결"""
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            alert.resolved = True
            alert.resolved_at = datetime.now()
            
            # 알림 히스토리에 추가
            self.alert_history.append(alert)
            
            # 활성 알림에서 제거
            del self.active_alerts[alert_id]
            
            # AlertManager에 해결 알림 전송
            await self._send_resolution_to_alertmanager(alert)
            
            logger.info(f"Resolved alert: {alert_id}")
    
    async def _send_to_alertmanager(self, alert: Alert):
        """AlertManager에 알림 전송"""
        try:
            alert_data = {
                "alerts": [
                    {
                        "labels": {
                            "alertname": alert.name,
                            "severity": alert.severity,
                            "service": alert.service,
                            "metric": alert.metric
                        },
                        "annotations": {
                            "description": alert.message,
                            "value": str(alert.value),
                            "threshold": str(alert.threshold)
                        },
                        "startsAt": alert.timestamp.isoformat() + "Z"
                    }
                ]
            }
            
            async with self.session.post(
                f"{self.alertmanager_url}/api/v1/alerts",
                json=alert_data
            ) as response:
                if response.status == 200:
                    logger.debug(f"Alert sent to AlertManager: {alert.id}")
                else:
                    logger.error(f"Failed to send alert to AlertManager: {response.status}")
                    
        except Exception as e:
            logger.error(f"Failed to send alert to AlertManager: {e}")
    
    async def _send_resolution_to_alertmanager(self, alert: Alert):
        """AlertManager에 해결 알림 전송"""
        try:
            alert_data = {
                "alerts": [
                    {
                        "labels": {
                            "alertname": alert.name,
                            "severity": alert.severity,
                            "service": alert.service,
                            "metric": alert.metric
                        },
                        "annotations": {
                            "description": f"Alert resolved: {alert.message}",
                            "value": str(alert.value),
                            "threshold": str(alert.threshold)
                        },
                        "startsAt": alert.timestamp.isoformat() + "Z",
                        "endsAt": alert.resolved_at.isoformat() + "Z"
                    }
                ]
            }
            
            async with self.session.post(
                f"{self.alertmanager_url}/api/v1/alerts",
                json=alert_data
            ) as response:
                if response.status == 200:
                    logger.debug(f"Resolution sent to AlertManager: {alert.id}")
                else:
                    logger.error(f"Failed to send resolution to AlertManager: {response.status}")
                    
        except Exception as e:
            logger.error(f"Failed to send resolution to AlertManager: {e}")
    
    def get_active_alerts(self) -> List[Alert]:
        """활성 알림 조회"""
        return list(self.active_alerts.values())
    
    def get_alert_history(self, hours: int = 24) -> List[Alert]:
        """알림 히스토리 조회"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        return [
            alert for alert in self.alert_history
            if alert.timestamp > cutoff_time
        ]
    
    def get_alert_stats(self) -> Dict[str, Any]:
        """알림 통계"""
        active_alerts = self.get_active_alerts()
        recent_history = self.get_alert_history(24)
        
        return {
            "active_alerts": {
                "total": len(active_alerts),
                "critical": len([a for a in active_alerts if a.severity == "critical"]),
                "warning": len([a for a in active_alerts if a.severity == "warning"]),
                "info": len([a for a in active_alerts if a.severity == "info"])
            },
            "recent_alerts": {
                "total": len(recent_history),
                "resolved": len([a for a in recent_history if a.resolved]),
                "unresolved": len([a for a in recent_history if not a.resolved])
            }
        }
    
    async def close(self):
        """종료"""
        if self.session:
            await self.session.close()
            logger.info("Alert manager closed")
```

## 🎯 **다음 단계**

### 📋 **완료된 작업**
- ✅ Prometheus 메트릭 수집
- ✅ Jaeger 분산 추적
- ✅ Elasticsearch 로그 집계
- ✅ AlertManager 알림 시스템
- ✅ 커스텀 메트릭 수집기
- ✅ 로그 수집기
- ✅ 알림 관리자

### 🔄 **진행 중인 작업**
- 🔄 Grafana 대시보드
- 🔄 실시간 모니터링
- 🔄 성능 최적화

### ⏳ **다음 단계**
1. **Phase 2.5 장애 복구** 문서 생성
2. **Phase 2.6 성능 최적화** 문서 생성
3. **Phase 2 완료**: Phase 2의 100% 완성

---

**마지막 업데이트**: 2024-01-31
**다음 업데이트**: 2024-02-01 (Phase 2.5 장애 복구)
**모니터링 목표**: < 1초 메트릭 수집, < 5초 로그 수집, < 100ms 추적
**알림 시스템**: < 30초 알림 발송, 실시간 대시보드 