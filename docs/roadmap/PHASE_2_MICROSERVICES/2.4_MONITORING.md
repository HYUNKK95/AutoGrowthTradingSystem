# ğŸ“Š Phase 2.4: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±

## ğŸ“‹ **ê°œìš”**

### ğŸ¯ **ëª©í‘œ**
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ì„œë¹„ìŠ¤ ìƒíƒœ ë° ì„±ëŠ¥ ì‹¤ì‹œê°„ ì¶”ì 
- **ë¶„ì‚° ì¶”ì **: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ í˜¸ì¶œ ì¶”ì 
- **ë¡œê·¸ ì§‘ê³„**: ì¤‘ì•™í™”ëœ ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„
- **ì•Œë¦¼ ì‹œìŠ¤í…œ**: ìë™í™”ëœ ì•Œë¦¼ ë° ì¥ì•  ëŒ€ì‘

### ğŸ“Š **ì„±ëŠ¥ ëª©í‘œ**
- **ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì§€ì—°**: < 1ì´ˆ
- **ë¡œê·¸ ìˆ˜ì§‘ ì§€ì—°**: < 5ì´ˆ
- **ë¶„ì‚° ì¶”ì  ì§€ì—°**: < 100ms
- **ì•Œë¦¼ ë°œì†¡ ì§€ì—°**: < 30ì´ˆ
- **ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸**: ì‹¤ì‹œê°„ (1ì´ˆ ê°„ê²©)

## ğŸ—ï¸ **ëª¨ë‹ˆí„°ë§ ì•„í‚¤í…ì²˜**

### ğŸ“ **ëª¨ë‹ˆí„°ë§ êµ¬ì¡°**
```
monitoring/
â”œâ”€â”€ metrics/                       # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
â”‚   â”œâ”€â”€ prometheus/               # Prometheus ë©”íŠ¸ë¦­
â”‚   â”œâ”€â”€ custom-metrics/           # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­
â”‚   â””â”€â”€ business-metrics/         # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
â”œâ”€â”€ tracing/                      # ë¶„ì‚° ì¶”ì 
â”‚   â”œâ”€â”€ jaeger/                  # Jaeger ì¶”ì 
â”‚   â”œâ”€â”€ zipkin/                  # Zipkin ì¶”ì 
â”‚   â””â”€â”€ custom-tracing/          # ì»¤ìŠ¤í…€ ì¶”ì 
â”œâ”€â”€ logging/                      # ë¡œê·¸ ê´€ë¦¬
â”‚   â”œâ”€â”€ elasticsearch/           # Elasticsearch
â”‚   â”œâ”€â”€ fluentd/                 # Fluentd
â”‚   â””â”€â”€ kibana/                  # Kibana
â”œâ”€â”€ alerting/                     # ì•Œë¦¼ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ alertmanager/            # AlertManager
â”‚   â”œâ”€â”€ notification/            # ì•Œë¦¼ ì±„ë„
â”‚   â””â”€â”€ escalation/              # ì•Œë¦¼ ì—ìŠ¤ì»¬ë ˆì´ì…˜
â””â”€â”€ dashboards/                   # ëŒ€ì‹œë³´ë“œ
    â”œâ”€â”€ grafana/                 # Grafana ëŒ€ì‹œë³´ë“œ
    â”œâ”€â”€ custom-dashboards/       # ì»¤ìŠ¤í…€ ëŒ€ì‹œë³´ë“œ
    â””â”€â”€ real-time/               # ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ
```

## ğŸ”§ **Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘**

### ğŸ“¦ **Prometheus ì„¤ì •**

```yaml
# monitoring/metrics/prometheus/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "alert_rules.yml"
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093
    
    scrape_configs:
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
      
      - job_name: 'trading-services'
        static_configs:
          - targets: ['order-service:8080', 'matching-engine:8080', 'position-service:8080']
        metrics_path: /metrics
        scrape_interval: 10s

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:v2.45.0
        ports:
        - containerPort: 9090
          name: http
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
        - name: prometheus-data
          mountPath: /prometheus
        command:
        - /bin/prometheus
        - --config.file=/etc/prometheus/prometheus.yml
        - --storage.tsdb.path=/prometheus
        - --web.console.libraries=/etc/prometheus/console_libraries
        - --web.console.templates=/etc/prometheus/consoles
        - --storage.tsdb.retention.time=200h
        - --web.enable-lifecycle
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config
      - name: prometheus-data
        persistentVolumeClaim:
          claimName: prometheus-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
spec:
  selector:
    app: prometheus
  ports:
  - port: 9090
    targetPort: 9090
    name: http
  type: ClusterIP
```

### ğŸ“¦ **ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°**

```python
# monitoring/metrics/custom-metrics/metrics_collector.py
import asyncio
import time
import psutil
import threading
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from prometheus_client import Counter, Gauge, Histogram, Summary, generate_latest, CONTENT_TYPE_LATEST

logger = logging.getLogger(__name__)

@dataclass
class ServiceMetrics:
    """ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­"""
    service_name: str
    request_count: int
    error_count: int
    response_time: float
    active_connections: int
    memory_usage: float
    cpu_usage: float
    timestamp: datetime

class MetricsCollector:
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, service_name: str):
        self.service_name = service_name
        
        # Prometheus ë©”íŠ¸ë¦­ ì •ì˜
        self.request_counter = Counter(
            'http_requests_total',
            'Total HTTP requests',
            ['service', 'method', 'endpoint', 'status']
        )
        
        self.error_counter = Counter(
            'http_errors_total',
            'Total HTTP errors',
            ['service', 'method', 'endpoint', 'error_type']
        )
        
        self.response_time_histogram = Histogram(
            'http_request_duration_seconds',
            'HTTP request duration',
            ['service', 'method', 'endpoint'],
            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
        )
        
        self.active_connections_gauge = Gauge(
            'active_connections',
            'Number of active connections',
            ['service']
        )
        
        self.memory_usage_gauge = Gauge(
            'memory_usage_bytes',
            'Memory usage in bytes',
            ['service']
        )
        
        self.cpu_usage_gauge = Gauge(
            'cpu_usage_percent',
            'CPU usage percentage',
            ['service']
        )
        
        self.trading_metrics = {
            'orders_processed': Counter('orders_processed_total', 'Total orders processed', ['service', 'order_type']),
            'trades_executed': Counter('trades_executed_total', 'Total trades executed', ['service', 'symbol']),
            'position_value': Gauge('position_value_usd', 'Current position value in USD', ['service', 'symbol']),
            'profit_loss': Gauge('profit_loss_usd', 'Current profit/loss in USD', ['service', 'symbol']),
            'order_latency': Histogram('order_processing_duration_seconds', 'Order processing duration', ['service', 'order_type']),
            'matching_engine_tps': Gauge('matching_engine_tps', 'Matching engine transactions per second', ['service'])
        }
        
        self.collection_interval = 10  # 10ì´ˆ
        self.metrics_history = []
        
        logger.info(f"Initialized metrics collector for service: {service_name}")
    
    def record_request(self, method: str, endpoint: str, status: int, duration: float):
        """HTTP ìš”ì²­ ê¸°ë¡"""
        try:
            # ìš”ì²­ ì¹´ìš´í„° ì¦ê°€
            self.request_counter.labels(
                service=self.service_name,
                method=method,
                endpoint=endpoint,
                status=status
            ).inc()
            
            # ì‘ë‹µ ì‹œê°„ íˆìŠ¤í† ê·¸ë¨
            self.response_time_histogram.labels(
                service=self.service_name,
                method=method,
                endpoint=endpoint
            ).observe(duration)
            
            # ì—ëŸ¬ ì¹´ìš´í„° (4xx, 5xx ìƒíƒœ ì½”ë“œ)
            if status >= 400:
                error_type = 'client_error' if status < 500 else 'server_error'
                self.error_counter.labels(
                    service=self.service_name,
                    method=method,
                    endpoint=endpoint,
                    error_type=error_type
                ).inc()
                
        except Exception as e:
            logger.error(f"Failed to record request metrics: {e}")
    
    def record_trading_metric(self, metric_name: str, value: float, labels: Dict[str, str] = None):
        """ê±°ë˜ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        try:
            if metric_name in self.trading_metrics:
                metric = self.trading_metrics[metric_name]
                
                if hasattr(metric, 'inc'):
                    # Counter íƒ€ì…
                    metric.labels(service=self.service_name, **labels or {}).inc(value)
                elif hasattr(metric, 'set'):
                    # Gauge íƒ€ì…
                    metric.labels(service=self.service_name, **labels or {}).set(value)
                elif hasattr(metric, 'observe'):
                    # Histogram íƒ€ì…
                    metric.labels(service=self.service_name, **labels or {}).observe(value)
                    
        except Exception as e:
            logger.error(f"Failed to record trading metric {metric_name}: {e}")
    
    def update_system_metrics(self):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memory = psutil.virtual_memory()
            self.memory_usage_gauge.labels(service=self.service_name).set(memory.used)
            
            # CPU ì‚¬ìš©ëŸ‰
            cpu_percent = psutil.cpu_percent(interval=1)
            self.cpu_usage_gauge.labels(service=self.service_name).set(cpu_percent)
            
            # í™œì„± ì—°ê²° ìˆ˜ (ì˜ˆì‹œ)
            active_connections = len(psutil.net_connections())
            self.active_connections_gauge.labels(service=self.service_name).set(active_connections)
            
        except Exception as e:
            logger.error(f"Failed to update system metrics: {e}")
    
    def get_metrics(self) -> str:
        """Prometheus í˜•ì‹ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        try:
            # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.update_system_metrics()
            
            # Prometheus ë©”íŠ¸ë¦­ ìƒì„±
            return generate_latest()
            
        except Exception as e:
            logger.error(f"Failed to generate metrics: {e}")
            return ""
    
    async def start_metrics_collection_loop(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë£¨í”„ ì‹œì‘"""
        while True:
            try:
                # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                self.update_system_metrics()
                
                # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ì— ì €ì¥
                metrics = ServiceMetrics(
                    service_name=self.service_name,
                    request_count=0,  # ì‹¤ì œë¡œëŠ” ì¹´ìš´í„°ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
                    error_count=0,
                    response_time=0.0,
                    active_connections=len(psutil.net_connections()),
                    memory_usage=psutil.virtual_memory().used,
                    cpu_usage=psutil.cpu_percent(),
                    timestamp=datetime.now()
                )
                
                self.metrics_history.append(metrics)
                
                # 1ì‹œê°„ ì´ì „ ë°ì´í„° ì œê±°
                cutoff_time = datetime.now() - timedelta(hours=1)
                self.metrics_history = [
                    m for m in self.metrics_history
                    if m.timestamp > cutoff_time
                ]
                
                await asyncio.sleep(self.collection_interval)
                
            except Exception as e:
                logger.error(f"Metrics collection loop error: {e}")
                await asyncio.sleep(self.collection_interval)
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ìš”ì•½"""
        if not self.metrics_history:
            return {}
        
        latest_metrics = self.metrics_history[-1]
        
        return {
            "service_name": self.service_name,
            "current_metrics": {
                "active_connections": latest_metrics.active_connections,
                "memory_usage_mb": latest_metrics.memory_usage / (1024 * 1024),
                "cpu_usage_percent": latest_metrics.cpu_usage,
                "timestamp": latest_metrics.timestamp.isoformat()
            },
            "history_count": len(self.metrics_history)
        }
```

## ğŸ”§ **Jaeger ë¶„ì‚° ì¶”ì **

### ğŸ“¦ **Jaeger ì„¤ì •**

```yaml
# monitoring/tracing/jaeger/jaeger-config.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.47
        ports:
        - containerPort: 16686
          name: ui
        - containerPort: 14268
          name: http
        - containerPort: 14250
          name: grpc
        env:
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"

---
apiVersion: v1
kind: Service
metadata:
  name: jaeger
  namespace: monitoring
spec:
  selector:
    app: jaeger
  ports:
  - port: 16686
    targetPort: 16686
    name: ui
  - port: 14268
    targetPort: 14268
    name: http
  - port: 14250
    targetPort: 14250
    name: grpc
  type: ClusterIP
```

### ğŸ“¦ **ë¶„ì‚° ì¶”ì  í´ë¼ì´ì–¸íŠ¸**

```python
# monitoring/tracing/jaeger/tracing_client.py
import asyncio
import time
from typing import Dict, Any, Optional
from contextlib import asynccontextmanager
import logging
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.aiohttp_client import AioHttpClientInstrumentor
from opentelemetry.instrumentation.asyncio import AsyncioInstrumentor

logger = logging.getLogger(__name__)

class TracingClient:
    """ë¶„ì‚° ì¶”ì  í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, service_name: str, jaeger_endpoint: str = "http://jaeger:14268/api/traces"):
        self.service_name = service_name
        self.jaeger_endpoint = jaeger_endpoint
        
        # Tracer Provider ì„¤ì •
        self.tracer_provider = TracerProvider()
        
        # Jaeger Exporter ì„¤ì •
        self.jaeger_exporter = JaegerExporter(
            agent_host_name="jaeger",
            agent_port=6831,
        )
        
        # Batch Span Processor ì„¤ì •
        self.span_processor = BatchSpanProcessor(self.jaeger_exporter)
        self.tracer_provider.add_span_processor(self.span_processor)
        
        # Tracer ì„¤ì •
        trace.set_tracer_provider(self.tracer_provider)
        self.tracer = trace.get_tracer(service_name)
        
        # ìë™ ê³„ì¸¡ ì„¤ì •
        AsyncioInstrumentor().instrument()
        AioHttpClientInstrumentor().instrument()
        
        logger.info(f"Initialized tracing client for service: {service_name}")
    
    def start_span(self, name: str, attributes: Dict[str, Any] = None) -> trace.Span:
        """ìŠ¤íŒ¬ ì‹œì‘"""
        return self.tracer.start_span(name, attributes=attributes or {})
    
    def add_event(self, span: trace.Span, name: str, attributes: Dict[str, Any] = None):
        """ì´ë²¤íŠ¸ ì¶”ê°€"""
        span.add_event(name, attributes=attributes or {})
    
    def set_attribute(self, span: trace.Span, key: str, value: Any):
        """ì†ì„± ì„¤ì •"""
        span.set_attribute(key, value)
    
    def record_exception(self, span: trace.Span, exception: Exception):
        """ì˜ˆì™¸ ê¸°ë¡"""
        span.record_exception(exception)
    
    @asynccontextmanager
    async def trace_operation(self, operation_name: str, attributes: Dict[str, Any] = None):
        """ì‘ì—… ì¶”ì  ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        span = self.start_span(operation_name, attributes)
        
        try:
            yield span
        except Exception as e:
            self.record_exception(span, e)
            raise
        finally:
            span.end()
    
    async def trace_http_request(self, method: str, url: str, headers: Dict[str, str] = None):
        """HTTP ìš”ì²­ ì¶”ì """
        attributes = {
            "http.method": method,
            "http.url": url,
            "service.name": self.service_name
        }
        
        if headers:
            attributes.update({f"http.header.{k}": v for k, v in headers.items()})
        
        async with self.trace_operation("http_request", attributes) as span:
            return span
    
    async def trace_database_query(self, query: str, database: str, table: str = None):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì¶”ì """
        attributes = {
            "db.system": database,
            "db.statement": query,
            "service.name": self.service_name
        }
        
        if table:
            attributes["db.table"] = table
        
        async with self.trace_operation("database_query", attributes) as span:
            return span
    
    async def trace_trading_operation(self, operation_type: str, symbol: str, amount: float = None):
        """ê±°ë˜ ì‘ì—… ì¶”ì """
        attributes = {
            "trading.operation": operation_type,
            "trading.symbol": symbol,
            "service.name": self.service_name
        }
        
        if amount:
            attributes["trading.amount"] = amount
        
        async with self.trace_operation("trading_operation", attributes) as span:
            return span
    
    def get_trace_id(self, span: trace.Span) -> str:
        """íŠ¸ë ˆì´ìŠ¤ ID ì¡°íšŒ"""
        return format(span.get_span_context().trace_id, "032x")
    
    def get_span_id(self, span: trace.Span) -> str:
        """ìŠ¤íŒ¬ ID ì¡°íšŒ"""
        return format(span.get_span_context().span_id, "016x")
    
    async def shutdown(self):
        """ì¢…ë£Œ"""
        await self.tracer_provider.shutdown()
        logger.info("Tracing client shutdown complete")

# ì‚¬ìš© ì˜ˆì‹œ
class TradingServiceWithTracing:
    """ì¶”ì ì´ í¬í•¨ëœ ê±°ë˜ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.tracing_client = TracingClient("trading-service")
    
    async def process_order(self, order_data: Dict[str, Any]):
        """ì£¼ë¬¸ ì²˜ë¦¬ (ì¶”ì  í¬í•¨)"""
        async with self.tracing_client.trace_trading_operation(
            "process_order",
            order_data.get("symbol", "unknown"),
            order_data.get("amount", 0)
        ) as span:
            
            # ì£¼ë¬¸ ê²€ì¦
            self.tracing_client.add_event(span, "order_validation_start")
            # ... ê²€ì¦ ë¡œì§
            self.tracing_client.add_event(span, "order_validation_complete")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            async with self.tracing_client.trace_database_query(
                "INSERT INTO orders",
                "postgresql",
                "orders"
            ) as db_span:
                # ... ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—…
                pass
            
            # ì™¸ë¶€ API í˜¸ì¶œ
            async with self.tracing_client.trace_http_request(
                "POST",
                "https://api.exchange.com/orders"
            ) as api_span:
                # ... API í˜¸ì¶œ
                pass
            
            # ê²°ê³¼ ë°˜í™˜
            span.set_attribute("order.status", "processed")
            return {"status": "success", "trace_id": self.tracing_client.get_trace_id(span)}
```

## ğŸ”§ **Elasticsearch ë¡œê·¸ ì§‘ê³„**

### ğŸ“¦ **Elasticsearch ì„¤ì •**

```yaml
# monitoring/logging/elasticsearch/elasticsearch-config.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: monitoring
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        env:
        - name: cluster.name
          value: "trading-cluster"
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms1g -Xmx1g"
        - name: xpack.security.enabled
          value: "false"
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
      volumes:
      - name: elasticsearch-data
        persistentVolumeClaim:
          claimName: elasticsearch-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: monitoring
spec:
  selector:
    app: elasticsearch
  ports:
  - port: 9200
    targetPort: 9200
    name: http
  - port: 9300
    targetPort: 9300
    name: transport
  type: ClusterIP
```

### ğŸ“¦ **ë¡œê·¸ ìˆ˜ì§‘ê¸°**

```python
# monitoring/logging/fluentd/log_collector.py
import asyncio
import json
import logging
from typing import Dict, Any, Optional
from datetime import datetime
import aiohttp
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class LogEntry:
    """ë¡œê·¸ ì—”íŠ¸ë¦¬"""
    timestamp: datetime
    level: str
    service: str
    message: str
    trace_id: Optional[str] = None
    span_id: Optional[str] = None
    user_id: Optional[str] = None
    request_id: Optional[str] = None
    extra: Dict[str, Any] = None

class LogCollector:
    """ë¡œê·¸ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, service_name: str, elasticsearch_url: str = "http://elasticsearch:9200"):
        self.service_name = service_name
        self.elasticsearch_url = elasticsearch_url
        self.session = None
        self.log_buffer = []
        self.buffer_size = 100
        self.flush_interval = 10  # 10ì´ˆ
        
        logger.info(f"Initialized log collector for service: {service_name}")
    
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        self.session = aiohttp.ClientSession()
        
        # Elasticsearch ì¸ë±ìŠ¤ ìƒì„±
        await self._create_index()
    
    async def _create_index(self):
        """Elasticsearch ì¸ë±ìŠ¤ ìƒì„±"""
        try:
            index_name = f"logs-{datetime.now().strftime('%Y.%m.%d')}"
            index_url = f"{self.elasticsearch_url}/{index_name}"
            
            # ì¸ë±ìŠ¤ ë§¤í•‘ ì„¤ì •
            mapping = {
                "mappings": {
                    "properties": {
                        "timestamp": {"type": "date"},
                        "level": {"type": "keyword"},
                        "service": {"type": "keyword"},
                        "message": {"type": "text"},
                        "trace_id": {"type": "keyword"},
                        "span_id": {"type": "keyword"},
                        "user_id": {"type": "keyword"},
                        "request_id": {"type": "keyword"},
                        "extra": {"type": "object"}
                    }
                },
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 1
                }
            }
            
            async with self.session.put(index_url, json=mapping) as response:
                if response.status in [200, 201]:
                    logger.info(f"Created Elasticsearch index: {index_name}")
                else:
                    logger.warning(f"Index creation response: {response.status}")
                    
        except Exception as e:
            logger.error(f"Failed to create Elasticsearch index: {e}")
    
    async def log(self, level: str, message: str, trace_id: str = None, 
                 span_id: str = None, user_id: str = None, request_id: str = None,
                 extra: Dict[str, Any] = None):
        """ë¡œê·¸ ê¸°ë¡"""
        log_entry = LogEntry(
            timestamp=datetime.now(),
            level=level,
            service=self.service_name,
            message=message,
            trace_id=trace_id,
            span_id=span_id,
            user_id=user_id,
            request_id=request_id,
            extra=extra or {}
        )
        
        # ë²„í¼ì— ì¶”ê°€
        self.log_buffer.append(log_entry)
        
        # ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ì¦‰ì‹œ ì „ì†¡
        if len(self.log_buffer) >= self.buffer_size:
            await self._flush_buffer()
    
    async def _flush_buffer(self):
        """ë²„í¼ í”ŒëŸ¬ì‹œ"""
        if not self.log_buffer:
            return
        
        try:
            # ë¡œê·¸ ë°ì´í„° ì¤€ë¹„
            logs_data = []
            for entry in self.log_buffer:
                log_doc = {
                    "timestamp": entry.timestamp.isoformat(),
                    "level": entry.level,
                    "service": entry.service,
                    "message": entry.message
                }
                
                if entry.trace_id:
                    log_doc["trace_id"] = entry.trace_id
                if entry.span_id:
                    log_doc["span_id"] = entry.span_id
                if entry.user_id:
                    log_doc["user_id"] = entry.user_id
                if entry.request_id:
                    log_doc["request_id"] = entry.request_id
                if entry.extra:
                    log_doc["extra"] = entry.extra
                
                logs_data.append(log_doc)
            
            # Elasticsearchì— ì „ì†¡
            index_name = f"logs-{datetime.now().strftime('%Y.%m.%d')}"
            bulk_url = f"{self.elasticsearch_url}/{index_name}/_bulk"
            
            # Bulk API í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„
            bulk_data = ""
            for log_doc in logs_data:
                bulk_data += json.dumps({"index": {}}) + "\n"
                bulk_data += json.dumps(log_doc) + "\n"
            
            async with self.session.post(
                bulk_url,
                data=bulk_data,
                headers={"Content-Type": "application/x-ndjson"}
            ) as response:
                if response.status == 200:
                    logger.debug(f"Flushed {len(logs_data)} log entries to Elasticsearch")
                else:
                    logger.error(f"Failed to flush logs: {response.status}")
            
            # ë²„í¼ í´ë¦¬ì–´
            self.log_buffer.clear()
            
        except Exception as e:
            logger.error(f"Failed to flush log buffer: {e}")
    
    async def start_flush_loop(self):
        """í”ŒëŸ¬ì‹œ ë£¨í”„ ì‹œì‘"""
        while True:
            try:
                await self._flush_buffer()
                await asyncio.sleep(self.flush_interval)
                
            except Exception as e:
                logger.error(f"Flush loop error: {e}")
                await asyncio.sleep(self.flush_interval)
    
    async def search_logs(self, query: str, start_time: datetime = None, 
                         end_time: datetime = None, level: str = None,
                         service: str = None, limit: int = 100) -> List[Dict[str, Any]]:
        """ë¡œê·¸ ê²€ìƒ‰"""
        try:
            index_name = f"logs-{datetime.now().strftime('%Y.%m.%d')}"
            search_url = f"{self.elasticsearch_url}/{index_name}/_search"
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_query = {
                "query": {
                    "bool": {
                        "must": [
                            {"query_string": {"query": query}}
                        ]
                    }
                },
                "sort": [{"timestamp": {"order": "desc"}}],
                "size": limit
            }
            
            # í•„í„° ì¶”ê°€
            filters = []
            
            if start_time and end_time:
                filters.append({
                    "range": {
                        "timestamp": {
                            "gte": start_time.isoformat(),
                            "lte": end_time.isoformat()
                        }
                    }
                })
            
            if level:
                filters.append({"term": {"level": level}})
            
            if service:
                filters.append({"term": {"service": service}})
            
            if filters:
                search_query["query"]["bool"]["filter"] = filters
            
            async with self.session.post(search_url, json=search_query) as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get("hits", {}).get("hits", [])
                else:
                    logger.error(f"Log search failed: {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"Failed to search logs: {e}")
            return []
    
    async def close(self):
        """ì¢…ë£Œ"""
        await self._flush_buffer()
        if self.session:
            await self.session.close()
            logger.info("Log collector closed")
```

## ğŸ”§ **AlertManager ì•Œë¦¼ ì‹œìŠ¤í…œ**

### ğŸ“¦ **AlertManager ì„¤ì •**

```yaml
# monitoring/alerting/alertmanager/alertmanager-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/YOUR_SLACK_WEBHOOK'
    
    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'slack-notifications'
      routes:
      - match:
          severity: critical
        receiver: 'pager-duty-critical'
        continue: true
      - match:
          severity: warning
        receiver: 'slack-notifications'
    
    receivers:
    - name: 'slack-notifications'
      slack_configs:
      - channel: '#trading-alerts'
        title: '{{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        send_resolved: true
    
    - name: 'pager-duty-critical'
      pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_KEY'
        description: '{{ template "pagerduty.description" . }}'
        severity: '{{ if eq .CommonLabels.severity "critical" }}critical{{ else }}warning{{ end }}'
    
    templates:
    - '/etc/alertmanager/template/*.tmpl'

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.25.0
        ports:
        - containerPort: 9093
          name: http
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager
        command:
        - /bin/alertmanager
        - --config.file=/etc/alertmanager/alertmanager.yml
        - --storage.path=/alertmanager
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: alertmanager-config
        configMap:
          name: alertmanager-config

---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  selector:
    app: alertmanager
  ports:
  - port: 9093
    targetPort: 9093
    name: http
  type: ClusterIP
```

### ğŸ“¦ **ì•Œë¦¼ ê´€ë¦¬ì**

```python
# monitoring/alerting/alertmanager/alert_manager.py
import asyncio
import aiohttp
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

@dataclass
class Alert:
    """ì•Œë¦¼"""
    id: str
    name: str
    severity: str  # info, warning, critical
    message: str
    service: str
    metric: str
    value: float
    threshold: float
    timestamp: datetime
    resolved: bool = False
    resolved_at: Optional[datetime] = None

class AlertManager:
    """ì•Œë¦¼ ê´€ë¦¬ì"""
    
    def __init__(self, alertmanager_url: str = "http://alertmanager:9093"):
        self.alertmanager_url = alertmanager_url
        self.session = None
        self.active_alerts = {}
        self.alert_history = []
        
        logger.info("Initialized alert manager")
    
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        self.session = aiohttp.ClientSession()
    
    async def create_alert(self, name: str, severity: str, message: str,
                          service: str, metric: str, value: float, threshold: float) -> str:
        """ì•Œë¦¼ ìƒì„±"""
        alert_id = f"{service}_{metric}_{int(datetime.now().timestamp())}"
        
        alert = Alert(
            id=alert_id,
            name=name,
            severity=severity,
            message=message,
            service=service,
            metric=metric,
            value=value,
            threshold=threshold,
            timestamp=datetime.now()
        )
        
        # í™œì„± ì•Œë¦¼ì— ì¶”ê°€
        self.active_alerts[alert_id] = alert
        
        # AlertManagerì— ì „ì†¡
        await self._send_to_alertmanager(alert)
        
        logger.info(f"Created alert: {alert_id} ({severity})")
        return alert_id
    
    async def resolve_alert(self, alert_id: str):
        """ì•Œë¦¼ í•´ê²°"""
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            alert.resolved = True
            alert.resolved_at = datetime.now()
            
            # ì•Œë¦¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.alert_history.append(alert)
            
            # í™œì„± ì•Œë¦¼ì—ì„œ ì œê±°
            del self.active_alerts[alert_id]
            
            # AlertManagerì— í•´ê²° ì•Œë¦¼ ì „ì†¡
            await self._send_resolution_to_alertmanager(alert)
            
            logger.info(f"Resolved alert: {alert_id}")
    
    async def _send_to_alertmanager(self, alert: Alert):
        """AlertManagerì— ì•Œë¦¼ ì „ì†¡"""
        try:
            alert_data = {
                "alerts": [
                    {
                        "labels": {
                            "alertname": alert.name,
                            "severity": alert.severity,
                            "service": alert.service,
                            "metric": alert.metric
                        },
                        "annotations": {
                            "description": alert.message,
                            "value": str(alert.value),
                            "threshold": str(alert.threshold)
                        },
                        "startsAt": alert.timestamp.isoformat() + "Z"
                    }
                ]
            }
            
            async with self.session.post(
                f"{self.alertmanager_url}/api/v1/alerts",
                json=alert_data
            ) as response:
                if response.status == 200:
                    logger.debug(f"Alert sent to AlertManager: {alert.id}")
                else:
                    logger.error(f"Failed to send alert to AlertManager: {response.status}")
                    
        except Exception as e:
            logger.error(f"Failed to send alert to AlertManager: {e}")
    
    async def _send_resolution_to_alertmanager(self, alert: Alert):
        """AlertManagerì— í•´ê²° ì•Œë¦¼ ì „ì†¡"""
        try:
            alert_data = {
                "alerts": [
                    {
                        "labels": {
                            "alertname": alert.name,
                            "severity": alert.severity,
                            "service": alert.service,
                            "metric": alert.metric
                        },
                        "annotations": {
                            "description": f"Alert resolved: {alert.message}",
                            "value": str(alert.value),
                            "threshold": str(alert.threshold)
                        },
                        "startsAt": alert.timestamp.isoformat() + "Z",
                        "endsAt": alert.resolved_at.isoformat() + "Z"
                    }
                ]
            }
            
            async with self.session.post(
                f"{self.alertmanager_url}/api/v1/alerts",
                json=alert_data
            ) as response:
                if response.status == 200:
                    logger.debug(f"Resolution sent to AlertManager: {alert.id}")
                else:
                    logger.error(f"Failed to send resolution to AlertManager: {response.status}")
                    
        except Exception as e:
            logger.error(f"Failed to send resolution to AlertManager: {e}")
    
    def get_active_alerts(self) -> List[Alert]:
        """í™œì„± ì•Œë¦¼ ì¡°íšŒ"""
        return list(self.active_alerts.values())
    
    def get_alert_history(self, hours: int = 24) -> List[Alert]:
        """ì•Œë¦¼ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        return [
            alert for alert in self.alert_history
            if alert.timestamp > cutoff_time
        ]
    
    def get_alert_stats(self) -> Dict[str, Any]:
        """ì•Œë¦¼ í†µê³„"""
        active_alerts = self.get_active_alerts()
        recent_history = self.get_alert_history(24)
        
        return {
            "active_alerts": {
                "total": len(active_alerts),
                "critical": len([a for a in active_alerts if a.severity == "critical"]),
                "warning": len([a for a in active_alerts if a.severity == "warning"]),
                "info": len([a for a in active_alerts if a.severity == "info"])
            },
            "recent_alerts": {
                "total": len(recent_history),
                "resolved": len([a for a in recent_history if a.resolved]),
                "unresolved": len([a for a in recent_history if not a.resolved])
            }
        }
    
    async def close(self):
        """ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
            logger.info("Alert manager closed")
```

## ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„**

### ğŸ“‹ **ì™„ë£Œëœ ì‘ì—…**
- âœ… Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- âœ… Jaeger ë¶„ì‚° ì¶”ì 
- âœ… Elasticsearch ë¡œê·¸ ì§‘ê³„
- âœ… AlertManager ì•Œë¦¼ ì‹œìŠ¤í…œ
- âœ… ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
- âœ… ë¡œê·¸ ìˆ˜ì§‘ê¸°
- âœ… ì•Œë¦¼ ê´€ë¦¬ì

### ğŸ”„ **ì§„í–‰ ì¤‘ì¸ ì‘ì—…**
- ğŸ”„ Grafana ëŒ€ì‹œë³´ë“œ
- ğŸ”„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ğŸ”„ ì„±ëŠ¥ ìµœì í™”

### â³ **ë‹¤ìŒ ë‹¨ê³„**
1. **Phase 2.5 ì¥ì•  ë³µêµ¬** ë¬¸ì„œ ìƒì„±
2. **Phase 2.6 ì„±ëŠ¥ ìµœì í™”** ë¬¸ì„œ ìƒì„±
3. **Phase 2 ì™„ë£Œ**: Phase 2ì˜ 100% ì™„ì„±

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-01-31
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: 2024-02-01 (Phase 2.5 ì¥ì•  ë³µêµ¬)
**ëª¨ë‹ˆí„°ë§ ëª©í‘œ**: < 1ì´ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘, < 5ì´ˆ ë¡œê·¸ ìˆ˜ì§‘, < 100ms ì¶”ì 
**ì•Œë¦¼ ì‹œìŠ¤í…œ**: < 30ì´ˆ ì•Œë¦¼ ë°œì†¡, ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ 