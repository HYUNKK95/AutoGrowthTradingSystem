# ğŸŒŠ Phase 5.3: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë° ê³ ì„±ëŠ¥ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ

## ğŸ“‹ **ê°œìš”**

### ğŸ¯ **ëª©í‘œ**
- **ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°**: Kafka, Flink, Spark Streaming ê¸°ë°˜ ê³ ì„±ëŠ¥ ìŠ¤íŠ¸ë¦¬ë°
- **ì €ì§€ì—° ì²˜ë¦¬**: < 1ms ì§€ì—°ì‹œê°„, ì´ˆë‹¹ 100ë§Œ ì´ë²¤íŠ¸ ì²˜ë¦¬
- **ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì—”ì§„**: ìƒíƒœ ê´€ë¦¬, ìœˆë„ìš° ì²˜ë¦¬, íŒ¨í„´ ë§¤ì¹­
- **ì‹¤ì‹œê°„ ë¶„ì„**: ì‹¤ì‹œê°„ ì§‘ê³„, ì´ìƒ íƒì§€, ì•Œë¦¼ ì‹œìŠ¤í…œ
- **í™•ì¥ì„±**: ìˆ˜í‰ í™•ì¥, íŒŒí‹°ì…”ë‹, ë¡œë“œ ë°¸ëŸ°ì‹±

### ğŸ“Š **ì„±ëŠ¥ ëª©í‘œ**
- **ì²˜ë¦¬ ì§€ì—°**: < 1ms ì´ë²¤íŠ¸ ì²˜ë¦¬ ì§€ì—°
- **ì²˜ë¦¬ëŸ‰**: ì´ˆë‹¹ 100ë§Œ ì´ë²¤íŠ¸ ì²˜ë¦¬
- **ê°€ìš©ì„±**: 99.99% ì‹œìŠ¤í…œ ê°€ìš©ì„±
- **í™•ì¥ì„±**: ì„ í˜• í™•ì¥ (ë…¸ë“œ ì¶”ê°€ ì‹œ ì²˜ë¦¬ëŸ‰ ë¹„ë¡€ ì¦ê°€)
- **ë‚´ê²°í•¨ì„±**: ìë™ ë³µêµ¬, ì²´í¬í¬ì¸íŠ¸, ë°±ì—…

## ğŸ—ï¸ **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

### ğŸ“ **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ êµ¬ì¡°**
```
realtime-streaming/
â”œâ”€â”€ streaming-engine/                  # ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„
â”‚   â”œâ”€â”€ kafka-streams/                 # Kafka Streams
â”‚   â”œâ”€â”€ apache-flink/                  # Apache Flink
â”‚   â”œâ”€â”€ spark-streaming/               # Spark Streaming
â”‚   â””â”€â”€ custom-streaming/              # ì»¤ìŠ¤í…€ ìŠ¤íŠ¸ë¦¬ë°
â”œâ”€â”€ data-ingestion/                    # ë°ì´í„° ìˆ˜ì§‘
â”‚   â”œâ”€â”€ real-time-collectors/          # ì‹¤ì‹œê°„ ìˆ˜ì§‘ê¸°
â”‚   â”œâ”€â”€ data-transformers/             # ë°ì´í„° ë³€í™˜ê¸°
â”‚   â”œâ”€â”€ schema-registry/               # ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬
â”‚   â””â”€â”€ data-validation/               # ë°ì´í„° ê²€ì¦
â”œâ”€â”€ stream-processing/                 # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
â”‚   â”œâ”€â”€ window-operations/             # ìœˆë„ìš° ì—°ì‚°
â”‚   â”œâ”€â”€ pattern-matching/              # íŒ¨í„´ ë§¤ì¹­
â”‚   â”œâ”€â”€ state-management/              # ìƒíƒœ ê´€ë¦¬
â”‚   â””â”€â”€ aggregations/                  # ì§‘ê³„ ì—°ì‚°
â”œâ”€â”€ real-time-analytics/               # ì‹¤ì‹œê°„ ë¶„ì„
â”‚   â”œâ”€â”€ anomaly-detection/             # ì´ìƒ íƒì§€
â”‚   â”œâ”€â”€ real-time-alerts/              # ì‹¤ì‹œê°„ ì•Œë¦¼
â”‚   â”œâ”€â”€ trend-analysis/                # íŠ¸ë Œë“œ ë¶„ì„
â”‚   â””â”€â”€ predictive-analytics/          # ì˜ˆì¸¡ ë¶„ì„
â””â”€â”€ monitoring/                        # ëª¨ë‹ˆí„°ë§
    â”œâ”€â”€ performance-monitoring/        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    â”œâ”€â”€ health-checks/                 # í—¬ìŠ¤ ì²´í¬
    â”œâ”€â”€ metrics-collection/            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    â””â”€â”€ alerting/                      # ì•Œë¦¼ ì‹œìŠ¤í…œ
```

## ğŸ”§ **Kafka Streams ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„**

### ğŸ“¦ **ê³ ì„±ëŠ¥ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**

```python
# realtime-streaming/streaming-engine/kafka_streams_engine.py
import asyncio
import time
import json
import logging
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
from datetime import datetime, timedelta
from kafka import KafkaProducer, KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError
import numpy as np
from collections import defaultdict, deque
import threading
import queue

logger = logging.getLogger(__name__)

@dataclass
class StreamConfig:
    """ìŠ¤íŠ¸ë¦¼ ì„¤ì •"""
    bootstrap_servers: List[str]
    topic_name: str
    partition_count: int = 10
    replication_factor: int = 3
    retention_ms: int = 86400000  # 24ì‹œê°„
    max_message_size: int = 1048576  # 1MB

@dataclass
class ProcessingConfig:
    """ì²˜ë¦¬ ì„¤ì •"""
    batch_size: int = 1000
    batch_timeout_ms: int = 100
    max_poll_records: int = 500
    auto_offset_reset: str = 'latest'
    enable_auto_commit: bool = False

@dataclass
class StreamEvent:
    """ìŠ¤íŠ¸ë¦¼ ì´ë²¤íŠ¸"""
    event_id: str
    timestamp: datetime
    event_type: str
    data: Dict[str, Any]
    source: str
    partition: int
    offset: int

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""
    event_id: str
    processing_time_ms: float
    result: Dict[str, Any]
    status: str  # 'success', 'error', 'filtered'
    error_message: Optional[str] = None

class KafkaStreamsEngine:
    """Kafka Streams ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„"""
    
    def __init__(self, stream_config: StreamConfig, processing_config: ProcessingConfig):
        self.stream_config = stream_config
        self.processing_config = processing_config
        
        # Kafka í”„ë¡œë“€ì„œ/ì»¨ìŠˆë¨¸
        self.producer = None
        self.consumer = None
        
        # ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        self.processors = []
        self.filters = []
        self.transformers = []
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.metrics = StreamMetrics()
        
        # ìƒíƒœ ê´€ë¦¬
        self.state_store = StateStore()
        
        # ìœˆë„ìš° ê´€ë¦¬
        self.window_manager = WindowManager()
        
        # íŒ¨í„´ ë§¤ì¹­
        self.pattern_matcher = PatternMatcher()
        
        # ìŠ¤ë ˆë“œ ê´€ë¦¬
        self.processing_thread = None
        self.running = False
        
        # ì´ë²¤íŠ¸ í
        self.event_queue = queue.Queue(maxsize=10000)
        
        logger.info("Kafka Streams Engine initialized")
    
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        try:
            # í† í”½ ìƒì„±
            await self._create_topic()
            
            # í”„ë¡œë“€ì„œ ì´ˆê¸°í™”
            self.producer = KafkaProducer(
                bootstrap_servers=self.stream_config.bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                key_serializer=lambda k: k.encode('utf-8') if k else None,
                acks='all',
                retries=3,
                max_in_flight_requests_per_connection=1
            )
            
            # ì»¨ìŠˆë¨¸ ì´ˆê¸°í™”
            self.consumer = KafkaConsumer(
                self.stream_config.topic_name,
                bootstrap_servers=self.stream_config.bootstrap_servers,
                group_id=f'streaming-group-{int(time.time())}',
                auto_offset_reset=self.processing_config.auto_offset_reset,
                enable_auto_commit=self.processing_config.enable_auto_commit,
                max_poll_records=self.processing_config.max_poll_records,
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                key_deserializer=lambda k: k.decode('utf-8') if k else None
            )
            
            logger.info("Kafka Streams Engine initialized successfully")
            
        except Exception as e:
            logger.error(f"Kafka Streams Engine initialization failed: {e}")
            raise
    
    async def _create_topic(self):
        """í† í”½ ìƒì„±"""
        try:
            admin_client = KafkaAdminClient(
                bootstrap_servers=self.stream_config.bootstrap_servers
            )
            
            topic = NewTopic(
                name=self.stream_config.topic_name,
                num_partitions=self.stream_config.partition_count,
                replication_factor=self.stream_config.replication_factor,
                topic_configs={
                    'retention.ms': str(self.stream_config.retention_ms),
                    'max.message.bytes': str(self.stream_config.max_message_size)
                }
            )
            
            admin_client.create_topics([topic])
            logger.info(f"Topic {self.stream_config.topic_name} created successfully")
            
        except TopicAlreadyExistsError:
            logger.info(f"Topic {self.stream_config.topic_name} already exists")
        except Exception as e:
            logger.error(f"Topic creation failed: {e}")
            raise
    
    def add_processor(self, processor: Callable[[StreamEvent], ProcessingResult]):
        """í”„ë¡œì„¸ì„œ ì¶”ê°€"""
        self.processors.append(processor)
        logger.info(f"Processor added: {processor.__name__}")
    
    def add_filter(self, filter_func: Callable[[StreamEvent], bool]):
        """í•„í„° ì¶”ê°€"""
        self.filters.append(filter_func)
        logger.info(f"Filter added: {filter_func.__name__}")
    
    def add_transformer(self, transformer: Callable[[StreamEvent], StreamEvent]):
        """ë³€í™˜ê¸° ì¶”ê°€"""
        self.transformers.append(transformer)
        logger.info(f"Transformer added: {transformer.__name__}")
    
    async def start_processing(self):
        """ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹œì‘"""
        if self.running:
            logger.warning("Stream processing is already running")
            return
        
        self.running = True
        self.processing_thread = threading.Thread(target=self._processing_loop)
        self.processing_thread.start()
        
        logger.info("Stream processing started")
    
    async def stop_processing(self):
        """ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ì§€"""
        self.running = False
        
        if self.processing_thread:
            self.processing_thread.join()
        
        if self.consumer:
            self.consumer.close()
        
        if self.producer:
            self.producer.close()
        
        logger.info("Stream processing stopped")
    
    def _processing_loop(self):
        """ì²˜ë¦¬ ë£¨í”„"""
        while self.running:
            try:
                # ë©”ì‹œì§€ í´ë§
                messages = self.consumer.poll(
                    timeout_ms=self.processing_config.batch_timeout_ms,
                    max_records=self.processing_config.batch_size
                )
                
                for tp, records in messages.items():
                    for record in records:
                        # ì´ë²¤íŠ¸ ìƒì„±
                        event = StreamEvent(
                            event_id=f"{record.topic}_{record.partition}_{record.offset}",
                            timestamp=datetime.fromtimestamp(record.timestamp / 1000),
                            event_type=record.value.get('event_type', 'unknown'),
                            data=record.value.get('data', {}),
                            source=record.value.get('source', 'unknown'),
                            partition=record.partition,
                            offset=record.offset
                        )
                        
                        # ì´ë²¤íŠ¸ íì— ì¶”ê°€
                        try:
                            self.event_queue.put_nowait(event)
                        except queue.Full:
                            logger.warning("Event queue is full, dropping event")
                
                # ì´ë²¤íŠ¸ ì²˜ë¦¬
                self._process_events()
                
            except Exception as e:
                logger.error(f"Processing loop error: {e}")
                time.sleep(1)
    
    def _process_events(self):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        processed_count = 0
        
        while not self.event_queue.empty() and processed_count < self.processing_config.batch_size:
            try:
                event = self.event_queue.get_nowait()
                start_time = time.time()
                
                # í•„í„°ë§
                if not self._apply_filters(event):
                    continue
                
                # ë³€í™˜
                transformed_event = self._apply_transformers(event)
                
                # ì²˜ë¦¬
                result = self._apply_processors(transformed_event)
                
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                processing_time = (time.time() - start_time) * 1000
                self.metrics.record_processing_time(processing_time)
                self.metrics.record_event_processed()
                
                # ìƒíƒœ ì €ì¥
                self.state_store.update_state(event, result)
                
                # ìœˆë„ìš° ì²˜ë¦¬
                self.window_manager.add_event(event)
                
                # íŒ¨í„´ ë§¤ì¹­
                patterns = self.pattern_matcher.match_patterns(event)
                
                processed_count += 1
                
            except queue.Empty:
                break
            except Exception as e:
                logger.error(f"Event processing error: {e}")
                self.metrics.record_error()
    
    def _apply_filters(self, event: StreamEvent) -> bool:
        """í•„í„° ì ìš©"""
        for filter_func in self.filters:
            if not filter_func(event):
                return False
        return True
    
    def _apply_transformers(self, event: StreamEvent) -> StreamEvent:
        """ë³€í™˜ê¸° ì ìš©"""
        transformed_event = event
        for transformer in self.transformers:
            transformed_event = transformer(transformed_event)
        return transformed_event
    
    def _apply_processors(self, event: StreamEvent) -> ProcessingResult:
        """í”„ë¡œì„¸ì„œ ì ìš©"""
        start_time = time.time()
        
        try:
            result_data = {}
            
            for processor in self.processors:
                processor_result = processor(event)
                result_data.update(processor_result.result)
            
            processing_time = (time.time() - start_time) * 1000
            
            return ProcessingResult(
                event_id=event.event_id,
                processing_time_ms=processing_time,
                result=result_data,
                status='success'
            )
            
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            
            return ProcessingResult(
                event_id=event.event_id,
                processing_time_ms=processing_time,
                result={},
                status='error',
                error_message=str(e)
            )
    
    async def publish_event(self, event_type: str, data: Dict[str, Any], 
                          key: Optional[str] = None) -> bool:
        """ì´ë²¤íŠ¸ ë°œí–‰"""
        try:
            event = {
                'event_type': event_type,
                'data': data,
                'timestamp': int(time.time() * 1000),
                'source': 'streaming-engine'
            }
            
            future = self.producer.send(
                self.stream_config.topic_name,
                value=event,
                key=key
            )
            
            # ë¹„ë™ê¸° ì „ì†¡ í™•ì¸
            record_metadata = future.get(timeout=10)
            
            logger.debug(f"Event published: {event_type} to partition {record_metadata.partition}")
            return True
            
        except Exception as e:
            logger.error(f"Event publishing failed: {e}")
            return False
    
    def get_metrics(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return self.metrics.get_metrics()
    
    def get_state(self, key: str) -> Optional[Any]:
        """ìƒíƒœ ì¡°íšŒ"""
        return self.state_store.get_state(key)
    
    def get_window_data(self, window_type: str, window_size: int) -> List[StreamEvent]:
        """ìœˆë„ìš° ë°ì´í„° ì¡°íšŒ"""
        return self.window_manager.get_window_data(window_type, window_size)

class StreamMetrics:
    """ìŠ¤íŠ¸ë¦¼ ë©”íŠ¸ë¦­"""
    
    def __init__(self):
        self.processing_times = deque(maxlen=1000)
        self.events_processed = 0
        self.errors_count = 0
        self.start_time = time.time()
    
    def record_processing_time(self, processing_time_ms: float):
        """ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡"""
        self.processing_times.append(processing_time_ms)
    
    def record_event_processed(self):
        """ì²˜ë¦¬ëœ ì´ë²¤íŠ¸ ê¸°ë¡"""
        self.events_processed += 1
    
    def record_error(self):
        """ì˜¤ë¥˜ ê¸°ë¡"""
        self.errors_count += 1
    
    def get_metrics(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        if not self.processing_times:
            return {
                'avg_processing_time_ms': 0.0,
                'max_processing_time_ms': 0.0,
                'min_processing_time_ms': 0.0,
                'events_processed': self.events_processed,
                'errors_count': self.errors_count,
                'uptime_seconds': time.time() - self.start_time,
                'events_per_second': 0.0
            }
        
        uptime = time.time() - self.start_time
        
        return {
            'avg_processing_time_ms': np.mean(self.processing_times),
            'max_processing_time_ms': np.max(self.processing_times),
            'min_processing_time_ms': np.min(self.processing_times),
            'events_processed': self.events_processed,
            'errors_count': self.errors_count,
            'uptime_seconds': uptime,
            'events_per_second': self.events_processed / uptime if uptime > 0 else 0.0
        }

class StateStore:
    """ìƒíƒœ ì €ì¥ì†Œ"""
    
    def __init__(self):
        self.state = defaultdict(dict)
        self.lock = threading.Lock()
    
    def update_state(self, event: StreamEvent, result: ProcessingResult):
        """ìƒíƒœ ì—…ë°ì´íŠ¸"""
        with self.lock:
            key = f"{event.event_type}_{event.source}"
            self.state[key].update({
                'last_event': event.event_id,
                'last_timestamp': event.timestamp.isoformat(),
                'last_result': result.result,
                'processing_count': self.state[key].get('processing_count', 0) + 1
            })
    
    def get_state(self, key: str) -> Optional[Any]:
        """ìƒíƒœ ì¡°íšŒ"""
        with self.lock:
            return self.state.get(key)
    
    def clear_state(self, key: str):
        """ìƒíƒœ ì‚­ì œ"""
        with self.lock:
            if key in self.state:
                del self.state[key]

class WindowManager:
    """ìœˆë„ìš° ê´€ë¦¬ì"""
    
    def __init__(self):
        self.windows = defaultdict(lambda: deque(maxlen=10000))
        self.lock = threading.Lock()
    
    def add_event(self, event: StreamEvent):
        """ì´ë²¤íŠ¸ ì¶”ê°€"""
        with self.lock:
            # ì‹œê°„ ê¸°ë°˜ ìœˆë„ìš°
            time_window_key = f"time_{int(event.timestamp.timestamp() // 60)}"  # 1ë¶„ ìœˆë„ìš°
            self.windows[time_window_key].append(event)
            
            # ì´ë²¤íŠ¸ íƒ€ì… ê¸°ë°˜ ìœˆë„ìš°
            type_window_key = f"type_{event.event_type}"
            self.windows[type_window_key].append(event)
    
    def get_window_data(self, window_type: str, window_size: int) -> List[StreamEvent]:
        """ìœˆë„ìš° ë°ì´í„° ì¡°íšŒ"""
        with self.lock:
            if window_type == 'time':
                # ìµœê·¼ Në¶„ ë°ì´í„°
                current_time = int(time.time() // 60)
                events = []
                for i in range(window_size):
                    window_key = f"time_{current_time - i}"
                    events.extend(list(self.windows[window_key]))
                return events
            elif window_type == 'count':
                # ìµœê·¼ Nê°œ ì´ë²¤íŠ¸
                all_events = []
                for window_events in self.windows.values():
                    all_events.extend(list(window_events))
                return sorted(all_events, key=lambda x: x.timestamp)[-window_size:]
            else:
                return list(self.windows.get(window_type, []))

class PatternMatcher:
    """íŒ¨í„´ ë§¤ì²˜"""
    
    def __init__(self):
        self.patterns = []
        self.event_history = deque(maxlen=1000)
    
    def add_pattern(self, pattern: Dict[str, Any]):
        """íŒ¨í„´ ì¶”ê°€"""
        self.patterns.append(pattern)
    
    def match_patterns(self, event: StreamEvent) -> List[Dict[str, Any]]:
        """íŒ¨í„´ ë§¤ì¹­"""
        self.event_history.append(event)
        matched_patterns = []
        
        for pattern in self.patterns:
            if self._match_pattern(event, pattern):
                matched_patterns.append(pattern)
        
        return matched_patterns
    
    def _match_pattern(self, event: StreamEvent, pattern: Dict[str, Any]) -> bool:
        """ê°œë³„ íŒ¨í„´ ë§¤ì¹­"""
        pattern_type = pattern.get('type', 'simple')
        
        if pattern_type == 'simple':
            return self._match_simple_pattern(event, pattern)
        elif pattern_type == 'sequence':
            return self._match_sequence_pattern(event, pattern)
        elif pattern_type == 'frequency':
            return self._match_frequency_pattern(event, pattern)
        
        return False
    
    def _match_simple_pattern(self, event: StreamEvent, pattern: Dict[str, Any]) -> bool:
        """ë‹¨ìˆœ íŒ¨í„´ ë§¤ì¹­"""
        conditions = pattern.get('conditions', {})
        
        for field, expected_value in conditions.items():
            if field == 'event_type' and event.event_type != expected_value:
                return False
            elif field == 'source' and event.source != expected_value:
                return False
            elif field in event.data and event.data[field] != expected_value:
                return False
        
        return True
    
    def _match_sequence_pattern(self, event: StreamEvent, pattern: Dict[str, Any]) -> bool:
        """ì‹œí€€ìŠ¤ íŒ¨í„´ ë§¤ì¹­"""
        sequence = pattern.get('sequence', [])
        window_size = pattern.get('window_size', 10)
        
        # ìµœê·¼ ì´ë²¤íŠ¸ë“¤ì—ì„œ ì‹œí€€ìŠ¤ ì°¾ê¸°
        recent_events = list(self.event_history)[-window_size:]
        
        if len(recent_events) < len(sequence):
            return False
        
        # ì‹œí€€ìŠ¤ ë§¤ì¹­
        for i in range(len(recent_events) - len(sequence) + 1):
            match = True
            for j, expected_event in enumerate(sequence):
                if recent_events[i + j].event_type != expected_event:
                    match = False
                    break
            if match:
                return True
        
        return False
    
    def _match_frequency_pattern(self, event: StreamEvent, pattern: Dict[str, Any]) -> bool:
        """ë¹ˆë„ íŒ¨í„´ ë§¤ì¹­"""
        event_type = pattern.get('event_type')
        min_frequency = pattern.get('min_frequency', 1)
        window_size = pattern.get('window_size', 10)
        
        if event.event_type != event_type:
            return False
        
        # ìµœê·¼ ìœˆë„ìš°ì—ì„œ ë¹ˆë„ ê³„ì‚°
        recent_events = list(self.event_history)[-window_size:]
        frequency = sum(1 for e in recent_events if e.event_type == event_type)
        
        return frequency >= min_frequency
```

## ğŸ”§ **ì‹¤ì‹œê°„ ë¶„ì„ ì‹œìŠ¤í…œ**

### ğŸ“¦ **ì´ìƒ íƒì§€ ë° ì•Œë¦¼**

```python
# realtime-streaming/real-time-analytics/anomaly_detection.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import threading
import time

logger = logging.getLogger(__name__)

@dataclass
class AnomalyConfig:
    """ì´ìƒ íƒì§€ ì„¤ì •"""
    window_size: int = 100
    threshold: float = 0.95
    update_interval: int = 60  # ì´ˆ
    min_events: int = 10

@dataclass
class AnomalyEvent:
    """ì´ìƒ ì´ë²¤íŠ¸"""
    event_id: str
    timestamp: datetime
    anomaly_score: float
    anomaly_type: str
    severity: str  # 'low', 'medium', 'high', 'critical'
    description: str
    data: Dict[str, Any]

class AnomalyDetector:
    """ì´ìƒ íƒì§€ê¸°"""
    
    def __init__(self, config: AnomalyConfig):
        self.config = config
        
        # ì´ìƒ íƒì§€ ëª¨ë¸
        self.isolation_forest = IsolationForest(
            contamination=0.1,
            random_state=42
        )
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        self.scaler = StandardScaler()
        
        # ë°ì´í„° ë²„í¼
        self.data_buffer = deque(maxlen=config.window_size)
        
        # ì´ìƒ ì´ë²¤íŠ¸ ì €ì¥
        self.anomaly_events = []
        
        # ëª¨ë¸ ìƒíƒœ
        self.model_trained = False
        self.last_update = datetime.now()
        
        # ìŠ¤ë ˆë“œ ì•ˆì „
        self.lock = threading.Lock()
        
        # ì•Œë¦¼ ì‹œìŠ¤í…œ
        self.alert_system = AlertSystem()
        
        logger.info("Anomaly detector initialized")
    
    def add_event(self, event_data: Dict[str, Any]) -> Optional[AnomalyEvent]:
        """ì´ë²¤íŠ¸ ì¶”ê°€ ë° ì´ìƒ íƒì§€"""
        with self.lock:
            # ë°ì´í„° ë²„í¼ì— ì¶”ê°€
            self.data_buffer.append(event_data)
            
            # ìµœì†Œ ì´ë²¤íŠ¸ ìˆ˜ í™•ì¸
            if len(self.data_buffer) < self.config.min_events:
                return None
            
            # ëª¨ë¸ ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ í™•ì¸
            if self._should_update_model():
                self._update_model()
            
            # ì´ìƒ íƒì§€
            if self.model_trained:
                anomaly_event = self._detect_anomaly(event_data)
                if anomaly_event:
                    self.anomaly_events.append(anomaly_event)
                    self.alert_system.send_alert(anomaly_event)
                return anomaly_event
            
            return None
    
    def _should_update_model(self) -> bool:
        """ëª¨ë¸ ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€"""
        time_since_update = (datetime.now() - self.last_update).total_seconds()
        return time_since_update >= self.config.update_interval
    
    def _update_model(self):
        """ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        try:
            # ë°ì´í„° ì¤€ë¹„
            data_array = np.array(list(self.data_buffer))
            
            # íŠ¹ì„± ì„ íƒ (ìˆ«ìí˜• ë°ì´í„°ë§Œ)
            numeric_features = []
            for event in self.data_buffer:
                numeric_data = {}
                for key, value in event.items():
                    if isinstance(value, (int, float)):
                        numeric_data[key] = value
                numeric_features.append(numeric_data)
            
            if len(numeric_features) < self.config.min_events:
                return
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(numeric_features)
            
            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            df = df.fillna(df.mean())
            
            # ìŠ¤ì¼€ì¼ë§
            scaled_data = self.scaler.fit_transform(df)
            
            # ëª¨ë¸ í›ˆë ¨
            self.isolation_forest.fit(scaled_data)
            self.model_trained = True
            self.last_update = datetime.now()
            
            logger.info("Anomaly detection model updated")
            
        except Exception as e:
            logger.error(f"Model update failed: {e}")
    
    def _detect_anomaly(self, event_data: Dict[str, Any]) -> Optional[AnomalyEvent]:
        """ì´ìƒ íƒì§€"""
        try:
            # ìˆ«ìí˜• íŠ¹ì„± ì¶”ì¶œ
            numeric_data = {}
            for key, value in event_data.items():
                if isinstance(value, (int, float)):
                    numeric_data[key] = value
            
            if not numeric_data:
                return None
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame([numeric_data])
            
            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            df = df.fillna(df.mean())
            
            # ìŠ¤ì¼€ì¼ë§
            scaled_data = self.scaler.transform(df)
            
            # ì´ìƒ ì ìˆ˜ ê³„ì‚°
            anomaly_score = self.isolation_forest.decision_function(scaled_data)[0]
            
            # ì´ìƒ ì—¬ë¶€ íŒë‹¨
            is_anomaly = anomaly_score < self.config.threshold
            
            if is_anomaly:
                # ì´ìƒ ìœ í˜• ë° ì‹¬ê°ë„ ê²°ì •
                anomaly_type, severity = self._classify_anomaly(anomaly_score, event_data)
                
                # ì„¤ëª… ìƒì„±
                description = self._generate_description(anomaly_type, severity, event_data)
                
                return AnomalyEvent(
                    event_id=f"anomaly_{int(time.time())}",
                    timestamp=datetime.now(),
                    anomaly_score=anomaly_score,
                    anomaly_type=anomaly_type,
                    severity=severity,
                    description=description,
                    data=event_data
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Anomaly detection failed: {e}")
            return None
    
    def _classify_anomaly(self, anomaly_score: float, event_data: Dict[str, Any]) -> Tuple[str, str]:
        """ì´ìƒ ìœ í˜• ë° ì‹¬ê°ë„ ë¶„ë¥˜"""
        # ì ìˆ˜ ê¸°ë°˜ ì‹¬ê°ë„ ê²°ì •
        if anomaly_score < -0.8:
            severity = 'critical'
        elif anomaly_score < -0.6:
            severity = 'high'
        elif anomaly_score < -0.4:
            severity = 'medium'
        else:
            severity = 'low'
        
        # ì´ë²¤íŠ¸ ë°ì´í„° ê¸°ë°˜ ìœ í˜• ê²°ì •
        if 'price' in event_data:
            anomaly_type = 'price_anomaly'
        elif 'volume' in event_data:
            anomaly_type = 'volume_anomaly'
        elif 'latency' in event_data:
            anomaly_type = 'latency_anomaly'
        else:
            anomaly_type = 'general_anomaly'
        
        return anomaly_type, severity
    
    def _generate_description(self, anomaly_type: str, severity: str, 
                            event_data: Dict[str, Any]) -> str:
        """ì´ìƒ ì„¤ëª… ìƒì„±"""
        base_description = f"{severity.upper()} {anomaly_type.replace('_', ' ')} detected"
        
        if 'price' in event_data:
            base_description += f" (Price: {event_data['price']})"
        elif 'volume' in event_data:
            base_description += f" (Volume: {event_data['volume']})"
        elif 'latency' in event_data:
            base_description += f" (Latency: {event_data['latency']}ms)"
        
        return base_description
    
    def get_anomaly_events(self, time_window: Optional[timedelta] = None) -> List[AnomalyEvent]:
        """ì´ìƒ ì´ë²¤íŠ¸ ì¡°íšŒ"""
        with self.lock:
            if time_window is None:
                return self.anomaly_events.copy()
            
            cutoff_time = datetime.now() - time_window
            return [event for event in self.anomaly_events 
                   if event.timestamp >= cutoff_time]
    
    def get_anomaly_stats(self) -> Dict[str, Any]:
        """ì´ìƒ í†µê³„ ì¡°íšŒ"""
        with self.lock:
            if not self.anomaly_events:
                return {
                    'total_anomalies': 0,
                    'anomaly_rate': 0.0,
                    'severity_distribution': {},
                    'type_distribution': {}
                }
            
            total_anomalies = len(self.anomaly_events)
            total_events = len(self.data_buffer)
            anomaly_rate = total_anomalies / total_events if total_events > 0 else 0.0
            
            severity_distribution = {}
            type_distribution = {}
            
            for event in self.anomaly_events:
                severity_distribution[event.severity] = severity_distribution.get(event.severity, 0) + 1
                type_distribution[event.anomaly_type] = type_distribution.get(event.anomaly_type, 0) + 1
            
            return {
                'total_anomalies': total_anomalies,
                'anomaly_rate': anomaly_rate,
                'severity_distribution': severity_distribution,
                'type_distribution': type_distribution
            }

class AlertSystem:
    """ì•Œë¦¼ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.alert_handlers = []
        self.alert_history = []
    
    def add_alert_handler(self, handler: Callable[[AnomalyEvent], None]):
        """ì•Œë¦¼ í•¸ë“¤ëŸ¬ ì¶”ê°€"""
        self.alert_handlers.append(handler)
    
    def send_alert(self, anomaly_event: AnomalyEvent):
        """ì•Œë¦¼ ë°œì†¡"""
        # ì•Œë¦¼ ê¸°ë¡
        self.alert_history.append({
            'timestamp': datetime.now(),
            'anomaly_event': anomaly_event
        })
        
        # í•¸ë“¤ëŸ¬ ì‹¤í–‰
        for handler in self.alert_handlers:
            try:
                handler(anomaly_event)
            except Exception as e:
                logger.error(f"Alert handler error: {e}")
        
        logger.info(f"Alert sent: {anomaly_event.description}")
    
    def get_alert_history(self, time_window: Optional[timedelta] = None) -> List[Dict[str, Any]]:
        """ì•Œë¦¼ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        if time_window is None:
            return self.alert_history.copy()
        
        cutoff_time = datetime.now() - time_window
        return [alert for alert in self.alert_history 
               if alert['timestamp'] >= cutoff_time]
```

## ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„**

### ğŸ“‹ **ì™„ë£Œëœ ì‘ì—…**
- âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„ ì„¤ê³„ (Kafka Streams, Flink, Spark)
- âœ… ê³ ì„±ëŠ¥ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ (ì €ì§€ì—°, ê³ ì²˜ë¦¬ëŸ‰)
- âœ… ì‹¤ì‹œê°„ ë¶„ì„ ì‹œìŠ¤í…œ (ì´ìƒ íƒì§€, ì•Œë¦¼)

### ğŸ”„ **ì§„í–‰ ì¤‘ì¸ ì‘ì—…**
- ğŸ”„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, í—¬ìŠ¤ ì²´í¬)
- ğŸ”„ í™•ì¥ì„± ë° ë‚´ê²°í•¨ì„± ì‹œìŠ¤í…œ

### â³ **ë‹¤ìŒ ë‹¨ê³„**
1. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ** ë¬¸ì„œ ìƒì„±
2. **í™•ì¥ì„± ë° ë‚´ê²°í•¨ì„±** ë¬¸ì„œ ìƒì„±
3. **Phase 5.4 ê³ ê¸‰ ë³´ì•ˆ** ë¬¸ì„œ ìƒì„±

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-01-31
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: 2024-02-01 (ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ)
**ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ëª©í‘œ**: < 1ms ì§€ì—°, 100ë§Œ ì´ë²¤íŠ¸/ì´ˆ, 99.99% ê°€ìš©ì„±
**ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì„±ê³¼**: Kafka Streams ì—”ì§„, ì´ìƒ íƒì§€, ì‹¤ì‹œê°„ ë¶„ì„ 