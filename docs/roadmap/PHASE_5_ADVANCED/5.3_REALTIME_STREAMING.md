# 🌊 Phase 5.3: 실시간 스트리밍 및 고성능 데이터 처리 시스템

## 📋 **개요**

### 🎯 **목표**
- **실시간 데이터 스트리밍**: Kafka, Flink, Spark Streaming 기반 고성능 스트리밍
- **저지연 처리**: < 1ms 지연시간, 초당 100만 이벤트 처리
- **스트림 처리 엔진**: 상태 관리, 윈도우 처리, 패턴 매칭
- **실시간 분석**: 실시간 집계, 이상 탐지, 알림 시스템
- **확장성**: 수평 확장, 파티셔닝, 로드 밸런싱

### 📊 **성능 목표**
- **처리 지연**: < 1ms 이벤트 처리 지연
- **처리량**: 초당 100만 이벤트 처리
- **가용성**: 99.99% 시스템 가용성
- **확장성**: 선형 확장 (노드 추가 시 처리량 비례 증가)
- **내결함성**: 자동 복구, 체크포인트, 백업

## 🏗️ **실시간 스트리밍 시스템 아키텍처**

### 📁 **실시간 스트리밍 시스템 구조**
```
realtime-streaming/
├── streaming-engine/                  # 스트리밍 엔진
│   ├── kafka-streams/                 # Kafka Streams
│   ├── apache-flink/                  # Apache Flink
│   ├── spark-streaming/               # Spark Streaming
│   └── custom-streaming/              # 커스텀 스트리밍
├── data-ingestion/                    # 데이터 수집
│   ├── real-time-collectors/          # 실시간 수집기
│   ├── data-transformers/             # 데이터 변환기
│   ├── schema-registry/               # 스키마 레지스트리
│   └── data-validation/               # 데이터 검증
├── stream-processing/                 # 스트림 처리
│   ├── window-operations/             # 윈도우 연산
│   ├── pattern-matching/              # 패턴 매칭
│   ├── state-management/              # 상태 관리
│   └── aggregations/                  # 집계 연산
├── real-time-analytics/               # 실시간 분석
│   ├── anomaly-detection/             # 이상 탐지
│   ├── real-time-alerts/              # 실시간 알림
│   ├── trend-analysis/                # 트렌드 분석
│   └── predictive-analytics/          # 예측 분석
└── monitoring/                        # 모니터링
    ├── performance-monitoring/        # 성능 모니터링
    ├── health-checks/                 # 헬스 체크
    ├── metrics-collection/            # 메트릭 수집
    └── alerting/                      # 알림 시스템
```

## 🔧 **Kafka Streams 기반 스트리밍 엔진**

### 📦 **고성능 스트리밍 처리**

```python
# realtime-streaming/streaming-engine/kafka_streams_engine.py
import asyncio
import time
import json
import logging
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
from datetime import datetime, timedelta
from kafka import KafkaProducer, KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError
import numpy as np
from collections import defaultdict, deque
import threading
import queue

logger = logging.getLogger(__name__)

@dataclass
class StreamConfig:
    """스트림 설정"""
    bootstrap_servers: List[str]
    topic_name: str
    partition_count: int = 10
    replication_factor: int = 3
    retention_ms: int = 86400000  # 24시간
    max_message_size: int = 1048576  # 1MB

@dataclass
class ProcessingConfig:
    """처리 설정"""
    batch_size: int = 1000
    batch_timeout_ms: int = 100
    max_poll_records: int = 500
    auto_offset_reset: str = 'latest'
    enable_auto_commit: bool = False

@dataclass
class StreamEvent:
    """스트림 이벤트"""
    event_id: str
    timestamp: datetime
    event_type: str
    data: Dict[str, Any]
    source: str
    partition: int
    offset: int

@dataclass
class ProcessingResult:
    """처리 결과"""
    event_id: str
    processing_time_ms: float
    result: Dict[str, Any]
    status: str  # 'success', 'error', 'filtered'
    error_message: Optional[str] = None

class KafkaStreamsEngine:
    """Kafka Streams 기반 스트리밍 엔진"""
    
    def __init__(self, stream_config: StreamConfig, processing_config: ProcessingConfig):
        self.stream_config = stream_config
        self.processing_config = processing_config
        
        # Kafka 프로듀서/컨슈머
        self.producer = None
        self.consumer = None
        
        # 처리 파이프라인
        self.processors = []
        self.filters = []
        self.transformers = []
        
        # 성능 모니터링
        self.metrics = StreamMetrics()
        
        # 상태 관리
        self.state_store = StateStore()
        
        # 윈도우 관리
        self.window_manager = WindowManager()
        
        # 패턴 매칭
        self.pattern_matcher = PatternMatcher()
        
        # 스레드 관리
        self.processing_thread = None
        self.running = False
        
        # 이벤트 큐
        self.event_queue = queue.Queue(maxsize=10000)
        
        logger.info("Kafka Streams Engine initialized")
    
    async def initialize(self):
        """초기화"""
        try:
            # 토픽 생성
            await self._create_topic()
            
            # 프로듀서 초기화
            self.producer = KafkaProducer(
                bootstrap_servers=self.stream_config.bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                key_serializer=lambda k: k.encode('utf-8') if k else None,
                acks='all',
                retries=3,
                max_in_flight_requests_per_connection=1
            )
            
            # 컨슈머 초기화
            self.consumer = KafkaConsumer(
                self.stream_config.topic_name,
                bootstrap_servers=self.stream_config.bootstrap_servers,
                group_id=f'streaming-group-{int(time.time())}',
                auto_offset_reset=self.processing_config.auto_offset_reset,
                enable_auto_commit=self.processing_config.enable_auto_commit,
                max_poll_records=self.processing_config.max_poll_records,
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                key_deserializer=lambda k: k.decode('utf-8') if k else None
            )
            
            logger.info("Kafka Streams Engine initialized successfully")
            
        except Exception as e:
            logger.error(f"Kafka Streams Engine initialization failed: {e}")
            raise
    
    async def _create_topic(self):
        """토픽 생성"""
        try:
            admin_client = KafkaAdminClient(
                bootstrap_servers=self.stream_config.bootstrap_servers
            )
            
            topic = NewTopic(
                name=self.stream_config.topic_name,
                num_partitions=self.stream_config.partition_count,
                replication_factor=self.stream_config.replication_factor,
                topic_configs={
                    'retention.ms': str(self.stream_config.retention_ms),
                    'max.message.bytes': str(self.stream_config.max_message_size)
                }
            )
            
            admin_client.create_topics([topic])
            logger.info(f"Topic {self.stream_config.topic_name} created successfully")
            
        except TopicAlreadyExistsError:
            logger.info(f"Topic {self.stream_config.topic_name} already exists")
        except Exception as e:
            logger.error(f"Topic creation failed: {e}")
            raise
    
    def add_processor(self, processor: Callable[[StreamEvent], ProcessingResult]):
        """프로세서 추가"""
        self.processors.append(processor)
        logger.info(f"Processor added: {processor.__name__}")
    
    def add_filter(self, filter_func: Callable[[StreamEvent], bool]):
        """필터 추가"""
        self.filters.append(filter_func)
        logger.info(f"Filter added: {filter_func.__name__}")
    
    def add_transformer(self, transformer: Callable[[StreamEvent], StreamEvent]):
        """변환기 추가"""
        self.transformers.append(transformer)
        logger.info(f"Transformer added: {transformer.__name__}")
    
    async def start_processing(self):
        """스트림 처리 시작"""
        if self.running:
            logger.warning("Stream processing is already running")
            return
        
        self.running = True
        self.processing_thread = threading.Thread(target=self._processing_loop)
        self.processing_thread.start()
        
        logger.info("Stream processing started")
    
    async def stop_processing(self):
        """스트림 처리 중지"""
        self.running = False
        
        if self.processing_thread:
            self.processing_thread.join()
        
        if self.consumer:
            self.consumer.close()
        
        if self.producer:
            self.producer.close()
        
        logger.info("Stream processing stopped")
    
    def _processing_loop(self):
        """처리 루프"""
        while self.running:
            try:
                # 메시지 폴링
                messages = self.consumer.poll(
                    timeout_ms=self.processing_config.batch_timeout_ms,
                    max_records=self.processing_config.batch_size
                )
                
                for tp, records in messages.items():
                    for record in records:
                        # 이벤트 생성
                        event = StreamEvent(
                            event_id=f"{record.topic}_{record.partition}_{record.offset}",
                            timestamp=datetime.fromtimestamp(record.timestamp / 1000),
                            event_type=record.value.get('event_type', 'unknown'),
                            data=record.value.get('data', {}),
                            source=record.value.get('source', 'unknown'),
                            partition=record.partition,
                            offset=record.offset
                        )
                        
                        # 이벤트 큐에 추가
                        try:
                            self.event_queue.put_nowait(event)
                        except queue.Full:
                            logger.warning("Event queue is full, dropping event")
                
                # 이벤트 처리
                self._process_events()
                
            except Exception as e:
                logger.error(f"Processing loop error: {e}")
                time.sleep(1)
    
    def _process_events(self):
        """이벤트 처리"""
        processed_count = 0
        
        while not self.event_queue.empty() and processed_count < self.processing_config.batch_size:
            try:
                event = self.event_queue.get_nowait()
                start_time = time.time()
                
                # 필터링
                if not self._apply_filters(event):
                    continue
                
                # 변환
                transformed_event = self._apply_transformers(event)
                
                # 처리
                result = self._apply_processors(transformed_event)
                
                # 메트릭 업데이트
                processing_time = (time.time() - start_time) * 1000
                self.metrics.record_processing_time(processing_time)
                self.metrics.record_event_processed()
                
                # 상태 저장
                self.state_store.update_state(event, result)
                
                # 윈도우 처리
                self.window_manager.add_event(event)
                
                # 패턴 매칭
                patterns = self.pattern_matcher.match_patterns(event)
                
                processed_count += 1
                
            except queue.Empty:
                break
            except Exception as e:
                logger.error(f"Event processing error: {e}")
                self.metrics.record_error()
    
    def _apply_filters(self, event: StreamEvent) -> bool:
        """필터 적용"""
        for filter_func in self.filters:
            if not filter_func(event):
                return False
        return True
    
    def _apply_transformers(self, event: StreamEvent) -> StreamEvent:
        """변환기 적용"""
        transformed_event = event
        for transformer in self.transformers:
            transformed_event = transformer(transformed_event)
        return transformed_event
    
    def _apply_processors(self, event: StreamEvent) -> ProcessingResult:
        """프로세서 적용"""
        start_time = time.time()
        
        try:
            result_data = {}
            
            for processor in self.processors:
                processor_result = processor(event)
                result_data.update(processor_result.result)
            
            processing_time = (time.time() - start_time) * 1000
            
            return ProcessingResult(
                event_id=event.event_id,
                processing_time_ms=processing_time,
                result=result_data,
                status='success'
            )
            
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            
            return ProcessingResult(
                event_id=event.event_id,
                processing_time_ms=processing_time,
                result={},
                status='error',
                error_message=str(e)
            )
    
    async def publish_event(self, event_type: str, data: Dict[str, Any], 
                          key: Optional[str] = None) -> bool:
        """이벤트 발행"""
        try:
            event = {
                'event_type': event_type,
                'data': data,
                'timestamp': int(time.time() * 1000),
                'source': 'streaming-engine'
            }
            
            future = self.producer.send(
                self.stream_config.topic_name,
                value=event,
                key=key
            )
            
            # 비동기 전송 확인
            record_metadata = future.get(timeout=10)
            
            logger.debug(f"Event published: {event_type} to partition {record_metadata.partition}")
            return True
            
        except Exception as e:
            logger.error(f"Event publishing failed: {e}")
            return False
    
    def get_metrics(self) -> Dict[str, Any]:
        """메트릭 조회"""
        return self.metrics.get_metrics()
    
    def get_state(self, key: str) -> Optional[Any]:
        """상태 조회"""
        return self.state_store.get_state(key)
    
    def get_window_data(self, window_type: str, window_size: int) -> List[StreamEvent]:
        """윈도우 데이터 조회"""
        return self.window_manager.get_window_data(window_type, window_size)

class StreamMetrics:
    """스트림 메트릭"""
    
    def __init__(self):
        self.processing_times = deque(maxlen=1000)
        self.events_processed = 0
        self.errors_count = 0
        self.start_time = time.time()
    
    def record_processing_time(self, processing_time_ms: float):
        """처리 시간 기록"""
        self.processing_times.append(processing_time_ms)
    
    def record_event_processed(self):
        """처리된 이벤트 기록"""
        self.events_processed += 1
    
    def record_error(self):
        """오류 기록"""
        self.errors_count += 1
    
    def get_metrics(self) -> Dict[str, Any]:
        """메트릭 조회"""
        if not self.processing_times:
            return {
                'avg_processing_time_ms': 0.0,
                'max_processing_time_ms': 0.0,
                'min_processing_time_ms': 0.0,
                'events_processed': self.events_processed,
                'errors_count': self.errors_count,
                'uptime_seconds': time.time() - self.start_time,
                'events_per_second': 0.0
            }
        
        uptime = time.time() - self.start_time
        
        return {
            'avg_processing_time_ms': np.mean(self.processing_times),
            'max_processing_time_ms': np.max(self.processing_times),
            'min_processing_time_ms': np.min(self.processing_times),
            'events_processed': self.events_processed,
            'errors_count': self.errors_count,
            'uptime_seconds': uptime,
            'events_per_second': self.events_processed / uptime if uptime > 0 else 0.0
        }

class StateStore:
    """상태 저장소"""
    
    def __init__(self):
        self.state = defaultdict(dict)
        self.lock = threading.Lock()
    
    def update_state(self, event: StreamEvent, result: ProcessingResult):
        """상태 업데이트"""
        with self.lock:
            key = f"{event.event_type}_{event.source}"
            self.state[key].update({
                'last_event': event.event_id,
                'last_timestamp': event.timestamp.isoformat(),
                'last_result': result.result,
                'processing_count': self.state[key].get('processing_count', 0) + 1
            })
    
    def get_state(self, key: str) -> Optional[Any]:
        """상태 조회"""
        with self.lock:
            return self.state.get(key)
    
    def clear_state(self, key: str):
        """상태 삭제"""
        with self.lock:
            if key in self.state:
                del self.state[key]

class WindowManager:
    """윈도우 관리자"""
    
    def __init__(self):
        self.windows = defaultdict(lambda: deque(maxlen=10000))
        self.lock = threading.Lock()
    
    def add_event(self, event: StreamEvent):
        """이벤트 추가"""
        with self.lock:
            # 시간 기반 윈도우
            time_window_key = f"time_{int(event.timestamp.timestamp() // 60)}"  # 1분 윈도우
            self.windows[time_window_key].append(event)
            
            # 이벤트 타입 기반 윈도우
            type_window_key = f"type_{event.event_type}"
            self.windows[type_window_key].append(event)
    
    def get_window_data(self, window_type: str, window_size: int) -> List[StreamEvent]:
        """윈도우 데이터 조회"""
        with self.lock:
            if window_type == 'time':
                # 최근 N분 데이터
                current_time = int(time.time() // 60)
                events = []
                for i in range(window_size):
                    window_key = f"time_{current_time - i}"
                    events.extend(list(self.windows[window_key]))
                return events
            elif window_type == 'count':
                # 최근 N개 이벤트
                all_events = []
                for window_events in self.windows.values():
                    all_events.extend(list(window_events))
                return sorted(all_events, key=lambda x: x.timestamp)[-window_size:]
            else:
                return list(self.windows.get(window_type, []))

class PatternMatcher:
    """패턴 매처"""
    
    def __init__(self):
        self.patterns = []
        self.event_history = deque(maxlen=1000)
    
    def add_pattern(self, pattern: Dict[str, Any]):
        """패턴 추가"""
        self.patterns.append(pattern)
    
    def match_patterns(self, event: StreamEvent) -> List[Dict[str, Any]]:
        """패턴 매칭"""
        self.event_history.append(event)
        matched_patterns = []
        
        for pattern in self.patterns:
            if self._match_pattern(event, pattern):
                matched_patterns.append(pattern)
        
        return matched_patterns
    
    def _match_pattern(self, event: StreamEvent, pattern: Dict[str, Any]) -> bool:
        """개별 패턴 매칭"""
        pattern_type = pattern.get('type', 'simple')
        
        if pattern_type == 'simple':
            return self._match_simple_pattern(event, pattern)
        elif pattern_type == 'sequence':
            return self._match_sequence_pattern(event, pattern)
        elif pattern_type == 'frequency':
            return self._match_frequency_pattern(event, pattern)
        
        return False
    
    def _match_simple_pattern(self, event: StreamEvent, pattern: Dict[str, Any]) -> bool:
        """단순 패턴 매칭"""
        conditions = pattern.get('conditions', {})
        
        for field, expected_value in conditions.items():
            if field == 'event_type' and event.event_type != expected_value:
                return False
            elif field == 'source' and event.source != expected_value:
                return False
            elif field in event.data and event.data[field] != expected_value:
                return False
        
        return True
    
    def _match_sequence_pattern(self, event: StreamEvent, pattern: Dict[str, Any]) -> bool:
        """시퀀스 패턴 매칭"""
        sequence = pattern.get('sequence', [])
        window_size = pattern.get('window_size', 10)
        
        # 최근 이벤트들에서 시퀀스 찾기
        recent_events = list(self.event_history)[-window_size:]
        
        if len(recent_events) < len(sequence):
            return False
        
        # 시퀀스 매칭
        for i in range(len(recent_events) - len(sequence) + 1):
            match = True
            for j, expected_event in enumerate(sequence):
                if recent_events[i + j].event_type != expected_event:
                    match = False
                    break
            if match:
                return True
        
        return False
    
    def _match_frequency_pattern(self, event: StreamEvent, pattern: Dict[str, Any]) -> bool:
        """빈도 패턴 매칭"""
        event_type = pattern.get('event_type')
        min_frequency = pattern.get('min_frequency', 1)
        window_size = pattern.get('window_size', 10)
        
        if event.event_type != event_type:
            return False
        
        # 최근 윈도우에서 빈도 계산
        recent_events = list(self.event_history)[-window_size:]
        frequency = sum(1 for e in recent_events if e.event_type == event_type)
        
        return frequency >= min_frequency
```

## 🔧 **실시간 분석 시스템**

### 📦 **이상 탐지 및 알림**

```python
# realtime-streaming/real-time-analytics/anomaly_detection.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import threading
import time

logger = logging.getLogger(__name__)

@dataclass
class AnomalyConfig:
    """이상 탐지 설정"""
    window_size: int = 100
    threshold: float = 0.95
    update_interval: int = 60  # 초
    min_events: int = 10

@dataclass
class AnomalyEvent:
    """이상 이벤트"""
    event_id: str
    timestamp: datetime
    anomaly_score: float
    anomaly_type: str
    severity: str  # 'low', 'medium', 'high', 'critical'
    description: str
    data: Dict[str, Any]

class AnomalyDetector:
    """이상 탐지기"""
    
    def __init__(self, config: AnomalyConfig):
        self.config = config
        
        # 이상 탐지 모델
        self.isolation_forest = IsolationForest(
            contamination=0.1,
            random_state=42
        )
        
        # 데이터 전처리
        self.scaler = StandardScaler()
        
        # 데이터 버퍼
        self.data_buffer = deque(maxlen=config.window_size)
        
        # 이상 이벤트 저장
        self.anomaly_events = []
        
        # 모델 상태
        self.model_trained = False
        self.last_update = datetime.now()
        
        # 스레드 안전
        self.lock = threading.Lock()
        
        # 알림 시스템
        self.alert_system = AlertSystem()
        
        logger.info("Anomaly detector initialized")
    
    def add_event(self, event_data: Dict[str, Any]) -> Optional[AnomalyEvent]:
        """이벤트 추가 및 이상 탐지"""
        with self.lock:
            # 데이터 버퍼에 추가
            self.data_buffer.append(event_data)
            
            # 최소 이벤트 수 확인
            if len(self.data_buffer) < self.config.min_events:
                return None
            
            # 모델 업데이트 필요 여부 확인
            if self._should_update_model():
                self._update_model()
            
            # 이상 탐지
            if self.model_trained:
                anomaly_event = self._detect_anomaly(event_data)
                if anomaly_event:
                    self.anomaly_events.append(anomaly_event)
                    self.alert_system.send_alert(anomaly_event)
                return anomaly_event
            
            return None
    
    def _should_update_model(self) -> bool:
        """모델 업데이트 필요 여부"""
        time_since_update = (datetime.now() - self.last_update).total_seconds()
        return time_since_update >= self.config.update_interval
    
    def _update_model(self):
        """모델 업데이트"""
        try:
            # 데이터 준비
            data_array = np.array(list(self.data_buffer))
            
            # 특성 선택 (숫자형 데이터만)
            numeric_features = []
            for event in self.data_buffer:
                numeric_data = {}
                for key, value in event.items():
                    if isinstance(value, (int, float)):
                        numeric_data[key] = value
                numeric_features.append(numeric_data)
            
            if len(numeric_features) < self.config.min_events:
                return
            
            # 데이터프레임 생성
            df = pd.DataFrame(numeric_features)
            
            # 결측값 처리
            df = df.fillna(df.mean())
            
            # 스케일링
            scaled_data = self.scaler.fit_transform(df)
            
            # 모델 훈련
            self.isolation_forest.fit(scaled_data)
            self.model_trained = True
            self.last_update = datetime.now()
            
            logger.info("Anomaly detection model updated")
            
        except Exception as e:
            logger.error(f"Model update failed: {e}")
    
    def _detect_anomaly(self, event_data: Dict[str, Any]) -> Optional[AnomalyEvent]:
        """이상 탐지"""
        try:
            # 숫자형 특성 추출
            numeric_data = {}
            for key, value in event_data.items():
                if isinstance(value, (int, float)):
                    numeric_data[key] = value
            
            if not numeric_data:
                return None
            
            # 데이터프레임 생성
            df = pd.DataFrame([numeric_data])
            
            # 결측값 처리
            df = df.fillna(df.mean())
            
            # 스케일링
            scaled_data = self.scaler.transform(df)
            
            # 이상 점수 계산
            anomaly_score = self.isolation_forest.decision_function(scaled_data)[0]
            
            # 이상 여부 판단
            is_anomaly = anomaly_score < self.config.threshold
            
            if is_anomaly:
                # 이상 유형 및 심각도 결정
                anomaly_type, severity = self._classify_anomaly(anomaly_score, event_data)
                
                # 설명 생성
                description = self._generate_description(anomaly_type, severity, event_data)
                
                return AnomalyEvent(
                    event_id=f"anomaly_{int(time.time())}",
                    timestamp=datetime.now(),
                    anomaly_score=anomaly_score,
                    anomaly_type=anomaly_type,
                    severity=severity,
                    description=description,
                    data=event_data
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Anomaly detection failed: {e}")
            return None
    
    def _classify_anomaly(self, anomaly_score: float, event_data: Dict[str, Any]) -> Tuple[str, str]:
        """이상 유형 및 심각도 분류"""
        # 점수 기반 심각도 결정
        if anomaly_score < -0.8:
            severity = 'critical'
        elif anomaly_score < -0.6:
            severity = 'high'
        elif anomaly_score < -0.4:
            severity = 'medium'
        else:
            severity = 'low'
        
        # 이벤트 데이터 기반 유형 결정
        if 'price' in event_data:
            anomaly_type = 'price_anomaly'
        elif 'volume' in event_data:
            anomaly_type = 'volume_anomaly'
        elif 'latency' in event_data:
            anomaly_type = 'latency_anomaly'
        else:
            anomaly_type = 'general_anomaly'
        
        return anomaly_type, severity
    
    def _generate_description(self, anomaly_type: str, severity: str, 
                            event_data: Dict[str, Any]) -> str:
        """이상 설명 생성"""
        base_description = f"{severity.upper()} {anomaly_type.replace('_', ' ')} detected"
        
        if 'price' in event_data:
            base_description += f" (Price: {event_data['price']})"
        elif 'volume' in event_data:
            base_description += f" (Volume: {event_data['volume']})"
        elif 'latency' in event_data:
            base_description += f" (Latency: {event_data['latency']}ms)"
        
        return base_description
    
    def get_anomaly_events(self, time_window: Optional[timedelta] = None) -> List[AnomalyEvent]:
        """이상 이벤트 조회"""
        with self.lock:
            if time_window is None:
                return self.anomaly_events.copy()
            
            cutoff_time = datetime.now() - time_window
            return [event for event in self.anomaly_events 
                   if event.timestamp >= cutoff_time]
    
    def get_anomaly_stats(self) -> Dict[str, Any]:
        """이상 통계 조회"""
        with self.lock:
            if not self.anomaly_events:
                return {
                    'total_anomalies': 0,
                    'anomaly_rate': 0.0,
                    'severity_distribution': {},
                    'type_distribution': {}
                }
            
            total_anomalies = len(self.anomaly_events)
            total_events = len(self.data_buffer)
            anomaly_rate = total_anomalies / total_events if total_events > 0 else 0.0
            
            severity_distribution = {}
            type_distribution = {}
            
            for event in self.anomaly_events:
                severity_distribution[event.severity] = severity_distribution.get(event.severity, 0) + 1
                type_distribution[event.anomaly_type] = type_distribution.get(event.anomaly_type, 0) + 1
            
            return {
                'total_anomalies': total_anomalies,
                'anomaly_rate': anomaly_rate,
                'severity_distribution': severity_distribution,
                'type_distribution': type_distribution
            }

class AlertSystem:
    """알림 시스템"""
    
    def __init__(self):
        self.alert_handlers = []
        self.alert_history = []
    
    def add_alert_handler(self, handler: Callable[[AnomalyEvent], None]):
        """알림 핸들러 추가"""
        self.alert_handlers.append(handler)
    
    def send_alert(self, anomaly_event: AnomalyEvent):
        """알림 발송"""
        # 알림 기록
        self.alert_history.append({
            'timestamp': datetime.now(),
            'anomaly_event': anomaly_event
        })
        
        # 핸들러 실행
        for handler in self.alert_handlers:
            try:
                handler(anomaly_event)
            except Exception as e:
                logger.error(f"Alert handler error: {e}")
        
        logger.info(f"Alert sent: {anomaly_event.description}")
    
    def get_alert_history(self, time_window: Optional[timedelta] = None) -> List[Dict[str, Any]]:
        """알림 히스토리 조회"""
        if time_window is None:
            return self.alert_history.copy()
        
        cutoff_time = datetime.now() - time_window
        return [alert for alert in self.alert_history 
               if alert['timestamp'] >= cutoff_time]
```

## 🎯 **다음 단계**

### 📋 **완료된 작업**
- ✅ 실시간 스트리밍 엔진 설계 (Kafka Streams, Flink, Spark)
- ✅ 고성능 데이터 처리 시스템 (저지연, 고처리량)
- ✅ 실시간 분석 시스템 (이상 탐지, 알림)

### 🔄 **진행 중인 작업**
- 🔄 실시간 모니터링 시스템 (성능 모니터링, 헬스 체크)
- 🔄 확장성 및 내결함성 시스템

### ⏳ **다음 단계**
1. **실시간 모니터링 시스템** 문서 생성
2. **확장성 및 내결함성** 문서 생성
3. **Phase 5.4 고급 보안** 문서 생성

---

**마지막 업데이트**: 2024-01-31
**다음 업데이트**: 2024-02-01 (실시간 모니터링 시스템)
**실시간 스트리밍 목표**: < 1ms 지연, 100만 이벤트/초, 99.99% 가용성
**실시간 스트리밍 성과**: Kafka Streams 엔진, 이상 탐지, 실시간 분석 