# ğŸ¤– Phase 5.2: ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œ

## ğŸ“‹ **ê°œìš”**

### ğŸ¯ **ëª©í‘œ**
- **ë”¥ëŸ¬ë‹ ëª¨ë¸**: Transformer, BERT, GPT ê¸°ë°˜ ê³ ê¸‰ ì˜ˆì¸¡ ëª¨ë¸
- **ê°•í™”í•™ìŠµ ê³ ë„í™”**: ë‹¤ì¤‘ ì—ì´ì „íŠ¸, ë©”íƒ€ëŸ¬ë‹, ê³„ì¸µì  ê°•í™”í•™ìŠµ
- **ì•™ìƒë¸” ì‹œìŠ¤í…œ**: ë™ì  ì•™ìƒë¸”, ìŠ¤íƒœí‚¹, ë¶€ìŠ¤íŒ… ê³ ë„í™”
- **ìë™í™”ëœ ML**: AutoML, í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”, ëª¨ë¸ ì„ íƒ ìë™í™”
- **ì‹¤ì‹œê°„ ì¶”ë¡ **: ê³ ì„±ëŠ¥ ì¶”ë¡  ì—”ì§„, ëª¨ë¸ ì••ì¶•, ì–‘ìí™”

### ğŸ“Š **ì„±ëŠ¥ ëª©í‘œ**
- **ëª¨ë¸ ì •í™•ë„**: 95% ì´ìƒ ì˜ˆì¸¡ ì •í™•ë„
- **ì¶”ë¡  ì†ë„**: < 10ms ì‹¤ì‹œê°„ ì¶”ë¡ 
- **í•™ìŠµ ì†ë„**: 50% ë¹ ë¥¸ ëª¨ë¸ í•™ìŠµ
- **ìë™í™” ìˆ˜ì¤€**: 90% ìë™í™”ëœ ëª¨ë¸ ì„ íƒ ë° ìµœì í™”
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 70% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ

## ğŸ—ï¸ **ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

### ğŸ“ **ê³ ê¸‰ ML ì‹œìŠ¤í…œ êµ¬ì¡°**
```
advanced-ml/
â”œâ”€â”€ deep-learning/                    # ë”¥ëŸ¬ë‹ ëª¨ë¸
â”‚   â”œâ”€â”€ transformer-models/           # Transformer ê¸°ë°˜ ëª¨ë¸
â”‚   â”œâ”€â”€ bert-models/                  # BERT ê¸°ë°˜ ëª¨ë¸
â”‚   â”œâ”€â”€ gpt-models/                   # GPT ê¸°ë°˜ ëª¨ë¸
â”‚   â””â”€â”€ attention-mechanisms/         # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
â”œâ”€â”€ reinforcement-learning/           # ê°•í™”í•™ìŠµ ê³ ë„í™”
â”‚   â”œâ”€â”€ multi-agent/                  # ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ meta-learning/                # ë©”íƒ€ëŸ¬ë‹
â”‚   â”œâ”€â”€ hierarchical-rl/              # ê³„ì¸µì  ê°•í™”í•™ìŠµ
â”‚   â””â”€â”€ curriculum-learning/          # ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹
â”œâ”€â”€ ensemble-systems/                 # ì•™ìƒë¸” ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ dynamic-ensemble/             # ë™ì  ì•™ìƒë¸”
â”‚   â”œâ”€â”€ stacking/                     # ìŠ¤íƒœí‚¹
â”‚   â”œâ”€â”€ boosting/                     # ë¶€ìŠ¤íŒ…
â”‚   â””â”€â”€ bagging/                      # ë°°ê¹…
â”œâ”€â”€ automl/                          # ìë™í™”ëœ ML
â”‚   â”œâ”€â”€ hyperparameter-optimization/  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
â”‚   â”œâ”€â”€ model-selection/              # ëª¨ë¸ ì„ íƒ
â”‚   â”œâ”€â”€ feature-engineering/          # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
â”‚   â””â”€â”€ neural-architecture-search/   # ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ íƒìƒ‰
â””â”€â”€ inference-engine/                 # ì¶”ë¡  ì—”ì§„
    â”œâ”€â”€ real-time-inference/          # ì‹¤ì‹œê°„ ì¶”ë¡ 
    â”œâ”€â”€ model-compression/            # ëª¨ë¸ ì••ì¶•
    â”œâ”€â”€ quantization/                 # ì–‘ìí™”
    â””â”€â”€ edge-inference/               # ì—£ì§€ ì¶”ë¡ 
```

## ğŸ”§ **ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‹œìŠ¤í…œ**

### ğŸ“¦ **Transformer ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸**

```python
# advanced-ml/deep-learning/transformer_models.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import logging
import numpy as np
from transformers import AutoTokenizer, AutoModel
import math

logger = logging.getLogger(__name__)

@dataclass
class TransformerConfig:
    """Transformer ì„¤ì •"""
    d_model: int = 512
    n_heads: int = 8
    n_layers: int = 6
    d_ff: int = 2048
    dropout: float = 0.1
    max_seq_length: int = 1000
    vocab_size: int = 50000

@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì •"""
    batch_size: int = 32
    learning_rate: float = 1e-4
    warmup_steps: int = 4000
    max_epochs: int = 100
    gradient_clip: float = 1.0
    weight_decay: float = 0.01

class MultiHeadAttention(nn.Module):
    """ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜"""
    
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.d_k)
    
    def forward(self, query: torch.Tensor, key: torch.Tensor, 
                value: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size = query.size(0)
        
        # ì„ í˜• ë³€í™˜ ë° í—¤ë“œ ë¶„í• 
        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # ì–´í…ì…˜ ê³„ì‚°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # ì¶œë ¥ ê³„ì‚°
        context = torch.matmul(attention_weights, V)
        context = context.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        output = self.w_o(context)
        return output

class PositionalEncoding(nn.Module):
    """ìœ„ì¹˜ ì¸ì½”ë”©"""
    
    def __init__(self, d_model: int, max_seq_length: int = 1000):
        super().__init__()
        
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:, :x.size(1)]

class TransformerBlock(nn.Module):
    """Transformer ë¸”ë¡"""
    
    def __init__(self, config: TransformerConfig):
        super().__init__()
        
        self.attention = MultiHeadAttention(
            config.d_model, config.n_heads, config.dropout
        )
        self.feed_forward = nn.Sequential(
            nn.Linear(config.d_model, config.d_ff),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_ff, config.d_model)
        )
        
        self.norm1 = nn.LayerNorm(config.d_model)
        self.norm2 = nn.LayerNorm(config.d_model)
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # ì–´í…ì…˜ ì„œë¸Œë ˆì´ì–´
        attn_output = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # í”¼ë“œí¬ì›Œë“œ ì„œë¸Œë ˆì´ì–´
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class TradingTransformer(nn.Module):
    """ê±°ë˜ìš© Transformer ëª¨ë¸"""
    
    def __init__(self, config: TransformerConfig, num_classes: int = 3):
        super().__init__()
        
        self.config = config
        
        # ì„ë² ë”© ë ˆì´ì–´
        self.embedding = nn.Linear(config.d_model, config.d_model)
        self.pos_encoding = PositionalEncoding(config.d_model, config.max_seq_length)
        
        # Transformer ë¸”ë¡ë“¤
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.n_layers)
        ])
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.output_layer = nn.Sequential(
            nn.Linear(config.d_model, config.d_model // 2),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_model // 2, num_classes)
        )
        
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # ì„ë² ë”© ë° ìœ„ì¹˜ ì¸ì½”ë”©
        x = self.embedding(x)
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        # Transformer ë¸”ë¡ë“¤ í†µê³¼
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x, mask)
        
        # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
        if mask is not None:
            x = (x * mask.unsqueeze(-1)).sum(dim=1) / mask.sum(dim=1, keepdim=True)
        else:
            x = x.mean(dim=1)
        
        # ì¶œë ¥ ë ˆì´ì–´
        output = self.output_layer(x)
        return output

class TransformerTrainer:
    """Transformer ëª¨ë¸ í›ˆë ¨ê¸°"""
    
    def __init__(self, model: TradingTransformer, config: TrainingConfig):
        self.model = model
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=10, T_mult=2
        )
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = nn.CrossEntropyLoss()
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
    
    def train_epoch(self, train_loader) -> Tuple[float, float]:
        """í•œ ì—í¬í¬ í›ˆë ¨"""
        self.model.train()
        total_loss = 0.0
        correct_predictions = 0
        total_predictions = 0
        
        for batch_idx, (data, targets) in enumerate(train_loader):
            data, targets = data.to(self.device), targets.to(self.device)
            
            # ìˆœì „íŒŒ
            self.optimizer.zero_grad()
            outputs = self.model(data)
            loss = self.criterion(outputs, targets)
            
            # ì—­ì „íŒŒ
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)
            self.optimizer.step()
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_predictions += targets.size(0)
            correct_predictions += (predicted == targets).sum().item()
            
            if batch_idx % 100 == 0:
                logger.info(f'Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct_predictions / total_predictions
        
        return avg_loss, accuracy
    
    def validate(self, val_loader) -> Tuple[float, float]:
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0.0
        correct_predictions = 0
        total_predictions = 0
        
        with torch.no_grad():
            for data, targets in val_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                
                outputs = self.model(data)
                loss = self.criterion(outputs, targets)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total_predictions += targets.size(0)
                correct_predictions += (predicted == targets).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = correct_predictions / total_predictions
        
        return avg_loss, accuracy
    
    def train(self, train_loader, val_loader) -> Dict[str, List[float]]:
        """ì „ì²´ í›ˆë ¨"""
        best_val_accuracy = 0.0
        
        for epoch in range(self.config.max_epochs):
            # í›ˆë ¨
            train_loss, train_accuracy = self.train_epoch(train_loader)
            
            # ê²€ì¦
            val_loss, val_accuracy = self.validate(val_loader)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.scheduler.step()
            
            # ë©”íŠ¸ë¦­ ì €ì¥
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_accuracy)
            self.val_accuracies.append(val_accuracy)
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
                torch.save(self.model.state_dict(), 'best_transformer_model.pth')
            
            logger.info(f'Epoch {epoch+1}/{self.config.max_epochs}:')
            logger.info(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}')
            logger.info(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')
        
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies
        }
```

## ğŸ”§ **ê°•í™”í•™ìŠµ ê³ ë„í™” ì‹œìŠ¤í…œ**

### ğŸ“¦ **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™”í•™ìŠµ**

```python
# advanced-ml/reinforcement-learning/multi_agent_rl.py
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import logging
import random
from collections import deque

logger = logging.getLogger(__name__)

@dataclass
class AgentConfig:
    """ì—ì´ì „íŠ¸ ì„¤ì •"""
    state_dim: int = 100
    action_dim: int = 10
    hidden_dim: int = 256
    learning_rate: float = 1e-4
    gamma: float = 0.99
    tau: float = 0.005
    buffer_size: int = 100000
    batch_size: int = 64

@dataclass
class MultiAgentConfig:
    """ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì„¤ì •"""
    num_agents: int = 5
    communication_enabled: bool = True
    shared_experience: bool = True
    competitive_mode: bool = False

class Actor(nn.Module):
    """ì•¡í„° ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int):
        super().__init__()
        
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        x = F.relu(self.bn1(self.fc1(state)))
        x = self.dropout(x)
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        x = torch.tanh(self.fc3(x))
        return x

class Critic(nn.Module):
    """í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int):
        super().__init__()
        
        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)
        
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        x = torch.cat([state, action], dim=1)
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.dropout(x)
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

class DDPGAgent:
    """DDPG ì—ì´ì „íŠ¸"""
    
    def __init__(self, config: AgentConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ë„¤íŠ¸ì›Œí¬
        self.actor = Actor(config.state_dim, config.action_dim, config.hidden_dim).to(self.device)
        self.critic = Critic(config.state_dim, config.action_dim, config.hidden_dim).to(self.device)
        
        self.actor_target = Actor(config.state_dim, config.action_dim, config.hidden_dim).to(self.device)
        self.critic_target = Critic(config.state_dim, config.action_dim, config.hidden_dim).to(self.device)
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # ì˜µí‹°ë§ˆì´ì €
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=config.learning_rate)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=config.learning_rate)
        
        # ë¦¬í”Œë ˆì´ ë²„í¼
        self.replay_buffer = deque(maxlen=config.buffer_size)
        
        # ë…¸ì´ì¦ˆ
        self.noise = 0.1
    
    def select_action(self, state: np.ndarray) -> np.ndarray:
        """ì•¡ì…˜ ì„ íƒ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().numpy()[0]
        
        # íƒí—˜ì„ ìœ„í•œ ë…¸ì´ì¦ˆ ì¶”ê°€
        action += np.random.normal(0, self.noise, action.shape)
        action = np.clip(action, -1, 1)
        
        return action
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, 
                        reward: float, next_state: np.ndarray, done: bool):
        """ê²½í—˜ ì €ì¥"""
        self.replay_buffer.append((state, action, reward, next_state, done))
    
    def update(self):
        """ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        if len(self.replay_buffer) < self.config.batch_size:
            return
        
        # ë°°ì¹˜ ìƒ˜í”Œë§
        batch = random.sample(self.replay_buffer, self.config.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.FloatTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # í¬ë¦¬í‹± ì—…ë°ì´íŠ¸
        next_actions = self.actor_target(next_states)
        target_q = self.critic_target(next_states, next_actions)
        target_q = rewards + (1 - dones) * self.config.gamma * target_q
        
        current_q = self.critic(states, actions)
        critic_loss = F.mse_loss(current_q, target_q.detach())
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # ì•¡í„° ì—…ë°ì´íŠ¸
        actor_loss = -self.critic(states, self.actor(states)).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self._update_target_networks()
    
    def _update_target_networks(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(self.config.tau * param.data + 
                                  (1 - self.config.tau) * target_param.data)
        
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(self.config.tau * param.data + 
                                  (1 - self.config.tau) * target_param.data)

class MultiAgentSystem:
    """ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: MultiAgentConfig, agent_config: AgentConfig):
        self.config = config
        self.agent_config = agent_config
        
        # ì—ì´ì „íŠ¸ ìƒì„±
        self.agents = [DDPGAgent(agent_config) for _ in range(config.num_agents)]
        
        # í†µì‹  ë„¤íŠ¸ì›Œí¬ (ì„ íƒì )
        if config.communication_enabled:
            self.communication_network = CommunicationNetwork(config.num_agents)
        
        # ê³µìœ  ê²½í—˜ ë²„í¼
        if config.shared_experience:
            self.shared_buffer = deque(maxlen=agent_config.buffer_size)
        
        # ì„±ëŠ¥ ì¶”ì 
        self.agent_performances = [[] for _ in range(config.num_agents)]
    
    def train_episode(self, env) -> List[float]:
        """ì—í”¼ì†Œë“œ í›ˆë ¨"""
        states = env.reset()
        episode_rewards = [0.0] * self.config.num_agents
        done = False
        
        while not done:
            actions = []
            
            # ê° ì—ì´ì „íŠ¸ì˜ ì•¡ì…˜ ì„ íƒ
            for i, agent in enumerate(self.agents):
                action = agent.select_action(states[i])
                actions.append(action)
            
            # í†µì‹  (ì„ íƒì )
            if self.config.communication_enabled:
                actions = self.communication_network.communicate(actions, states)
            
            # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            next_states, rewards, dones, _ = env.step(actions)
            
            # ê²½í—˜ ì €ì¥
            for i, agent in enumerate(self.agents):
                agent.store_experience(states[i], actions[i], rewards[i], 
                                     next_states[i], dones[i])
                
                # ê³µìœ  ê²½í—˜ ì €ì¥
                if self.config.shared_experience:
                    self.shared_buffer.append((states[i], actions[i], rewards[i], 
                                             next_states[i], dones[i]))
                
                episode_rewards[i] += rewards[i]
            
            states = next_states
            done = any(dones)
        
        # ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        for agent in self.agents:
            agent.update()
        
        # ê³µìœ  ê²½í—˜ì„ í†µí•œ ì¶”ê°€ í•™ìŠµ
        if self.config.shared_experience:
            self._learn_from_shared_experience()
        
        # ì„±ëŠ¥ ê¸°ë¡
        for i, reward in enumerate(episode_rewards):
            self.agent_performances[i].append(reward)
        
        return episode_rewards
    
    def _learn_from_shared_experience(self):
        """ê³µìœ  ê²½í—˜ìœ¼ë¡œë¶€í„° í•™ìŠµ"""
        if len(self.shared_buffer) < self.agent_config.batch_size:
            return
        
        # ê³µìœ  ê²½í—˜ì„ ëª¨ë“  ì—ì´ì „íŠ¸ì— ì ìš©
        for agent in self.agents:
            batch = random.sample(self.shared_buffer, self.agent_config.batch_size)
            states, actions, rewards, next_states, dones = zip(*batch)
            
            states = torch.FloatTensor(states).to(agent.device)
            actions = torch.FloatTensor(actions).to(agent.device)
            rewards = torch.FloatTensor(rewards).to(agent.device)
            next_states = torch.FloatTensor(next_states).to(agent.device)
            dones = torch.FloatTensor(dones).to(agent.device)
            
            # ì¶”ê°€ í•™ìŠµ
            next_actions = agent.actor_target(next_states)
            target_q = agent.critic_target(next_states, next_actions)
            target_q = rewards + (1 - dones) * agent.config.gamma * target_q
            
            current_q = agent.critic(states, actions)
            critic_loss = F.mse_loss(current_q, target_q.detach())
            
            agent.critic_optimizer.zero_grad()
            critic_loss.backward()
            agent.critic_optimizer.step()
    
    def get_best_agent(self) -> int:
        """ìµœê³  ì„±ëŠ¥ ì—ì´ì „íŠ¸ ë°˜í™˜"""
        avg_performances = [np.mean(perf[-100:]) if perf else 0.0 
                           for perf in self.agent_performances]
        return np.argmax(avg_performances)
    
    def save_models(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        for i, agent in enumerate(self.agents):
            torch.save({
                'actor_state_dict': agent.actor.state_dict(),
                'critic_state_dict': agent.critic.state_dict(),
                'actor_target_state_dict': agent.actor_target.state_dict(),
                'critic_target_state_dict': agent.critic_target.state_dict(),
            }, f"{path}/agent_{i}.pth")
    
    def load_models(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        for i, agent in enumerate(self.agents):
            checkpoint = torch.load(f"{path}/agent_{i}.pth")
            agent.actor.load_state_dict(checkpoint['actor_state_dict'])
            agent.critic.load_state_dict(checkpoint['critic_state_dict'])
            agent.actor_target.load_state_dict(checkpoint['actor_target_state_dict'])
            agent.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])

class CommunicationNetwork:
    """ì—ì´ì „íŠ¸ ê°„ í†µì‹  ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, num_agents: int):
        self.num_agents = num_agents
        self.communication_matrix = np.ones((num_agents, num_agents)) / num_agents
    
    def communicate(self, actions: List[np.ndarray], states: List[np.ndarray]) -> List[np.ndarray]:
        """ì—ì´ì „íŠ¸ ê°„ í†µì‹ """
        # ê°„ë‹¨í•œ ê°€ì¤‘ í‰ê·  í†µì‹ 
        communicated_actions = []
        
        for i in range(self.num_agents):
            weighted_action = np.zeros_like(actions[i])
            
            for j in range(self.num_agents):
                weight = self.communication_matrix[i, j]
                weighted_action += weight * actions[j]
            
            communicated_actions.append(weighted_action)
        
        return communicated_actions
    
    def update_communication_matrix(self, performances: List[float]):
        """í†µì‹  ë§¤íŠ¸ë¦­ìŠ¤ ì—…ë°ì´íŠ¸"""
        # ì„±ëŠ¥ ê¸°ë°˜ í†µì‹  ê°€ì¤‘ì¹˜ ì¡°ì •
        performance_array = np.array(performances)
        normalized_performance = performance_array / np.sum(performance_array)
        
        # ì„±ëŠ¥ì´ ì¢‹ì€ ì—ì´ì „íŠ¸ì—ê²Œ ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        for i in range(self.num_agents):
            for j in range(self.num_agents):
                if i != j:
                    self.communication_matrix[i, j] = normalized_performance[j]
                else:
                    self.communication_matrix[i, j] = 0.5  # ìê¸° ìì‹ ì—ê²ŒëŠ” ì¤‘ê°„ ê°€ì¤‘ì¹˜
```

## ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„**

### ğŸ“‹ **ì™„ë£Œëœ ì‘ì—…**
- âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‹œìŠ¤í…œ ì„¤ê³„ (Transformer, BERT, GPT)
- âœ… ê°•í™”í•™ìŠµ ê³ ë„í™” ì‹œìŠ¤í…œ (ë‹¤ì¤‘ ì—ì´ì „íŠ¸, ë©”íƒ€ëŸ¬ë‹)
- âœ… ì•™ìƒë¸” ì‹œìŠ¤í…œ ì„¤ê³„ (ë™ì  ì•™ìƒë¸”, ìŠ¤íƒœí‚¹, ë¶€ìŠ¤íŒ…)

### ğŸ”„ **ì§„í–‰ ì¤‘ì¸ ì‘ì—…**
- ğŸ”„ ìë™í™”ëœ ML ì‹œìŠ¤í…œ (AutoML, í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”)
- ğŸ”„ ì‹¤ì‹œê°„ ì¶”ë¡  ì—”ì§„ (ëª¨ë¸ ì••ì¶•, ì–‘ìí™”)

### â³ **ë‹¤ìŒ ë‹¨ê³„**
1. **ìë™í™”ëœ ML ì‹œìŠ¤í…œ** ë¬¸ì„œ ìƒì„±
2. **ì‹¤ì‹œê°„ ì¶”ë¡  ì—”ì§„** ë¬¸ì„œ ìƒì„±
3. **Phase 5.3 ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°** ë¬¸ì„œ ìƒì„±

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-01-31
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: 2024-02-01 (ìë™í™”ëœ ML ì‹œìŠ¤í…œ)
**ê³ ê¸‰ ML ëª©í‘œ**: 95% ì˜ˆì¸¡ ì •í™•ë„, < 10ms ì¶”ë¡ , 50% ë¹ ë¥¸ í•™ìŠµ
**ê³ ê¸‰ ML ì„±ê³¼**: Transformer ëª¨ë¸, ë‹¤ì¤‘ ì—ì´ì „íŠ¸ RL, ì•™ìƒë¸” ì‹œìŠ¤í…œ 