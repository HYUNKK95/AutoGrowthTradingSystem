# ğŸ§  Phase 3.5.4: ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ (í•™ìŠµ + í’ˆì§ˆê´€ë¦¬ + ëª¨ë‹ˆí„°ë§ í†µí•©)

## ğŸ“‹ **ê°œìš”**

### ğŸ¯ **ëª©í‘œ**
- **ì¦ë¶„ í•™ìŠµ**: ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸
- **ê°œë… ë“œë¦¬í”„íŠ¸ ê°ì§€**: ì‹œì¥ íŒ¨í„´ ë³€í™”ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€
- **ì ì‘í˜• íŒŒë¼ë¯¸í„°**: ì‹œì¥ ìƒí™©ì— ë”°ë¼ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìë™ ì¡°ì •
- **ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬**: ë°ì´í„° ê²€ì¦, ì •ì œ, ì¦ê°• ì‹œìŠ¤í…œ
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ì‹œìŠ¤í…œ ì„±ëŠ¥ ë° ì „ëµ ì„±ê³¼ ì‹¤ì‹œê°„ ì¶”ì 
- **ì‹¤ì‹œê°„ ëª¨ë¸ ì—…ë°ì´íŠ¸**: 5ë¶„ ì´ë‚´ ëª¨ë¸ ì¬í›ˆë ¨ ì™„ë£Œ
- **ë©”íƒ€ëŸ¬ë‹**: ë‹¤ì–‘í•œ ì‹œì¥ ìƒí™©ì—ì„œ ìµœì ì˜ í•™ìŠµ ë°©ë²• ì„ íƒ

### ğŸ“Š **ì„±ëŠ¥ ëª©í‘œ**
- **ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œê°„**: < 5ë¶„ ì˜¨ë¼ì¸ í•™ìŠµ ì™„ë£Œ
- **ê°œë… ë“œë¦¬í”„íŠ¸ ê°ì§€**: < 1ì‹œê°„ ë³€í™” ê°ì§€
- **ì ì‘ ì†ë„**: < 10ë¶„ íŒŒë¼ë¯¸í„° ì¡°ì •
- **ë°ì´í„° í’ˆì§ˆ ì •í™•ë„**: > 99.9% ë°ì´í„° ì •í™•ë„
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: < 100ms ëª¨ë‹ˆí„°ë§ ì§€ì—°
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: < 2GB ëª¨ë¸ ë©”ëª¨ë¦¬
- **í•™ìŠµ ì •í™•ë„**: > 70% ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì •í™•ë„

## ğŸ—ï¸ **ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

### ğŸ“ **ì‹œìŠ¤í…œ êµ¬ì¡°**
```
online-learning/
â”œâ”€â”€ incremental-learning/                 # ì¦ë¶„ í•™ìŠµ
â”‚   â”œâ”€â”€ streaming-data-processor/        # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ model-updater/                   # ëª¨ë¸ ì—…ë°ì´í„°
â”‚   â”œâ”€â”€ memory-management/               # ë©”ëª¨ë¦¬ ê´€ë¦¬
â”‚   â””â”€â”€ learning-rate-adapter/           # í•™ìŠµë¥  ì ì‘ê¸°
â”œâ”€â”€ concept-drift-detection/              # ê°œë… ë“œë¦¬í”„íŠ¸ ê°ì§€
â”‚   â”œâ”€â”€ drift-detector/                  # ë“œë¦¬í”„íŠ¸ ê°ì§€ê¸°
â”‚   â”œâ”€â”€ change-point-detection/          # ë³€í™”ì  ê°ì§€
â”‚   â”œâ”€â”€ distribution-monitor/            # ë¶„í¬ ëª¨ë‹ˆí„°
â”‚   â””â”€â”€ drift-alert-system/              # ë“œë¦¬í”„íŠ¸ ì•Œë¦¼
â”œâ”€â”€ adaptive-parameters/                  # ì ì‘í˜• íŒŒë¼ë¯¸í„°
â”‚   â”œâ”€â”€ parameter-optimizer/             # íŒŒë¼ë¯¸í„° ìµœì í™”
â”‚   â”œâ”€â”€ market-regime-detector/          # ì‹œì¥ ì²´ì œ ê°ì§€
â”‚   â”œâ”€â”€ dynamic-adjustment/              # ë™ì  ì¡°ì •
â”‚   â””â”€â”€ performance-tracker/             # ì„±ëŠ¥ ì¶”ì 
â”œâ”€â”€ meta-learning/                        # ë©”íƒ€ëŸ¬ë‹
â”‚   â”œâ”€â”€ algorithm-selector/              # ì•Œê³ ë¦¬ì¦˜ ì„ íƒê¸°
â”‚   â”œâ”€â”€ hyperparameter-optimizer/        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
â”‚   â”œâ”€â”€ ensemble-manager/                # ì•™ìƒë¸” ê´€ë¦¬
â”‚   â””â”€â”€ learning-strategy-adapter/       # í•™ìŠµ ì „ëµ ì ì‘
â”œâ”€â”€ model-updating/                       # ëª¨ë¸ ì—…ë°ì´íŠ¸
â”‚   â”œâ”€â”€ version-control/                 # ë²„ì „ ê´€ë¦¬
â”‚   â”œâ”€â”€ rollback-manager/                # ë¡¤ë°± ê´€ë¦¬
â”‚   â”œâ”€â”€ a-b-testing/                     # A/B í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ deployment-manager/              # ë°°í¬ ê´€ë¦¬
â”œâ”€â”€ performance-monitoring/               # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    â”œâ”€â”€ real-time-metrics/               # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­
    â”œâ”€â”€ model-health-monitor/            # ëª¨ë¸ ê±´ê°•ë„ ëª¨ë‹ˆí„°
    â”œâ”€â”€ alert-system/                    # ì•Œë¦¼ ì‹œìŠ¤í…œ
    â””â”€â”€ performance-dashboard/           # ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ
â”œâ”€â”€ data-quality-management/              # ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬
    â”œâ”€â”€ data-validation/                 # ë°ì´í„° ê²€ì¦
    â”œâ”€â”€ data-cleansing/                  # ë°ì´í„° ì •ì œ
    â”œâ”€â”€ data-augmentation/               # ë°ì´í„° ì¦ê°•
    â”œâ”€â”€ outlier-detection/               # ì´ìƒì¹˜ ê°ì§€
    â””â”€â”€ quality-monitoring/              # í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
â””â”€â”€ real-time-monitoring/                 # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
    â”œâ”€â”€ system-performance/              # ì‹œìŠ¤í…œ ì„±ëŠ¥
    â”œâ”€â”€ strategy-performance/            # ì „ëµ ì„±ê³¼
    â”œâ”€â”€ risk-monitoring/                 # ìœ„í—˜ ëª¨ë‹ˆí„°ë§
    â””â”€â”€ alert-management/                # ì•Œë¦¼ ê´€ë¦¬
```

## ğŸ”§ **ì¦ë¶„ í•™ìŠµ ì‹œìŠ¤í…œ**

### ğŸ“¦ **ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ê¸°**

```python
# online-learning/incremental-learning/streaming_data_processor.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
import asyncio
from collections import deque
import threading
import queue

logger = logging.getLogger(__name__)

@dataclass
class DataPoint:
    """ë°ì´í„° í¬ì¸íŠ¸"""
    timestamp: datetime
    features: Dict[str, float]
    target: Optional[float] = None
    metadata: Dict[str, Any] = None

@dataclass
class DataBatch:
    """ë°ì´í„° ë°°ì¹˜"""
    batch_id: str
    data_points: List[DataPoint]
    batch_size: int
    timestamp: datetime
    features_mean: Dict[str, float]
    features_std: Dict[str, float]

class StreamingDataProcessor:
    """ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def __init__(self, batch_size: int = 100, max_buffer_size: int = 1000):
        self.batch_size = batch_size
        self.max_buffer_size = max_buffer_size
        self.data_buffer = deque(maxlen=max_buffer_size)
        self.batch_queue = queue.Queue()
        self.processing_task = None
        self.is_running = False
        
        # í†µê³„ ì •ë³´
        self.feature_stats = {}
        self.total_processed = 0
        self.last_update_time = datetime.now()
    
    async def start_processing(self):
        """ë°ì´í„° ì²˜ë¦¬ ì‹œì‘"""
        self.is_running = True
        self.processing_task = asyncio.create_task(self._processing_loop())
        logger.info("ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    
    async def stop_processing(self):
        """ë°ì´í„° ì²˜ë¦¬ ì¤‘ì§€"""
        self.is_running = False
        if self.processing_task:
            self.processing_task.cancel()
        logger.info("ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ ì¤‘ì§€")
    
    async def add_data_point(self, data_point: DataPoint):
        """ë°ì´í„° í¬ì¸íŠ¸ ì¶”ê°€"""
        try:
            # ë²„í¼ì— ì¶”ê°€
            self.data_buffer.append(data_point)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_feature_stats(data_point)
            
            # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ë°°ì¹˜ ìƒì„±
            if len(self.data_buffer) >= self.batch_size:
                await self._create_batch()
                
        except Exception as e:
            logger.error(f"ë°ì´í„° í¬ì¸íŠ¸ ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    async def _processing_loop(self):
        """ì²˜ë¦¬ ë£¨í”„"""
        while self.is_running:
            try:
                # ë°°ì¹˜ íì—ì„œ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
                try:
                    batch = self.batch_queue.get_nowait()
                    await self._process_batch(batch)
                except queue.Empty:
                    await asyncio.sleep(0.1)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ì²˜ë¦¬ ë£¨í”„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def _create_batch(self):
        """ë°°ì¹˜ ìƒì„±"""
        try:
            if len(self.data_buffer) < self.batch_size:
                return
            
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
            batch_data = list(self.data_buffer)[:self.batch_size]
            
            # ë°°ì¹˜ í†µê³„ ê³„ì‚°
            features_mean = self._calculate_batch_mean(batch_data)
            features_std = self._calculate_batch_std(batch_data, features_mean)
            
            # ë°°ì¹˜ ìƒì„±
            batch = DataBatch(
                batch_id=f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                data_points=batch_data,
                batch_size=len(batch_data),
                timestamp=datetime.now(),
                features_mean=features_mean,
                features_std=features_std
            )
            
            # íì— ë°°ì¹˜ ì¶”ê°€
            self.batch_queue.put(batch)
            
            # ë²„í¼ì—ì„œ ì²˜ë¦¬ëœ ë°ì´í„° ì œê±°
            for _ in range(self.batch_size):
                if self.data_buffer:
                    self.data_buffer.popleft()
            
            logger.info(f"ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch.batch_id} - {len(batch_data)}ê°œ ë°ì´í„°")
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ìƒì„± ì˜¤ë¥˜: {e}")
    
    def _update_feature_stats(self, data_point: DataPoint):
        """íŠ¹ì„± í†µê³„ ì—…ë°ì´íŠ¸"""
        for feature_name, feature_value in data_point.features.items():
            if feature_name not in self.feature_stats:
                self.feature_stats[feature_name] = {
                    'count': 0,
                    'sum': 0.0,
                    'sum_sq': 0.0,
                    'min': float('inf'),
                    'max': float('-inf')
                }
            
            stats = self.feature_stats[feature_name]
            stats['count'] += 1
            stats['sum'] += feature_value
            stats['sum_sq'] += feature_value ** 2
            stats['min'] = min(stats['min'], feature_value)
            stats['max'] = max(stats['max'], feature_value)
    
    def _calculate_batch_mean(self, batch_data: List[DataPoint]) -> Dict[str, float]:
        """ë°°ì¹˜ í‰ê·  ê³„ì‚°"""
        if not batch_data:
            return {}
        
        feature_sums = {}
        feature_counts = {}
        
        for data_point in batch_data:
            for feature_name, feature_value in data_point.features.items():
                if feature_name not in feature_sums:
                    feature_sums[feature_name] = 0.0
                    feature_counts[feature_name] = 0
                
                feature_sums[feature_name] += feature_value
                feature_counts[feature_name] += 1
        
        return {
            feature_name: feature_sums[feature_name] / feature_counts[feature_name]
            for feature_name in feature_sums
        }
    
    def _calculate_batch_std(self, batch_data: List[DataPoint], 
                           batch_mean: Dict[str, float]) -> Dict[str, float]:
        """ë°°ì¹˜ í‘œì¤€í¸ì°¨ ê³„ì‚°"""
        if not batch_data:
            return {}
        
        feature_sums_sq = {}
        feature_counts = {}
        
        for data_point in batch_data:
            for feature_name, feature_value in data_point.features.items():
                if feature_name not in feature_sums_sq:
                    feature_sums_sq[feature_name] = 0.0
                    feature_counts[feature_name] = 0
                
                mean = batch_mean.get(feature_name, 0.0)
                feature_sums_sq[feature_name] += (feature_value - mean) ** 2
                feature_counts[feature_name] += 1
        
        return {
            feature_name: np.sqrt(feature_sums_sq[feature_name] / feature_counts[feature_name])
            for feature_name in feature_sums_sq
        }
    
    async def _process_batch(self, batch: DataBatch):
        """ë°°ì¹˜ ì²˜ë¦¬"""
        try:
            # ë°ì´í„° ì •ê·œí™”
            normalized_data = self._normalize_batch(batch)
            
            # ëª¨ë¸ ì—…ë°ì´íŠ¸ ìš”ì²­
            await self._request_model_update(normalized_data)
            
            # ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜ ì—…ë°ì´íŠ¸
            self.total_processed += batch.batch_size
            self.last_update_time = datetime.now()
            
            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {batch.batch_id}")
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def _normalize_batch(self, batch: DataBatch) -> List[DataPoint]:
        """ë°°ì¹˜ ì •ê·œí™”"""
        normalized_data = []
        
        for data_point in batch.data_points:
            normalized_features = {}
            
            for feature_name, feature_value in data_point.features.items():
                mean = batch.features_mean.get(feature_name, 0.0)
                std = batch.features_std.get(feature_name, 1.0)
                
                if std > 0:
                    normalized_value = (feature_value - mean) / std
                else:
                    normalized_value = feature_value - mean
                
                normalized_features[feature_name] = normalized_value
            
            normalized_data_point = DataPoint(
                timestamp=data_point.timestamp,
                features=normalized_features,
                target=data_point.target,
                metadata=data_point.metadata
            )
            
            normalized_data.append(normalized_data_point)
        
        return normalized_data
    
    async def _request_model_update(self, normalized_data: List[DataPoint]):
        """ëª¨ë¸ ì—…ë°ì´íŠ¸ ìš”ì²­"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëª¨ë¸ ì—…ë°ì´í„°ì— ìš”ì²­
        # ì—¬ê¸°ì„œëŠ” ë¡œê¹…ë§Œ ìˆ˜í–‰
        logger.info(f"ëª¨ë¸ ì—…ë°ì´íŠ¸ ìš”ì²­: {len(normalized_data)}ê°œ ë°ì´í„°")
```

### ğŸ“¦ **ê°œë… ë“œë¦¬í”„íŠ¸ ê°ì§€ê¸°**

```python
# online-learning/concept-drift-detection/concept_drift_detector.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from scipy import stats
from sklearn.metrics import kl_divergence
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

@dataclass
class DriftAlert:
    """ë“œë¦¬í”„íŠ¸ ì•Œë¦¼"""
    alert_id: str
    drift_type: str
    severity: str  # 'low', 'medium', 'high', 'critical'
    confidence: float
    affected_features: List[str]
    timestamp: datetime
    description: str
    recommendations: List[str]

class ConceptDriftDetector:
    """ê°œë… ë“œë¦¬í”„íŠ¸ ê°ì§€ê¸°"""
    
    def __init__(self, window_size: int = 1000, detection_threshold: float = 0.05):
        self.window_size = window_size
        self.detection_threshold = detection_threshold
        self.reference_window = []
        self.current_window = []
        self.drift_history = []
        self.detection_methods = {
            'statistical_test': self._statistical_test_detection,
            'distribution_comparison': self._distribution_comparison_detection,
            'change_point_detection': self._change_point_detection,
            'kl_divergence': self._kl_divergence_detection
        }
    
    def add_data_point(self, data_point: DataPoint):
        """ë°ì´í„° í¬ì¸íŠ¸ ì¶”ê°€"""
        try:
            # í˜„ì¬ ìœˆë„ìš°ì— ì¶”ê°€
            self.current_window.append(data_point)
            
            # ìœˆë„ìš° í¬ê¸° ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
            if len(self.current_window) > self.window_size:
                self.current_window.pop(0)
            
            # ì°¸ì¡° ìœˆë„ìš°ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì´ˆê¸°í™”
            if not self.reference_window and len(self.current_window) >= self.window_size // 2:
                self.reference_window = self.current_window.copy()
            
            # ë“œë¦¬í”„íŠ¸ ê°ì§€ ìˆ˜í–‰
            if len(self.current_window) >= self.window_size // 2:
                drift_detected = self._detect_drift()
                
                if drift_detected:
                    self._handle_drift_detection(drift_detected)
                
        except Exception as e:
            logger.error(f"ë°ì´í„° í¬ì¸íŠ¸ ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    def _detect_drift(self) -> Optional[DriftAlert]:
        """ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        try:
            if len(self.reference_window) < self.window_size // 2:
                return None
            
            # ê° ê°ì§€ ë°©ë²•ìœ¼ë¡œ ë“œë¦¬í”„íŠ¸ í™•ì¸
            drift_results = {}
            
            for method_name, method_func in self.detection_methods.items():
                try:
                    result = method_func()
                    if result:
                        drift_results[method_name] = result
                except Exception as e:
                    logger.error(f"ë“œë¦¬í”„íŠ¸ ê°ì§€ ë°©ë²• ì˜¤ë¥˜ {method_name}: {e}")
            
            # ì¢…í•©ì ì¸ ë“œë¦¬í”„íŠ¸ íŒë‹¨
            if drift_results:
                return self._combine_drift_results(drift_results)
            
            return None
            
        except Exception as e:
            logger.error(f"ë“œë¦¬í”„íŠ¸ ê°ì§€ ì˜¤ë¥˜: {e}")
            return None
    
    def _statistical_test_detection(self) -> Optional[Dict]:
        """í†µê³„ì  ê²€ì •ì„ í†µí•œ ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        try:
            if len(self.reference_window) < 30 or len(self.current_window) < 30:
                return None
            
            # ê° íŠ¹ì„±ì— ëŒ€í•´ t-ê²€ì • ìˆ˜í–‰
            drift_scores = {}
            
            for feature_name in self.reference_window[0].features.keys():
                ref_values = [dp.features[feature_name] for dp in self.reference_window]
                cur_values = [dp.features[feature_name] for dp in self.current_window]
                
                # t-ê²€ì •
                t_stat, p_value = stats.ttest_ind(ref_values, cur_values)
                
                # p-valueê°€ ì„ê³„ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ ë“œë¦¬í”„íŠ¸ë¡œ íŒë‹¨
                if p_value < self.detection_threshold:
                    drift_scores[feature_name] = {
                        'p_value': p_value,
                        't_stat': t_stat,
                        'severity': 'high' if p_value < 0.01 else 'medium'
                    }
            
            if drift_scores:
                return {
                    'method': 'statistical_test',
                    'drift_scores': drift_scores,
                    'confidence': 1 - min(score['p_value'] for score in drift_scores.values())
                }
            
            return None
            
        except Exception as e:
            logger.error(f"í†µê³„ì  ê²€ì • ë“œë¦¬í”„íŠ¸ ê°ì§€ ì˜¤ë¥˜: {e}")
            return None
    
    def _distribution_comparison_detection(self) -> Optional[Dict]:
        """ë¶„í¬ ë¹„êµë¥¼ í†µí•œ ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        try:
            if len(self.reference_window) < 50 or len(self.current_window) < 50:
                return None
            
            # ê° íŠ¹ì„±ì— ëŒ€í•´ ë¶„í¬ ë¹„êµ
            drift_scores = {}
            
            for feature_name in self.reference_window[0].features.keys():
                ref_values = [dp.features[feature_name] for dp in self.reference_window]
                cur_values = [dp.features[feature_name] for dp in self.current_window]
                
                # Kolmogorov-Smirnov ê²€ì •
                ks_stat, ks_p_value = stats.ks_2samp(ref_values, cur_values)
                
                # Anderson-Darling ê²€ì •
                try:
                    ad_stat, ad_critical_values, ad_significance_levels = stats.anderson_ksamp([ref_values, cur_values])
                    ad_p_value = 1 - ad_significance_levels[0] / 100
                except:
                    ad_p_value = 1.0
                
                # ì¢…í•©ì ì¸ ë“œë¦¬í”„íŠ¸ ì ìˆ˜
                combined_p_value = (ks_p_value + ad_p_value) / 2
                
                if combined_p_value < self.detection_threshold:
                    drift_scores[feature_name] = {
                        'ks_stat': ks_stat,
                        'ks_p_value': ks_p_value,
                        'ad_p_value': ad_p_value,
                        'combined_p_value': combined_p_value,
                        'severity': 'high' if combined_p_value < 0.01 else 'medium'
                    }
            
            if drift_scores:
                return {
                    'method': 'distribution_comparison',
                    'drift_scores': drift_scores,
                    'confidence': 1 - min(score['combined_p_value'] for score in drift_scores.values())
                }
            
            return None
            
        except Exception as e:
            logger.error(f"ë¶„í¬ ë¹„êµ ë“œë¦¬í”„íŠ¸ ê°ì§€ ì˜¤ë¥˜: {e}")
            return None
    
    def _change_point_detection(self) -> Optional[Dict]:
        """ë³€í™”ì  ê°ì§€ë¥¼ í†µí•œ ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        try:
            if len(self.current_window) < 100:
                return None
            
            # ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜
            time_series_data = []
            for dp in self.current_window:
                # íŠ¹ì„±ë“¤ì˜ í‰ê· ê°’ ì‚¬ìš©
                avg_feature = np.mean(list(dp.features.values()))
                time_series_data.append(avg_feature)
            
            # CUSUM (Cumulative Sum) ë³€í™”ì  ê°ì§€
            mean_val = np.mean(time_series_data)
            std_val = np.std(time_series_data)
            
            if std_val == 0:
                return None
            
            # CUSUM ê³„ì‚°
            cusum_positive = []
            cusum_negative = []
            
            for i, value in enumerate(time_series_data):
                if i == 0:
                    cusum_positive.append(0)
                    cusum_negative.append(0)
                else:
                    # ì–‘ì˜ CUSUM
                    pos_val = max(0, cusum_positive[i-1] + (value - mean_val) / std_val)
                    cusum_positive.append(pos_val)
                    
                    # ìŒì˜ CUSUM
                    neg_val = max(0, cusum_negative[i-1] - (value - mean_val) / std_val)
                    cusum_negative.append(neg_val)
            
            # ì„ê³„ê°’ ì„¤ì • (ì¼ë°˜ì ìœ¼ë¡œ 5 ì‚¬ìš©)
            threshold = 5
            
            # ë³€í™”ì  ê°ì§€
            change_points = []
            for i in range(len(cusum_positive)):
                if cusum_positive[i] > threshold or cusum_negative[i] > threshold:
                    change_points.append(i)
            
            if change_points:
                # ìµœê·¼ ë³€í™”ì 
                latest_change = max(change_points)
                change_ratio = latest_change / len(time_series_data)
                
                return {
                    'method': 'change_point_detection',
                    'change_points': change_points,
                    'latest_change': latest_change,
                    'change_ratio': change_ratio,
                    'confidence': min(1.0, len(change_points) / 10),
                    'severity': 'high' if change_ratio > 0.8 else 'medium'
                }
            
            return None
            
        except Exception as e:
            logger.error(f"ë³€í™”ì  ê°ì§€ ë“œë¦¬í”„íŠ¸ ê°ì§€ ì˜¤ë¥˜: {e}")
            return None
    
    def _kl_divergence_detection(self) -> Optional[Dict]:
        """KL ë°œì‚°ì„ í†µí•œ ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        try:
            if len(self.reference_window) < 50 or len(self.current_window) < 50:
                return None
            
            # ê° íŠ¹ì„±ì— ëŒ€í•´ KL ë°œì‚° ê³„ì‚°
            drift_scores = {}
            
            for feature_name in self.reference_window[0].features.keys():
                ref_values = [dp.features[feature_name] for dp in self.reference_window]
                cur_values = [dp.features[feature_name] for dp in self.current_window]
                
                # íˆìŠ¤í† ê·¸ë¨ ìƒì„±
                ref_hist, ref_bins = np.histogram(ref_values, bins=20, density=True)
                cur_hist, cur_bins = np.histogram(cur_values, bins=20, density=True)
                
                # KL ë°œì‚° ê³„ì‚°
                try:
                    kl_div = kl_divergence(ref_hist, cur_hist)
                    
                    # ì„ê³„ê°’ ì„¤ì • (ê²½í—˜ì ìœ¼ë¡œ 0.1 ì‚¬ìš©)
                    if kl_div > 0.1:
                        drift_scores[feature_name] = {
                            'kl_divergence': kl_div,
                            'severity': 'high' if kl_div > 0.5 else 'medium'
                        }
                except:
                    continue
            
            if drift_scores:
                return {
                    'method': 'kl_divergence',
                    'drift_scores': drift_scores,
                    'confidence': min(1.0, max(score['kl_divergence'] for score in drift_scores.values())),
                    'severity': 'high' if any(score['severity'] == 'high' for score in drift_scores.values()) else 'medium'
                }
            
            return None
            
        except Exception as e:
            logger.error(f"KL ë°œì‚° ë“œë¦¬í”„íŠ¸ ê°ì§€ ì˜¤ë¥˜: {e}")
            return None
    
    def _combine_drift_results(self, drift_results: Dict) -> DriftAlert:
        """ë“œë¦¬í”„íŠ¸ ê²°ê³¼ ê²°í•©"""
        try:
            # ëª¨ë“  ê°ì§€ëœ íŠ¹ì„± ìˆ˜ì§‘
            all_features = set()
            for result in drift_results.values():
                if 'drift_scores' in result:
                    all_features.update(result['drift_scores'].keys())
            
            # ì¢…í•©ì ì¸ ì‹ ë¢°ë„ ê³„ì‚°
            confidences = [result.get('confidence', 0.0) for result in drift_results.values()]
            overall_confidence = np.mean(confidences)
            
            # ì‹¬ê°ë„ ê²°ì •
            severities = []
            for result in drift_results.values():
                if 'severity' in result:
                    severities.append(result['severity'])
                elif 'drift_scores' in result:
                    for score in result['drift_scores'].values():
                        if 'severity' in score:
                            severities.append(score['severity'])
            
            overall_severity = 'high' if 'high' in severities else 'medium'
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            recommendations = [
                "ëª¨ë¸ ì¬í›ˆë ¨ ê³ ë ¤",
                "ë°ì´í„° ë¶„í¬ ì¬ê²€í† ",
                "íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì—…ë°ì´íŠ¸"
            ]
            
            if overall_severity == 'high':
                recommendations.append("ì¦‰ì‹œ ëª¨ë¸ ì—…ë°ì´íŠ¸ í•„ìš”")
            
            return DriftAlert(
                alert_id=f"drift_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                drift_type="concept_drift",
                severity=overall_severity,
                confidence=overall_confidence,
                affected_features=list(all_features),
                timestamp=datetime.now(),
                description=f"ê°œë… ë“œë¦¬í”„íŠ¸ ê°ì§€ë¨ - {len(drift_results)}ê°œ ë°©ë²•ìœ¼ë¡œ í™•ì¸",
                recommendations=recommendations
            )
            
        except Exception as e:
            logger.error(f"ë“œë¦¬í”„íŠ¸ ê²°ê³¼ ê²°í•© ì˜¤ë¥˜: {e}")
            return None
    
    def _handle_drift_detection(self, drift_alert: DriftAlert):
        """ë“œë¦¬í”„íŠ¸ ê°ì§€ ì²˜ë¦¬"""
        try:
            # ë“œë¦¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.drift_history.append(drift_alert)
            
            # ì•Œë¦¼ ë°œì†¡
            self._send_drift_alert(drift_alert)
            
            # ì°¸ì¡° ìœˆë„ìš° ì—…ë°ì´íŠ¸
            self._update_reference_window()
            
            logger.warning(f"ê°œë… ë“œë¦¬í”„íŠ¸ ê°ì§€: {drift_alert.description}")
            
        except Exception as e:
            logger.error(f"ë“œë¦¬í”„íŠ¸ ê°ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def _update_reference_window(self):
        """ì°¸ì¡° ìœˆë„ìš° ì—…ë°ì´íŠ¸"""
        try:
            # í˜„ì¬ ìœˆë„ìš°ë¥¼ ìƒˆë¡œìš´ ì°¸ì¡° ìœˆë„ìš°ë¡œ ì„¤ì •
            if len(self.current_window) >= self.window_size // 2:
                self.reference_window = self.current_window.copy()
                logger.info("ì°¸ì¡° ìœˆë„ìš° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì°¸ì¡° ìœˆë„ìš° ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def _send_drift_alert(self, drift_alert: DriftAlert):
        """ë“œë¦¬í”„íŠ¸ ì•Œë¦¼ ë°œì†¡"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì•Œë¦¼ ì‹œìŠ¤í…œ ì‚¬ìš©
        logger.warning(f"ë“œë¦¬í”„íŠ¸ ì•Œë¦¼: {drift_alert.description}")
```

## ğŸ”§ **ì ì‘í˜• íŒŒë¼ë¯¸í„° ì‹œìŠ¤í…œ**

### ğŸ“¦ **ì ì‘í˜• íŒŒë¼ë¯¸í„° ìµœì í™”ê¸°**

```python
# online-learning/adaptive-parameters/parameter_optimizer.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
import optuna

logger = logging.getLogger(__name__)

@dataclass
class MarketRegime:
    """ì‹œì¥ ì²´ì œ"""
    regime_id: str
    regime_type: str  # 'trending', 'ranging', 'volatile', 'stable'
    volatility_level: float
    trend_strength: float
    volume_level: float
    start_time: datetime
    end_time: Optional[datetime] = None
    parameters: Dict[str, float] = None

@dataclass
class ParameterSet:
    """íŒŒë¼ë¯¸í„° ì„¸íŠ¸"""
    parameter_id: str
    parameters: Dict[str, float]
    performance_score: float
    market_regime: str
    created_time: datetime
    is_active: bool = False

class AdaptiveParameterOptimizer:
    """ì ì‘í˜• íŒŒë¼ë¯¸í„° ìµœì í™”ê¸°"""
    
    def __init__(self, optimization_interval: int = 3600):
        self.optimization_interval = optimization_interval
        self.current_regime = None
        self.parameter_history = []
        self.performance_history = []
        self.regime_history = []
        self.optimization_task = None
        
        # íŒŒë¼ë¯¸í„° ë²”ìœ„ ì •ì˜
        self.parameter_ranges = {
            'learning_rate': (0.001, 0.1),
            'batch_size': (32, 256),
            'momentum': (0.8, 0.99),
            'regularization': (0.001, 0.1),
            'dropout_rate': (0.1, 0.5)
        }
    
    async def start_optimization(self):
        """ìµœì í™” ì‹œì‘"""
        self.optimization_task = asyncio.create_task(self._optimization_loop())
        logger.info("ì ì‘í˜• íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
    
    async def stop_optimization(self):
        """ìµœì í™” ì¤‘ì§€"""
        if self.optimization_task:
            self.optimization_task.cancel()
        logger.info("ì ì‘í˜• íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘ì§€")
    
    async def _optimization_loop(self):
        """ìµœì í™” ë£¨í”„"""
        while True:
            try:
                # ì‹œì¥ ì²´ì œ ê°ì§€
                current_regime = await self._detect_market_regime()
                
                if current_regime != self.current_regime:
                    # ì²´ì œ ë³€ê²½ ê°ì§€
                    await self._handle_regime_change(current_regime)
                
                # íŒŒë¼ë¯¸í„° ìµœì í™” ìˆ˜í–‰
                await self._optimize_parameters()
                
                # ìµœì í™” ê°„ê²©ë§Œí¼ ëŒ€ê¸°
                await asyncio.sleep(self.optimization_interval)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ìµœì í™” ë£¨í”„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(self.optimization_interval)
    
    async def _detect_market_regime(self) -> MarketRegime:
        """ì‹œì¥ ì²´ì œ ê°ì§€"""
        try:
            # ìµœê·¼ ì‹œì¥ ë°ì´í„° ë¶„ì„
            market_data = await self._get_recent_market_data()
            
            if not market_data:
                return self.current_regime
            
            # ë³€ë™ì„± ê³„ì‚°
            volatility = self._calculate_volatility(market_data)
            
            # íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚°
            trend_strength = self._calculate_trend_strength(market_data)
            
            # ê±°ë˜ëŸ‰ ë ˆë²¨ ê³„ì‚°
            volume_level = self._calculate_volume_level(market_data)
            
            # ì²´ì œ ë¶„ë¥˜
            regime_type = self._classify_regime(volatility, trend_strength, volume_level)
            
            regime = MarketRegime(
                regime_id=f"regime_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                regime_type=regime_type,
                volatility_level=volatility,
                trend_strength=trend_strength,
                volume_level=volume_level,
                start_time=datetime.now()
            )
            
            return regime
            
        except Exception as e:
            logger.error(f"ì‹œì¥ ì²´ì œ ê°ì§€ ì˜¤ë¥˜: {e}")
            return self.current_regime
    
    def _calculate_volatility(self, market_data: List[Dict]) -> float:
        """ë³€ë™ì„± ê³„ì‚°"""
        try:
            if len(market_data) < 2:
                return 0.0
            
            returns = []
            for i in range(1, len(market_data)):
                prev_price = market_data[i-1].get('close', 0)
                curr_price = market_data[i].get('close', 0)
                
                if prev_price > 0:
                    ret = (curr_price - prev_price) / prev_price
                    returns.append(ret)
            
            if returns:
                return np.std(returns)
            
            return 0.0
            
        except Exception as e:
            logger.error(f"ë³€ë™ì„± ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _calculate_trend_strength(self, market_data: List[Dict]) -> float:
        """íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚°"""
        try:
            if len(market_data) < 20:
                return 0.0
            
            prices = [d.get('close', 0) for d in market_data]
            
            # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚°
            x = np.arange(len(prices)).reshape(-1, 1)
            y = np.array(prices)
            
            reg = LinearRegression()
            reg.fit(x, y)
            
            # RÂ² ì ìˆ˜ë¡œ íŠ¸ë Œë“œ ê°•ë„ ì¸¡ì •
            trend_strength = reg.score(x, y)
            
            return max(0.0, trend_strength)
            
        except Exception as e:
            logger.error(f"íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _calculate_volume_level(self, market_data: List[Dict]) -> float:
        """ê±°ë˜ëŸ‰ ë ˆë²¨ ê³„ì‚°"""
        try:
            if not market_data:
                return 0.0
            
            volumes = [d.get('volume', 0) for d in market_data]
            
            if volumes:
                # ìµœê·¼ ê±°ë˜ëŸ‰ì˜ í‰ê· 
                recent_volume = np.mean(volumes[-10:]) if len(volumes) >= 10 else np.mean(volumes)
                
                # ì „ì²´ ê¸°ê°„ í‰ê·  ëŒ€ë¹„ ìƒëŒ€ì  ê±°ë˜ëŸ‰
                avg_volume = np.mean(volumes)
                
                if avg_volume > 0:
                    return recent_volume / avg_volume
                
            return 1.0
            
        except Exception as e:
            logger.error(f"ê±°ë˜ëŸ‰ ë ˆë²¨ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 1.0
    
    def _classify_regime(self, volatility: float, trend_strength: float, 
                        volume_level: float) -> str:
        """ì²´ì œ ë¶„ë¥˜"""
        try:
            # ë³€ë™ì„± ê¸°ì¤€
            if volatility > 0.03:  # 3% ì´ìƒ
                if trend_strength > 0.7:
                    return 'trending'
                else:
                    return 'volatile'
            else:
                if trend_strength > 0.5:
                    return 'trending'
                else:
                    return 'ranging'
            
        except Exception as e:
            logger.error(f"ì²´ì œ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return 'stable'
    
    async def _handle_regime_change(self, new_regime: MarketRegime):
        """ì²´ì œ ë³€ê²½ ì²˜ë¦¬"""
        try:
            # ì´ì „ ì²´ì œ ì¢…ë£Œ
            if self.current_regime:
                self.current_regime.end_time = datetime.now()
                self.regime_history.append(self.current_regime)
            
            # ìƒˆ ì²´ì œ ì„¤ì •
            self.current_regime = new_regime
            
            # ì²´ì œë³„ ìµœì  íŒŒë¼ë¯¸í„° ë¡œë“œ
            optimal_params = self._get_optimal_parameters_for_regime(new_regime.regime_type)
            
            if optimal_params:
                await self._apply_parameters(optimal_params)
            
            logger.info(f"ì‹œì¥ ì²´ì œ ë³€ê²½: {new_regime.regime_type}")
            
        except Exception as e:
            logger.error(f"ì²´ì œ ë³€ê²½ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def _get_optimal_parameters_for_regime(self, regime_type: str) -> Optional[Dict[str, float]]:
        """ì²´ì œë³„ ìµœì  íŒŒë¼ë¯¸í„° ì¡°íšŒ"""
        try:
            # ì²´ì œë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            regime_parameters = {
                'trending': {
                    'learning_rate': 0.01,
                    'batch_size': 64,
                    'momentum': 0.9,
                    'regularization': 0.01,
                    'dropout_rate': 0.2
                },
                'ranging': {
                    'learning_rate': 0.005,
                    'batch_size': 128,
                    'momentum': 0.85,
                    'regularization': 0.05,
                    'dropout_rate': 0.3
                },
                'volatile': {
                    'learning_rate': 0.02,
                    'batch_size': 32,
                    'momentum': 0.95,
                    'regularization': 0.02,
                    'dropout_rate': 0.4
                },
                'stable': {
                    'learning_rate': 0.001,
                    'batch_size': 256,
                    'momentum': 0.8,
                    'regularization': 0.1,
                    'dropout_rate': 0.1
                }
            }
            
            return regime_parameters.get(regime_type, regime_parameters['stable'])
            
        except Exception as e:
            logger.error(f"ì²´ì œë³„ ìµœì  íŒŒë¼ë¯¸í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    async def _optimize_parameters(self):
        """íŒŒë¼ë¯¸í„° ìµœì í™”"""
        try:
            if not self.current_regime:
                return
            
            # ìµœê·¼ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
            performance_data = await self._get_recent_performance_data()
            
            if len(performance_data) < 100:
                return
            
            # Optunaë¥¼ ì‚¬ìš©í•œ ë² ì´ì§€ì•ˆ ìµœì í™”
            study = optuna.create_study(direction='maximize')
            
            def objective(trial):
                # íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
                params = {
                    'learning_rate': trial.suggest_float('learning_rate', 
                                                       self.parameter_ranges['learning_rate'][0],
                                                       self.parameter_ranges['learning_rate'][1],
                                                       log=True),
                    'batch_size': trial.suggest_int('batch_size',
                                                   self.parameter_ranges['batch_size'][0],
                                                   self.parameter_ranges['batch_size'][1]),
                    'momentum': trial.suggest_float('momentum',
                                                   self.parameter_ranges['momentum'][0],
                                                   self.parameter_ranges['momentum'][1]),
                    'regularization': trial.suggest_float('regularization',
                                                         self.parameter_ranges['regularization'][0],
                                                         self.parameter_ranges['regularization'][1],
                                                         log=True),
                    'dropout_rate': trial.suggest_float('dropout_rate',
                                                       self.parameter_ranges['dropout_rate'][0],
                                                       self.parameter_ranges['dropout_rate'][1])
                }
                
                # ì„±ëŠ¥ í‰ê°€
                performance_score = self._evaluate_parameters(params, performance_data)
                
                return performance_score
            
            # ìµœì í™” ì‹¤í–‰
            study.optimize(objective, n_trials=50, timeout=300)
            
            # ìµœì  íŒŒë¼ë¯¸í„° ì ìš©
            best_params = study.best_params
            best_score = study.best_value
            
            # íŒŒë¼ë¯¸í„° ì„¸íŠ¸ ìƒì„±
            parameter_set = ParameterSet(
                parameter_id=f"params_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                parameters=best_params,
                performance_score=best_score,
                market_regime=self.current_regime.regime_type,
                created_time=datetime.now(),
                is_active=True
            )
            
            # íŒŒë¼ë¯¸í„° ì ìš©
            await self._apply_parameters(best_params)
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.parameter_history.append(parameter_set)
            
            logger.info(f"íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ: {best_score:.4f}")
            
        except Exception as e:
            logger.error(f"íŒŒë¼ë¯¸í„° ìµœì í™” ì˜¤ë¥˜: {e}")
    
    def _evaluate_parameters(self, params: Dict[str, float], 
                           performance_data: List[Dict]) -> float:
        """íŒŒë¼ë¯¸í„° ì„±ëŠ¥ í‰ê°€"""
        try:
            # ê°„ë‹¨í•œ ì„±ëŠ¥ í‰ê°€ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •)
            # ì—¬ê¸°ì„œëŠ” íŒŒë¼ë¯¸í„° ì¡°í•©ì˜ ì í•©ì„±ì„ í‰ê°€
            
            score = 0.0
            
            # í•™ìŠµë¥  í‰ê°€
            if 0.001 <= params['learning_rate'] <= 0.1:
                score += 0.2
            
            # ë°°ì¹˜ í¬ê¸° í‰ê°€
            if 32 <= params['batch_size'] <= 256:
                score += 0.2
            
            # ëª¨ë©˜í…€ í‰ê°€
            if 0.8 <= params['momentum'] <= 0.99:
                score += 0.2
            
            # ì •ê·œí™” í‰ê°€
            if 0.001 <= params['regularization'] <= 0.1:
                score += 0.2
            
            # ë“œë¡­ì•„ì›ƒ í‰ê°€
            if 0.1 <= params['dropout_rate'] <= 0.5:
                score += 0.2
            
            return score
            
        except Exception as e:
            logger.error(f"íŒŒë¼ë¯¸í„° ì„±ëŠ¥ í‰ê°€ ì˜¤ë¥˜: {e}")
            return 0.0
    
    async def _apply_parameters(self, parameters: Dict[str, float]):
        """íŒŒë¼ë¯¸í„° ì ìš©"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëª¨ë¸ì— íŒŒë¼ë¯¸í„° ì ìš©
            logger.info(f"íŒŒë¼ë¯¸í„° ì ìš©: {parameters}")
            
        except Exception as e:
            logger.error(f"íŒŒë¼ë¯¸í„° ì ìš© ì˜¤ë¥˜: {e}")
    
    async def _get_recent_market_data(self) -> List[Dict]:
        """ìµœê·¼ ì‹œì¥ ë°ì´í„° ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹œì¥ ë°ì´í„° API í˜¸ì¶œ
        return []
    
    async def _get_recent_performance_data(self) -> List[Dict]:
        """ìµœê·¼ ì„±ëŠ¥ ë°ì´í„° ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì„±ëŠ¥ ë°ì´í„° ì¡°íšŒ
        return []
```

## ğŸ”§ **ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ**

### ğŸ“¦ **ë°ì´í„° ê²€ì¦ê¸°**

```python
# online-learning/data-quality-management/data_validator.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import logging
from enum import Enum

logger = logging.getLogger(__name__)

class ValidationRule(Enum):
    """ê²€ì¦ ê·œì¹™"""
    RANGE_CHECK = "range_check"
    TYPE_CHECK = "type_check"
    NULL_CHECK = "null_check"
    UNIQUENESS_CHECK = "uniqueness_check"
    CONSISTENCY_CHECK = "consistency_check"

@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    rule: ValidationRule
    field: str
    is_valid: bool
    error_message: Optional[str] = None
    timestamp: datetime = None

@dataclass
class DataQualityReport:
    """ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ"""
    total_records: int
    valid_records: int
    invalid_records: int
    quality_score: float
    validation_results: List[ValidationResult]
    timestamp: datetime

class DataValidator:
    """ë°ì´í„° ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.validation_rules = self._load_validation_rules()
        self.quality_threshold = 0.95
    
    def _load_validation_rules(self) -> Dict[str, Dict]:
        """ê²€ì¦ ê·œì¹™ ë¡œë“œ"""
        return {
            'price': {
                'type': 'float',
                'range': (0.0, float('inf')),
                'null_allowed': False
            },
            'volume': {
                'type': 'float',
                'range': (0.0, float('inf')),
                'null_allowed': False
            },
            'timestamp': {
                'type': 'datetime',
                'null_allowed': False
            },
            'symbol': {
                'type': 'string',
                'null_allowed': False,
                'max_length': 20
            }
        }
    
    def validate_data(self, data: pd.DataFrame) -> DataQualityReport:
        """ë°ì´í„° ê²€ì¦"""
        try:
            validation_results = []
            total_records = len(data)
            valid_records = 0
            
            for column in data.columns:
                if column in self.validation_rules:
                    rule = self.validation_rules[column]
                    column_results = self._validate_column(data[column], column, rule)
                    validation_results.extend(column_results)
                    
                    # ìœ íš¨í•œ ë ˆì½”ë“œ ìˆ˜ ê³„ì‚°
                    valid_count = sum(1 for result in column_results if result.is_valid)
                    valid_records = min(valid_records, valid_count) if valid_records > 0 else valid_count
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = valid_records / total_records if total_records > 0 else 0.0
            
            return DataQualityReport(
                total_records=total_records,
                valid_records=valid_records,
                invalid_records=total_records - valid_records,
                quality_score=quality_score,
                validation_results=validation_results,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜: {e}")
            return DataQualityReport(
                total_records=0,
                valid_records=0,
                invalid_records=0,
                quality_score=0.0,
                validation_results=[],
                timestamp=datetime.now()
            )
    
    def _validate_column(self, column_data: pd.Series, column_name: str, 
                        rule: Dict) -> List[ValidationResult]:
        """ì»¬ëŸ¼ ê²€ì¦"""
        results = []
        
        # íƒ€ì… ê²€ì¦
        if 'type' in rule:
            type_result = self._validate_type(column_data, column_name, rule['type'])
            results.append(type_result)
        
        # ë²”ìœ„ ê²€ì¦
        if 'range' in rule:
            range_result = self._validate_range(column_data, column_name, rule['range'])
            results.append(range_result)
        
        # NULL ê²€ì¦
        if 'null_allowed' in rule:
            null_result = self._validate_null(column_data, column_name, rule['null_allowed'])
            results.append(null_result)
        
        return results
    
    def _validate_type(self, column_data: pd.Series, column_name: str, 
                      expected_type: str) -> ValidationResult:
        """íƒ€ì… ê²€ì¦"""
        try:
            if expected_type == 'float':
                is_valid = pd.to_numeric(column_data, errors='coerce').notna().all()
            elif expected_type == 'datetime':
                is_valid = pd.to_datetime(column_data, errors='coerce').notna().all()
            elif expected_type == 'string':
                is_valid = column_data.astype(str).notna().all()
            else:
                is_valid = True
            
            return ValidationResult(
                rule=ValidationRule.TYPE_CHECK,
                field=column_name,
                is_valid=is_valid,
                error_message=None if is_valid else f"íƒ€ì… ë¶ˆì¼ì¹˜: {expected_type}",
                timestamp=datetime.now()
            )
            
        except Exception as e:
            return ValidationResult(
                rule=ValidationRule.TYPE_CHECK,
                field=column_name,
                is_valid=False,
                error_message=f"íƒ€ì… ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                timestamp=datetime.now()
            )
```

### ğŸ“¦ **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ**

```python
# online-learning/real-time-monitoring/real_time_monitor.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
import asyncio
import threading
import time
from collections import deque

logger = logging.getLogger(__name__)

@dataclass
class SystemMetric:
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­"""
    metric_name: str
    value: float
    unit: str
    timestamp: datetime
    threshold: Optional[float] = None
    is_alert: bool = False

@dataclass
class StrategyPerformance:
    """ì „ëµ ì„±ê³¼"""
    strategy_name: str
    total_return: float
    sharpe_ratio: float
    max_drawdown: float
    win_rate: float
    total_trades: int
    timestamp: datetime

@dataclass
class RiskMetric:
    """ìœ„í—˜ ë©”íŠ¸ë¦­"""
    var_95: float
    var_99: float
    portfolio_volatility: float
    correlation: float
    beta: float
    timestamp: datetime

class RealTimeMonitor:
    """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, alert_thresholds: Dict[str, float] = None):
        self.alert_thresholds = alert_thresholds or {
            'cpu_usage': 80.0,
            'memory_usage': 85.0,
            'response_time': 1000.0,  # ms
            'error_rate': 5.0,  # %
            'drawdown': 10.0  # %
        }
        
        self.system_metrics = deque(maxlen=1000)
        self.strategy_performance = deque(maxlen=100)
        self.risk_metrics = deque(maxlen=100)
        self.alerts = deque(maxlen=100)
        
        self.monitoring_active = False
        self.monitoring_thread = None
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if not self.monitoring_active:
            self.monitoring_active = True
            self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
            self.monitoring_thread.daemon = True
            self.monitoring_thread.start()
            logger.info("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
        logger.info("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring_active:
            try:
                # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                system_metrics = self._collect_system_metrics()
                self.system_metrics.extend(system_metrics)
                
                # ì „ëµ ì„±ê³¼ ìˆ˜ì§‘
                strategy_performance = self._collect_strategy_performance()
                self.strategy_performance.extend(strategy_performance)
                
                # ìœ„í—˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                risk_metrics = self._collect_risk_metrics()
                self.risk_metrics.extend(risk_metrics)
                
                # ì•Œë¦¼ í™•ì¸
                alerts = self._check_alerts()
                self.alerts.extend(alerts)
                
                # 1ì´ˆ ëŒ€ê¸°
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(5)
    
    def _collect_system_metrics(self) -> List[SystemMetric]:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            metrics = []
            current_time = datetime.now()
            
            # CPU ì‚¬ìš©ë¥ 
            cpu_usage = self._get_cpu_usage()
            metrics.append(SystemMetric(
                metric_name='cpu_usage',
                value=cpu_usage,
                unit='%',
                timestamp=current_time,
                threshold=self.alert_thresholds['cpu_usage'],
                is_alert=cpu_usage > self.alert_thresholds['cpu_usage']
            ))
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory_usage = self._get_memory_usage()
            metrics.append(SystemMetric(
                metric_name='memory_usage',
                value=memory_usage,
                unit='%',
                timestamp=current_time,
                threshold=self.alert_thresholds['memory_usage'],
                is_alert=memory_usage > self.alert_thresholds['memory_usage']
            ))
            
            # ì‘ë‹µ ì‹œê°„
            response_time = self._get_response_time()
            metrics.append(SystemMetric(
                metric_name='response_time',
                value=response_time,
                unit='ms',
                timestamp=current_time,
                threshold=self.alert_thresholds['response_time'],
                is_alert=response_time > self.alert_thresholds['response_time']
            ))
            
            return metrics
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _get_cpu_usage(self) -> float:
        """CPU ì‚¬ìš©ë¥  ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” psutil ì‚¬ìš©
        import random
        return random.uniform(20.0, 90.0)
    
    def _get_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” psutil ì‚¬ìš©
        import random
        return random.uniform(30.0, 95.0)
    
    def _get_response_time(self) -> float:
        """ì‘ë‹µ ì‹œê°„ ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        import random
        return random.uniform(50.0, 1500.0)
    
    def _collect_strategy_performance(self) -> List[StrategyPerformance]:
        """ì „ëµ ì„±ê³¼ ìˆ˜ì§‘"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì „ëµ ì„±ê³¼ ë°ì´í„° ì¡°íšŒ
            return []
            
        except Exception as e:
            logger.error(f"ì „ëµ ì„±ê³¼ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _collect_risk_metrics(self) -> List[RiskMetric]:
        """ìœ„í—˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ìœ„í—˜ ë©”íŠ¸ë¦­ ê³„ì‚°
            return []
            
        except Exception as e:
            logger.error(f"ìœ„í—˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _check_alerts(self) -> List[Dict[str, Any]]:
        """ì•Œë¦¼ í™•ì¸"""
        alerts = []
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì•Œë¦¼ í™•ì¸
        for metric in list(self.system_metrics)[-10:]:  # ìµœê·¼ 10ê°œ ë©”íŠ¸ë¦­
            if metric.is_alert:
                alerts.append({
                    'type': 'system_alert',
                    'metric': metric.metric_name,
                    'value': metric.value,
                    'threshold': metric.threshold,
                    'timestamp': metric.timestamp,
                    'message': f"{metric.metric_name} ì„ê³„ê°’ ì´ˆê³¼: {metric.value}{metric.unit}"
                })
        
        return alerts
    
    def get_system_health(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ê±´ê°•ë„ ì¡°íšŒ"""
        try:
            if not self.system_metrics:
                return {'status': 'unknown', 'score': 0.0}
            
            recent_metrics = list(self.system_metrics)[-10:]  # ìµœê·¼ 10ê°œ
            
            # ì•Œë¦¼ ê°œìˆ˜ ê³„ì‚°
            alert_count = sum(1 for metric in recent_metrics if metric.is_alert)
            
            # ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚° (0-100)
            health_score = max(0, 100 - (alert_count * 10))
            
            # ìƒíƒœ ê²°ì •
            if health_score >= 80:
                status = 'healthy'
            elif health_score >= 60:
                status = 'warning'
            else:
                status = 'critical'
            
            return {
                'status': status,
                'score': health_score,
                'alert_count': alert_count,
                'last_update': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ê±´ê°•ë„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {'status': 'error', 'score': 0.0}
```

## ğŸ“Š **ì„±ê³¼ ì§€í‘œ**

### **ëª©í‘œ ì„±ê³¼**
- **ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œê°„**: < 5ë¶„
- **ê°œë… ë“œë¦¬í”„íŠ¸ ê°ì§€**: < 1ì‹œê°„
- **ì ì‘ ì†ë„**: < 10ë¶„
- **ë°ì´í„° í’ˆì§ˆ ì •í™•ë„**: > 99.9% ë°ì´í„° ì •í™•ë„
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: < 100ms ëª¨ë‹ˆí„°ë§ ì§€ì—°
- **í•™ìŠµ ì •í™•ë„**: > 70%
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: < 2GB

### **ì„±ëŠ¥ ì§€í‘œ**
- **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì†ë„**: < 100ms per batch
- **ë“œë¦¬í”„íŠ¸ ê°ì§€ ì •í™•ë„**: > 85%
- **íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œê°„**: < 5ë¶„
- **ë°ì´í„° ê²€ì¦ ì‹œê°„**: < 50ms
- **ëª¨ë‹ˆí„°ë§ ì§€ì—°**: < 100ms
- **ì‹œìŠ¤í…œ ê°€ë™ë¥ **: > 99.5%
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: < 2GB

## ğŸ”— **ê´€ë ¨ ë¬¸ì„œ**

- [Phase 3.5.1: ê¸°ìˆ ì  ì§€í‘œ ë¶„ì„](3.5.1_TECHNICAL_ANALYSIS.md)
- [Phase 3.5.2: ê±°ë˜ ì „ëµ ë¼ì´ë¸ŒëŸ¬ë¦¬](3.5.2_TRADING_STRATEGIES.md)
- [Phase 3.5.3: ë‰´ìŠ¤ ì´ë²¤íŠ¸ ë¶„ì„](3.5.3_NEWS_EVENT_ANALYSIS.md)
- [Phase 3.5.5: ì„¤ëª… ê°€ëŠ¥í•œ AI](3.5.5_EXPLAINABLE_AI.md)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-01-26  
**í”„ë¡œì íŠ¸ ìƒíƒœ**: ì„¤ê³„ ì™„ë£Œ, ê°œë°œ ì¤€ë¹„  
**ë‹¤ìŒ ë‹¨ê³„**: ì„¤ëª… ê°€ëŠ¥í•œ AI ì‹œìŠ¤í…œ êµ¬í˜„ 