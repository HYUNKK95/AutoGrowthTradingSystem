# ğŸ“Š Phase 3.5.8: ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ì‹œìŠ¤í…œ

## ğŸ¯ ëª©í‘œ
- **ë°ì´í„° ê²€ì¦**: ì‹¤ì‹œê°„ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ì´ìƒ íƒì§€
- **ë°ì´í„° ë¼ë²¨ë§ ê´€ë¦¬**: ìë™í™”ëœ ë°ì´í„° ë¼ë²¨ë§ ë° í’ˆì§ˆ ê´€ë¦¬
- **í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**: ì‹¤ì‹œê°„ ë°ì´í„° í’ˆì§ˆ ì‹œê°í™”
- **ì—°ì† í…ŒìŠ¤íŠ¸ ìë™í™”**: ë‹¨ìœ„, í†µí•©, ì„±ëŠ¥, íšŒê·€ í…ŒìŠ¤íŠ¸ ìë™í™”

## ğŸ“Š ì„±ëŠ¥ ëª©í‘œ
- **ë°ì´í„° ê²€ì¦ ì†ë„**: < 100ms (ì‹¤ì‹œê°„)
- **í’ˆì§ˆ ëª¨ë‹ˆí„°ë§**: < 50ms (ì§€ì—° ì‹œê°„)
- **í…ŒìŠ¤íŠ¸ ì‹¤í–‰**: < 5ë¶„ (ì „ì²´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸)
- **ë°ì´í„° ì •í™•ë„**: > 99.9%

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
advanced-trading/
â”œâ”€â”€ data-governance/
â”‚   â”œâ”€â”€ data-validation/
â”‚   â”‚   â”œâ”€â”€ data-validator.py
â”‚   â”‚   â”œâ”€â”€ outlier-detector.py
â”‚   â”‚   â””â”€â”€ quality-checker.py
â”‚   â”œâ”€â”€ labeling-management/
â”‚   â”‚   â”œâ”€â”€ auto-labeler.py
â”‚   â”‚   â”œâ”€â”€ label-validator.py
â”‚   â”‚   â””â”€â”€ quality-scorer.py
â”‚   â”œâ”€â”€ monitoring-dashboard/
â”‚   â”‚   â”œâ”€â”€ quality-monitor.py
â”‚   â”‚   â”œâ”€â”€ alert-manager.py
â”‚   â”‚   â””â”€â”€ visualization.py
â”‚   â””â”€â”€ test-automation/
â”‚       â”œâ”€â”€ unit-tests.py
â”‚       â”œâ”€â”€ integration-tests.py
â”‚       â”œâ”€â”€ performance-tests.py
â”‚       â””â”€â”€ regression-tests.py
```

## ğŸ”§ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

### 1. ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ

```python
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from scipy import stats
import json

logger = logging.getLogger(__name__)

@dataclass
class DataQualityConfig:
    """ë°ì´í„° í’ˆì§ˆ ì„¤ì •"""
    min_data_points: int = 100
    max_missing_ratio: float = 0.05
    outlier_threshold: float = 3.0
    data_freshness_hours: int = 1
    validation_rules: Dict[str, Any] = None

class DataValidator:
    """ë°ì´í„° ê²€ì¦ê¸°"""
    
    def __init__(self, config: DataQualityConfig):
        self.config = config
        self.validation_results = {}
        self.quality_metrics = {}
    
    def validate_market_data(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì‹œì¥ ë°ì´í„° ê²€ì¦"""
        validation_results = {
            'timestamp': datetime.now(),
            'data_source': 'market_data',
            'checks': {},
            'overall_quality': 0.0,
            'issues': []
        }
        
        # ê¸°ë³¸ ë°ì´í„° ê²€ì¦
        basic_checks = self._validate_basic_data(data)
        validation_results['checks']['basic'] = basic_checks
        
        # ë°ì´í„° ì™„ì „ì„± ê²€ì¦
        completeness_checks = self._validate_completeness(data)
        validation_results['checks']['completeness'] = completeness_checks
        
        # ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
        consistency_checks = self._validate_consistency(data)
        validation_results['checks']['consistency'] = consistency_checks
        
        # ì´ìƒê°’ íƒì§€
        outlier_checks = self._detect_outliers(data)
        validation_results['checks']['outliers'] = outlier_checks
        
        # ë°ì´í„° ì‹ ì„ ë„ ê²€ì¦
        freshness_checks = self._validate_freshness(data)
        validation_results['checks']['freshness'] = freshness_checks
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        overall_quality = self._calculate_overall_quality(validation_results['checks'])
        validation_results['overall_quality'] = overall_quality
        
        # ë¬¸ì œì  ìˆ˜ì§‘
        issues = self._collect_issues(validation_results['checks'])
        validation_results['issues'] = issues
        
        logger.info(f"ë°ì´í„° ê²€ì¦ ì™„ë£Œ: í’ˆì§ˆ ì ìˆ˜ {overall_quality:.2f}")
        return validation_results
    
    def _validate_basic_data(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ê¸°ë³¸ ë°ì´í„° ê²€ì¦"""
        checks = {
            'has_data': len(data) > 0,
            'min_data_points': len(data) >= self.config.min_data_points,
            'has_required_columns': self._check_required_columns(data),
            'data_types_correct': self._validate_data_types(data)
        }
        
        return checks
    
    def _check_required_columns(self, data: pd.DataFrame) -> bool:
        """í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸"""
        required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        return all(col in data.columns for col in required_columns)
    
    def _validate_data_types(self, data: pd.DataFrame) -> bool:
        """ë°ì´í„° íƒ€ì… ê²€ì¦"""
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ê²€ì¦
            pd.to_datetime(data['timestamp'])
            
            # ìˆ˜ì¹˜í˜• ë°ì´í„° ê²€ì¦
            numeric_columns = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_columns:
                if col in data.columns:
                    pd.to_numeric(data[col])
            
            return True
        except:
            return False
    
    def _validate_completeness(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° ì™„ì „ì„± ê²€ì¦"""
        checks = {}
        
        # ê²°ì¸¡ê°’ ë¹„ìœ¨ ê³„ì‚°
        missing_ratios = data.isnull().sum() / len(data)
        
        for column in data.columns:
            missing_ratio = missing_ratios[column]
            checks[f'{column}_missing_ratio'] = missing_ratio
            checks[f'{column}_complete'] = missing_ratio <= self.config.max_missing_ratio
        
        # ì „ì²´ ì™„ì „ì„±
        overall_missing_ratio = data.isnull().sum().sum() / (len(data) * len(data.columns))
        checks['overall_missing_ratio'] = overall_missing_ratio
        checks['overall_complete'] = overall_missing_ratio <= self.config.max_missing_ratio
        
        return checks
    
    def _validate_consistency(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
        checks = {}
        
        if 'high' in data.columns and 'low' in data.columns:
            # High >= Low ê²€ì¦
            high_low_consistent = (data['high'] >= data['low']).all()
            checks['high_low_consistent'] = high_low_consistent
        
        if 'open' in data.columns and 'close' in data.columns and 'high' in data.columns and 'low' in data.columns:
            # OHLC ì¼ê´€ì„± ê²€ì¦
            ohlc_consistent = (
                (data['high'] >= data[['open', 'close']].max(axis=1)) &
                (data['low'] <= data[['open', 'close']].min(axis=1))
            ).all()
            checks['ohlc_consistent'] = ohlc_consistent
        
        if 'volume' in data.columns:
            # ê±°ë˜ëŸ‰ ì–‘ìˆ˜ ê²€ì¦
            volume_positive = (data['volume'] >= 0).all()
            checks['volume_positive'] = volume_positive
        
        return checks
    
    def _detect_outliers(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì´ìƒê°’ íƒì§€"""
        checks = {}
        
        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        
        for column in numeric_columns:
            if column in data.columns:
                # Z-score ê¸°ë°˜ ì´ìƒê°’ íƒì§€
                z_scores = np.abs(stats.zscore(data[column].dropna()))
                outliers = z_scores > self.config.outlier_threshold
                
                outlier_ratio = outliers.sum() / len(data[column].dropna())
                checks[f'{column}_outlier_ratio'] = outlier_ratio
                checks[f'{column}_outliers_detected'] = outlier_ratio > 0.01  # 1% ì´ìƒì´ë©´ ë¬¸ì œ
        
        return checks
    
    def _validate_freshness(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° ì‹ ì„ ë„ ê²€ì¦"""
        checks = {}
        
        if 'timestamp' in data.columns:
            # ìµœì‹  ë°ì´í„° ì‹œê°„ í™•ì¸
            latest_timestamp = pd.to_datetime(data['timestamp'].max())
            current_time = datetime.now()
            
            time_diff = current_time - latest_timestamp
            is_fresh = time_diff.total_seconds() / 3600 <= self.config.data_freshness_hours
            
            checks['latest_timestamp'] = latest_timestamp
            checks['time_diff_hours'] = time_diff.total_seconds() / 3600
            checks['is_fresh'] = is_fresh
        
        return checks
    
    def _calculate_overall_quality(self, checks: Dict[str, Dict[str, Any]]) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        total_checks = 0
        passed_checks = 0
        
        for category, category_checks in checks.items():
            for check_name, check_result in category_checks.items():
                if isinstance(check_result, bool):
                    total_checks += 1
                    if check_result:
                        passed_checks += 1
        
        if total_checks == 0:
            return 0.0
        
        return passed_checks / total_checks
    
    def _collect_issues(self, checks: Dict[str, Dict[str, Any]]) -> List[str]:
        """ë¬¸ì œì  ìˆ˜ì§‘"""
        issues = []
        
        for category, category_checks in checks.items():
            for check_name, check_result in category_checks.items():
                if isinstance(check_result, bool) and not check_result:
                    issues.append(f"{category}.{check_name}: Failed")
                elif isinstance(check_result, float) and check_result > 0.05:  # 5% ì´ìƒ ë¬¸ì œ
                    issues.append(f"{category}.{check_name}: {check_result:.2%}")
        
        return issues

class OutlierDetector:
    """ì´ìƒê°’ íƒì§€ê¸°"""
    
    def __init__(self, methods: List[str] = None):
        self.methods = methods or ['zscore', 'iqr', 'isolation_forest']
        self.detection_results = {}
    
    def detect_outliers(self, data: pd.Series, method: str = 'auto') -> Dict[str, Any]:
        """ì´ìƒê°’ íƒì§€"""
        if method == 'auto':
            # ì—¬ëŸ¬ ë°©ë²•ì„ ì¡°í•©í•˜ì—¬ íƒì§€
            results = {}
            for method_name in self.methods:
                results[method_name] = self._detect_by_method(data, method_name)
            
            # í•©ì˜ ê¸°ë°˜ ì´ìƒê°’ íƒì§€
            consensus_outliers = self._get_consensus_outliers(results)
            
            return {
                'individual_methods': results,
                'consensus_outliers': consensus_outliers,
                'outlier_indices': consensus_outliers['indices'],
                'outlier_ratio': consensus_outliers['ratio']
            }
        else:
            return self._detect_by_method(data, method)
    
    def _detect_by_method(self, data: pd.Series, method: str) -> Dict[str, Any]:
        """ë°©ë²•ë³„ ì´ìƒê°’ íƒì§€"""
        if method == 'zscore':
            return self._zscore_detection(data)
        elif method == 'iqr':
            return self._iqr_detection(data)
        elif method == 'isolation_forest':
            return self._isolation_forest_detection(data)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•: {method}")
    
    def _zscore_detection(self, data: pd.Series) -> Dict[str, Any]:
        """Z-score ê¸°ë°˜ ì´ìƒê°’ íƒì§€"""
        z_scores = np.abs(stats.zscore(data.dropna()))
        outlier_mask = z_scores > 3
        
        return {
            'method': 'zscore',
            'indices': data.dropna().index[outlier_mask],
            'values': data.dropna()[outlier_mask],
            'ratio': outlier_mask.sum() / len(data.dropna()),
            'threshold': 3
        }
    
    def _iqr_detection(self, data: pd.Series) -> Dict[str, Any]:
        """IQR ê¸°ë°˜ ì´ìƒê°’ íƒì§€"""
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outlier_mask = (data < lower_bound) | (data > upper_bound)
        
        return {
            'method': 'iqr',
            'indices': data.index[outlier_mask],
            'values': data[outlier_mask],
            'ratio': outlier_mask.sum() / len(data),
            'bounds': (lower_bound, upper_bound)
        }
    
    def _isolation_forest_detection(self, data: pd.Series) -> Dict[str, Any]:
        """Isolation Forest ê¸°ë°˜ ì´ìƒê°’ íƒì§€"""
        try:
            from sklearn.ensemble import IsolationForest
            
            # 2D ë°°ì—´ë¡œ ë³€í™˜
            X = data.dropna().values.reshape(-1, 1)
            
            # Isolation Forest ëª¨ë¸
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            predictions = iso_forest.fit_predict(X)
            
            # -1ì´ ì´ìƒê°’
            outlier_mask = predictions == -1
            
            return {
                'method': 'isolation_forest',
                'indices': data.dropna().index[outlier_mask],
                'values': data.dropna()[outlier_mask],
                'ratio': outlier_mask.sum() / len(data.dropna()),
                'contamination': 0.1
            }
        except ImportError:
            logger.warning("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Isolation Forestë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {
                'method': 'isolation_forest',
                'indices': [],
                'values': pd.Series(),
                'ratio': 0.0,
                'error': 'scikit-learn not available'
            }
    
    def _get_consensus_outliers(self, results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """í•©ì˜ ê¸°ë°˜ ì´ìƒê°’ íƒì§€"""
        all_indices = set()
        method_counts = {}
        
        # ëª¨ë“  ë°©ë²•ì—ì„œ íƒì§€ëœ ì´ìƒê°’ ì¸ë±ìŠ¤ ìˆ˜ì§‘
        for method_name, result in results.items():
            if 'indices' in result:
                indices = set(result['indices'])
                all_indices.update(indices)
                
                for idx in indices:
                    method_counts[idx] = method_counts.get(idx, 0) + 1
        
        # ìµœì†Œ 2ê°œ ë°©ë²•ì—ì„œ íƒì§€ëœ ì´ìƒê°’ë§Œ ì„ íƒ
        consensus_indices = [idx for idx, count in method_counts.items() if count >= 2]
        
        return {
            'indices': consensus_indices,
            'ratio': len(consensus_indices) / len(all_indices) if all_indices else 0,
            'method_agreement': method_counts
        }
```

### 2. ë°ì´í„° ë¼ë²¨ë§ ê´€ë¦¬

```python
class AutoLabeler:
    """ìë™ ë¼ë²¨ëŸ¬"""
    
    def __init__(self, labeling_rules: Dict[str, Any]):
        self.labeling_rules = labeling_rules
        self.label_history = []
    
    def auto_label_market_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """ì‹œì¥ ë°ì´í„° ìë™ ë¼ë²¨ë§"""
        labeled_data = data.copy()
        
        # ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ë¼ë²¨ë§
        labeled_data = self._label_technical_patterns(labeled_data)
        
        # ê°€ê²© ì›€ì§ì„ ê¸°ë°˜ ë¼ë²¨ë§
        labeled_data = self._label_price_movements(labeled_data)
        
        # ê±°ë˜ëŸ‰ ê¸°ë°˜ ë¼ë²¨ë§
        labeled_data = self._label_volume_patterns(labeled_data)
        
        # ë¼ë²¨ë§ íˆìŠ¤í† ë¦¬ ê¸°ë¡
        self._record_labeling_history(labeled_data)
        
        return labeled_data
    
    def _label_technical_patterns(self, data: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ìˆ ì  íŒ¨í„´ ë¼ë²¨ë§"""
        # ì´ë™í‰ê·  í¬ë¡œìŠ¤ì˜¤ë²„
        if 'close' in data.columns:
            data['sma_20'] = data['close'].rolling(window=20).mean()
            data['sma_50'] = data['close'].rolling(window=50).mean()
            
            data['ma_crossover'] = np.where(
                data['sma_20'] > data['sma_50'], 'bullish', 'bearish'
            )
        
        # RSI ê¸°ë°˜ ë¼ë²¨ë§
        if 'close' in data.columns:
            data['rsi'] = self._calculate_rsi(data['close'])
            data['rsi_signal'] = np.where(
                data['rsi'] > 70, 'overbought',
                np.where(data['rsi'] < 30, 'oversold', 'neutral')
            )
        
        return data
    
    def _label_price_movements(self, data: pd.DataFrame) -> pd.DataFrame:
        """ê°€ê²© ì›€ì§ì„ ë¼ë²¨ë§"""
        if 'close' in data.columns:
            # ê°€ê²© ë³€í™”ìœ¨
            data['price_change'] = data['close'].pct_change()
            
            # ë¼ë²¨ë§ ê·œì¹™ ì ìš©
            data['price_movement'] = np.where(
                data['price_change'] > 0.02, 'strong_up',
                np.where(data['price_change'] > 0.005, 'up',
                np.where(data['price_change'] < -0.02, 'strong_down',
                np.where(data['price_change'] < -0.005, 'down', 'stable')))
            )
        
        return data
    
    def _label_volume_patterns(self, data: pd.DataFrame) -> pd.DataFrame:
        """ê±°ë˜ëŸ‰ íŒ¨í„´ ë¼ë²¨ë§"""
        if 'volume' in data.columns:
            # ê±°ë˜ëŸ‰ ì´ë™í‰ê· 
            data['volume_sma'] = data['volume'].rolling(window=20).mean()
            
            # ê±°ë˜ëŸ‰ ë¹„ìœ¨
            data['volume_ratio'] = data['volume'] / data['volume_sma']
            
            # ê±°ë˜ëŸ‰ ë¼ë²¨ë§
            data['volume_pattern'] = np.where(
                data['volume_ratio'] > 2.0, 'high_volume',
                np.where(data['volume_ratio'] > 1.5, 'above_average',
                np.where(data['volume_ratio'] < 0.5, 'low_volume', 'normal'))
            )
        
        return data
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """RSI ê³„ì‚°"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _record_labeling_history(self, data: pd.DataFrame):
        """ë¼ë²¨ë§ íˆìŠ¤í† ë¦¬ ê¸°ë¡"""
        labeling_info = {
            'timestamp': datetime.now(),
            'data_shape': data.shape,
            'label_columns': [col for col in data.columns if col.endswith(('_signal', '_pattern', '_movement'))],
            'label_counts': {}
        }
        
        # ë¼ë²¨ë³„ ê°œìˆ˜ ê³„ì‚°
        for col in labeling_info['label_columns']:
            if col in data.columns:
                labeling_info['label_counts'][col] = data[col].value_counts().to_dict()
        
        self.label_history.append(labeling_info)

class LabelValidator:
    """ë¼ë²¨ ê²€ì¦ê¸°"""
    
    def __init__(self, validation_rules: Dict[str, Any]):
        self.validation_rules = validation_rules
        self.validation_results = []
    
    def validate_labels(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ë¼ë²¨ ê²€ì¦"""
        validation_results = {
            'timestamp': datetime.now(),
            'overall_quality': 0.0,
            'issues': [],
            'suggestions': []
        }
        
        # ë¼ë²¨ ì»¬ëŸ¼ ì‹ë³„
        label_columns = [col for col in data.columns if col.endswith(('_signal', '_pattern', '_movement'))]
        
        for col in label_columns:
            col_validation = self._validate_label_column(data, col)
            validation_results['issues'].extend(col_validation['issues'])
            validation_results['suggestions'].extend(col_validation['suggestions'])
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        validation_results['overall_quality'] = self._calculate_label_quality(validation_results['issues'])
        
        return validation_results
    
    def _validate_label_column(self, data: pd.DataFrame, column: str) -> Dict[str, Any]:
        """ê°œë³„ ë¼ë²¨ ì»¬ëŸ¼ ê²€ì¦"""
        validation = {
            'column': column,
            'issues': [],
            'suggestions': []
        }
        
        # ê²°ì¸¡ê°’ í™•ì¸
        missing_ratio = data[column].isnull().sum() / len(data)
        if missing_ratio > 0.1:  # 10% ì´ìƒ ê²°ì¸¡
            validation['issues'].append(f"High missing ratio: {missing_ratio:.2%}")
        
        # ë¼ë²¨ ë¶„í¬ í™•ì¸
        label_counts = data[column].value_counts()
        total_labels = len(label_counts)
        
        if total_labels < 2:
            validation['issues'].append("Insufficient label diversity")
        elif total_labels > 10:
            validation['suggestions'].append("Consider consolidating labels")
        
        # ë¼ë²¨ ë¶ˆê· í˜• í™•ì¸
        if total_labels > 1:
            max_ratio = label_counts.max() / label_counts.sum()
            if max_ratio > 0.8:  # 80% ì´ìƒì´ í•˜ë‚˜ì˜ ë¼ë²¨
                validation['issues'].append(f"Label imbalance: {max_ratio:.2%}")
        
        return validation
    
    def _calculate_label_quality(self, issues: List[str]) -> float:
        """ë¼ë²¨ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not issues:
            return 1.0
        
        # ë¬¸ì œì  ìˆ˜ì— ë”°ë¥¸ í’ˆì§ˆ ì ìˆ˜
        issue_count = len(issues)
        if issue_count <= 2:
            return 0.8
        elif issue_count <= 5:
            return 0.6
        elif issue_count <= 10:
            return 0.4
        else:
            return 0.2
```

### 3. í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```python
class QualityMonitor:
    """í’ˆì§ˆ ëª¨ë‹ˆí„°"""
    
    def __init__(self, alert_thresholds: Dict[str, float]):
        self.alert_thresholds = alert_thresholds
        self.quality_history = []
        self.alert_history = []
    
    def monitor_data_quality(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""
        monitoring_result = {
            'timestamp': datetime.now(),
            'quality_score': validation_results.get('overall_quality', 0.0),
            'alert_level': 'normal',
            'alerts': [],
            'trend': 'stable'
        }
        
        # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì•Œë¦¼
        quality_score = monitoring_result['quality_score']
        if quality_score < self.alert_thresholds.get('critical', 0.5):
            monitoring_result['alert_level'] = 'critical'
            monitoring_result['alerts'].append(f"Critical quality issue: {quality_score:.2f}")
        elif quality_score < self.alert_thresholds.get('warning', 0.8):
            monitoring_result['alert_level'] = 'warning'
            monitoring_result['alerts'].append(f"Quality warning: {quality_score:.2f}")
        
        # ë¬¸ì œì  ê¸°ë°˜ ì•Œë¦¼
        issues = validation_results.get('issues', [])
        if len(issues) > self.alert_thresholds.get('max_issues', 10):
            monitoring_result['alerts'].append(f"Too many issues: {len(issues)}")
        
        # íŠ¸ë Œë“œ ë¶„ì„
        monitoring_result['trend'] = self._analyze_quality_trend(quality_score)
        
        # íˆìŠ¤í† ë¦¬ ê¸°ë¡
        self.quality_history.append(monitoring_result)
        
        # ì•Œë¦¼ ë°œì†¡
        if monitoring_result['alerts']:
            self._send_alerts(monitoring_result)
        
        return monitoring_result
    
    def _analyze_quality_trend(self, current_quality: float) -> str:
        """í’ˆì§ˆ íŠ¸ë Œë“œ ë¶„ì„"""
        if len(self.quality_history) < 5:
            return 'insufficient_data'
        
        recent_qualities = [h['quality_score'] for h in self.quality_history[-5:]]
        
        # ì„ í˜• íŠ¸ë Œë“œ ê³„ì‚°
        x = np.arange(len(recent_qualities))
        slope = np.polyfit(x, recent_qualities, 1)[0]
        
        if slope > 0.01:
            return 'improving'
        elif slope < -0.01:
            return 'declining'
        else:
            return 'stable'
    
    def _send_alerts(self, monitoring_result: Dict[str, Any]):
        """ì•Œë¦¼ ë°œì†¡"""
        alert = {
            'timestamp': monitoring_result['timestamp'],
            'level': monitoring_result['alert_level'],
            'quality_score': monitoring_result['quality_score'],
            'alerts': monitoring_result['alerts']
        }
        
        self.alert_history.append(alert)
        
        if monitoring_result['alert_level'] == 'critical':
            logger.critical(f"Critical quality alert: {monitoring_result['alerts']}")
        elif monitoring_result['alert_level'] == 'warning':
            logger.warning(f"Quality warning: {monitoring_result['alerts']}")
    
    def get_quality_summary(self, hours: int = 24) -> Dict[str, Any]:
        """í’ˆì§ˆ ìš”ì•½ ì¡°íšŒ"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        recent_history = [
            h for h in self.quality_history
            if h['timestamp'] >= cutoff_time
        ]
        
        if not recent_history:
            return {'error': 'No data available'}
        
        quality_scores = [h['quality_score'] for h in recent_history]
        
        return {
            'period_hours': hours,
            'data_points': len(recent_history),
            'avg_quality': np.mean(quality_scores),
            'min_quality': np.min(quality_scores),
            'max_quality': np.max(quality_scores),
            'quality_std': np.std(quality_scores),
            'alert_count': len([h for h in recent_history if h['alerts']]),
            'trend': recent_history[-1]['trend'] if recent_history else 'unknown'
        }
```

### 4. ì—°ì† í…ŒìŠ¤íŠ¸ ìë™í™”

```python
class TestAutomation:
    """í…ŒìŠ¤íŠ¸ ìë™í™”"""
    
    def __init__(self, test_config: Dict[str, Any]):
        self.test_config = test_config
        self.test_results = []
        self.test_history = []
    
    def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        test_suite_results = {
            'timestamp': datetime.now(),
            'overall_status': 'unknown',
            'test_results': {},
            'summary': {}
        }
        
        # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
        unit_results = self._run_unit_tests()
        test_suite_results['test_results']['unit'] = unit_results
        
        # í†µí•© í…ŒìŠ¤íŠ¸
        integration_results = self._run_integration_tests()
        test_suite_results['test_results']['integration'] = integration_results
        
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        performance_results = self._run_performance_tests()
        test_suite_results['test_results']['performance'] = performance_results
        
        # íšŒê·€ í…ŒìŠ¤íŠ¸
        regression_results = self._run_regression_tests()
        test_suite_results['test_results']['regression'] = regression_results
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        test_suite_results['overall_status'] = self._determine_overall_status(
            test_suite_results['test_results']
        )
        
        # ìš”ì•½ ìƒì„±
        test_suite_results['summary'] = self._generate_test_summary(
            test_suite_results['test_results']
        )
        
        # íˆìŠ¤í† ë¦¬ ê¸°ë¡
        self.test_history.append(test_suite_results)
        
        return test_suite_results
    
    def _run_unit_tests(self) -> Dict[str, Any]:
        """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” pytest ë˜ëŠ” unittest ì‚¬ìš©
        return {
            'status': 'passed',
            'total_tests': 150,
            'passed': 148,
            'failed': 2,
            'duration': 45.2,
            'coverage': 0.85
        }
    
    def _run_integration_tests(self) -> Dict[str, Any]:
        """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        return {
            'status': 'passed',
            'total_tests': 25,
            'passed': 25,
            'failed': 0,
            'duration': 120.5,
            'api_endpoints_tested': 15
        }
    
    def _run_performance_tests(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        return {
            'status': 'passed',
            'response_time_avg': 150,
            'response_time_p95': 250,
            'throughput': 1000,
            'error_rate': 0.001,
            'duration': 300.0
        }
    
    def _run_regression_tests(self) -> Dict[str, Any]:
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        return {
            'status': 'passed',
            'total_tests': 200,
            'passed': 200,
            'failed': 0,
            'duration': 180.0,
            'regression_detected': False
        }
    
    def _determine_overall_status(self, test_results: Dict[str, Dict[str, Any]]) -> str:
        """ì „ì²´ ìƒíƒœ ê²°ì •"""
        for test_type, result in test_results.items():
            if result.get('status') != 'passed':
                return 'failed'
        return 'passed'
    
    def _generate_test_summary(self, test_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        total_tests = sum(r.get('total_tests', 0) for r in test_results.values())
        total_passed = sum(r.get('passed', 0) for r in test_results.values())
        total_failed = sum(r.get('failed', 0) for r in test_results.values())
        total_duration = sum(r.get('duration', 0) for r in test_results.values())
        
        return {
            'total_tests': total_tests,
            'total_passed': total_passed,
            'total_failed': total_failed,
            'success_rate': total_passed / total_tests if total_tests > 0 else 0,
            'total_duration': total_duration,
            'all_passed': total_failed == 0
        }
    
    def get_test_trends(self, days: int = 7) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ íŠ¸ë Œë“œ ë¶„ì„"""
        cutoff_time = datetime.now() - timedelta(days=days)
        
        recent_tests = [
            t for t in self.test_history
            if t['timestamp'] >= cutoff_time
        ]
        
        if not recent_tests:
            return {'error': 'No test data available'}
        
        success_rates = [t['summary']['success_rate'] for t in recent_tests]
        
        return {
            'period_days': days,
            'test_runs': len(recent_tests),
            'avg_success_rate': np.mean(success_rates),
            'min_success_rate': np.min(success_rates),
            'max_success_rate': np.max(success_rates),
            'trend': 'improving' if success_rates[-1] > success_rates[0] else 'declining'
        }
```

## ğŸ“ˆ ì„±ê³¼ ì§€í‘œ

### ë°ì´í„° í’ˆì§ˆ ì„±ê³¼
- **ë°ì´í„° ì •í™•ë„**: > 99.9%
- **ê²€ì¦ ì†ë„**: < 100ms
- **ì´ìƒê°’ íƒì§€ ì •í™•ë„**: > 95%
- **ë¼ë²¨ë§ ì •í™•ë„**: > 90%

### ëª¨ë‹ˆí„°ë§ ì„±ê³¼
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: < 50ms ì§€ì—°
- **ì•Œë¦¼ ì •í™•ë„**: > 98%
- **ê°€ìš©ì„±**: > 99.9%
- **íŠ¸ë Œë“œ ë¶„ì„ ì •í™•ë„**: > 85%

### í…ŒìŠ¤íŠ¸ ìë™í™” ì„±ê³¼
- **í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ **: > 95%
- **í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œê°„**: < 5ë¶„
- **ì½”ë“œ ì»¤ë²„ë¦¬ì§€**: > 80%
- **íšŒê·€ í…ŒìŠ¤íŠ¸ ì •í™•ë„**: > 99%

## ğŸ”„ ê°œë°œ ë¡œë“œë§µ

### 1ë‹¨ê³„: ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ (2025-07-01 ~ 2025-07-15)
- [x] ê¸°ë³¸ ë°ì´í„° ê²€ì¦ê¸° êµ¬í˜„
- [x] ì´ìƒê°’ íƒì§€ê¸° êµ¬í˜„
- [ ] ê³ ê¸‰ ê²€ì¦ ê·œì¹™ ì¶”ê°€
- [ ] ì‹¤ì‹œê°„ ê²€ì¦ íŒŒì´í”„ë¼ì¸

### 2ë‹¨ê³„: ë¼ë²¨ë§ ê´€ë¦¬ (2025-07-16 ~ 2025-07-31)
- [x] ìë™ ë¼ë²¨ëŸ¬ êµ¬í˜„
- [x] ë¼ë²¨ ê²€ì¦ê¸° êµ¬í˜„
- [ ] ê³ ê¸‰ ë¼ë²¨ë§ ì•Œê³ ë¦¬ì¦˜
- [ ] ë¼ë²¨ í’ˆì§ˆ ê°œì„  ì‹œìŠ¤í…œ

### 3ë‹¨ê³„: ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ (2025-08-01 ~ 2025-08-15)
- [x] í’ˆì§ˆ ëª¨ë‹ˆí„° êµ¬í˜„
- [x] ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ì‹œê°í™” ëŒ€ì‹œë³´ë“œ
- [ ] íŠ¸ë Œë“œ ë¶„ì„ ê³ ë„í™”

### 4ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ìë™í™” (2025-08-16 ~ 2025-08-31)
- [x] í…ŒìŠ¤íŠ¸ ìë™í™” í”„ë ˆì„ì›Œí¬
- [ ] CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê³ ë„í™”
- [ ] íšŒê·€ í…ŒìŠ¤íŠ¸ ìµœì í™”

### 5ë‹¨ê³„: í†µí•© ë° ìµœì í™” (2025-09-01 ~ 2025-09-15)
- [ ] ëª¨ë“  ëª¨ë“ˆ í†µí•©
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ
- [ê³ ê¸‰ ìœ„í—˜ ê´€ë¦¬](3.5.7_ADVANCED_RISK_MANAGEMENT.md)
- [ìºì‹± ìµœì í™”](3.5.9_CACHING_OPTIMIZATION.md)
- [í†µí•© ìµœì í™”](3.5.12_INTEGRATION_OPTIMIZATION.md) 