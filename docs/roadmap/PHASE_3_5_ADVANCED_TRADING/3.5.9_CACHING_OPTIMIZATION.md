# âš¡ Phase 3.5.9: ìºì‹± ìµœì í™” ì‹œìŠ¤í…œ

## ğŸ¯ ëª©í‘œ
- **í† í°/ì§€í‘œ ìºì‹±**: ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì˜ ê³ ì„±ëŠ¥ ìºì‹±
- **ì§€ì—° ì‹œê°„ ìµœì í™”**: ìºì‹œ íˆíŠ¸ìœ¨ ìµœëŒ€í™” ë° ì‘ë‹µ ì‹œê°„ ìµœì†Œí™”
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ìºì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
- **ë¶„ì‚° ìºì‹±**: ë‹¤ì¤‘ ì„œë²„ í™˜ê²½ì—ì„œì˜ ìºì‹œ ë™ê¸°í™”

## ğŸ“Š ì„±ëŠ¥ ëª©í‘œ
- **ìºì‹œ íˆíŠ¸ìœ¨**: > 90%
- **ì‘ë‹µ ì‹œê°„**: < 10ms (ìºì‹œ íˆíŠ¸ ì‹œ)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: < 2GB (ì „ì²´ ìºì‹œ)
- **ìºì‹œ ë™ê¸°í™”**: < 100ms (ë¶„ì‚° í™˜ê²½)

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
advanced-trading/
â”œâ”€â”€ caching-optimization/
â”‚   â”œâ”€â”€ cache-manager/
â”‚   â”‚   â”œâ”€â”€ redis-cache.py
â”‚   â”‚   â”œâ”€â”€ memory-cache.py
â”‚   â”‚   â””â”€â”€ distributed-cache.py
â”‚   â”œâ”€â”€ token-cache/
â”‚   â”‚   â”œâ”€â”€ token-cache-manager.py
â”‚   â”‚   â”œâ”€â”€ indicator-cache.py
â”‚   â”‚   â””â”€â”€ price-cache.py
â”‚   â”œâ”€â”€ latency-optimizer/
â”‚   â”‚   â”œâ”€â”€ cache-predictor.py
â”‚   â”‚   â”œâ”€â”€ prefetch-manager.py
â”‚   â”‚   â””â”€â”€ eviction-strategy.py
â”‚   â”œâ”€â”€ performance-monitor/
â”‚   â”‚   â”œâ”€â”€ cache-metrics.py
â”‚   â”‚   â”œâ”€â”€ latency-monitor.py
â”‚   â”‚   â””â”€â”€ memory-optimizer.py
â”‚   â””â”€â”€ distributed-cache/
â”‚       â”œâ”€â”€ cache-sync.py
â”‚       â”œâ”€â”€ load-balancer.py
â”‚       â””â”€â”€ failover-manager.py
```

## ğŸ”§ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

### 1. ìºì‹œ ê´€ë¦¬ì

```python
import redis
import json
import pickle
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
import hashlib
from functools import wraps
import threading
import time

logger = logging.getLogger(__name__)

@dataclass
class CacheConfig:
    """ìºì‹œ ì„¤ì •"""
    redis_host: str = 'localhost'
    redis_port: int = 6379
    redis_db: int = 0
    default_ttl: int = 3600  # 1ì‹œê°„
    max_memory: int = 1024 * 1024 * 1024  # 1GB
    cache_strategy: str = 'lru'  # lru, lfu, fifo

class CacheManager:
    """ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.redis_client = redis.Redis(
            host=config.redis_host,
            port=config.redis_port,
            db=config.redis_db,
            decode_responses=False
        )
        self.memory_cache = {}
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0
        }
        self.lock = threading.Lock()
    
    def get(self, key: str, default: Any = None) -> Any:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        try:
            # ë©”ëª¨ë¦¬ ìºì‹œ ë¨¼ì € í™•ì¸
            if key in self.memory_cache:
                self.cache_stats['hits'] += 1
                logger.debug(f"Memory cache hit: {key}")
                return self.memory_cache[key]['value']
            
            # Redis ìºì‹œ í™•ì¸
            cached_value = self.redis_client.get(key)
            if cached_value is not None:
                self.cache_stats['hits'] += 1
                value = pickle.loads(cached_value)
                
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                self.memory_cache[key] = {
                    'value': value,
                    'timestamp': datetime.now()
                }
                
                logger.debug(f"Redis cache hit: {key}")
                return value
            else:
                self.cache_stats['misses'] += 1
                logger.debug(f"Cache miss: {key}")
                return default
                
        except Exception as e:
            logger.error(f"Cache get error for key {key}: {e}")
            return default
    
    def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """ìºì‹œì— ê°’ ì €ì¥"""
        try:
            ttl = ttl or self.config.default_ttl
            
            # Redisì— ì €ì¥
            serialized_value = pickle.dumps(value)
            success = self.redis_client.setex(key, ttl, serialized_value)
            
            if success:
                self.cache_stats['sets'] += 1
                
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                with self.lock:
                    self.memory_cache[key] = {
                        'value': value,
                        'timestamp': datetime.now()
                    }
                
                logger.debug(f"Cache set: {key}")
                return True
            else:
                logger.error(f"Failed to set cache: {key}")
                return False
                
        except Exception as e:
            logger.error(f"Cache set error for key {key}: {e}")
            return False
    
    def delete(self, key: str) -> bool:
        """ìºì‹œì—ì„œ ê°’ ì‚­ì œ"""
        try:
            # Redisì—ì„œ ì‚­ì œ
            result = self.redis_client.delete(key)
            
            # ë©”ëª¨ë¦¬ ìºì‹œì—ì„œë„ ì‚­ì œ
            with self.lock:
                self.memory_cache.pop(key, None)
            
            self.cache_stats['deletes'] += 1
            logger.debug(f"Cache delete: {key}")
            return result > 0
            
        except Exception as e:
            logger.error(f"Cache delete error for key {key}: {e}")
            return False
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = self.cache_stats['hits'] / total_requests if total_requests > 0 else 0
        
        return {
            'hits': self.cache_stats['hits'],
            'misses': self.cache_stats['misses'],
            'sets': self.cache_stats['sets'],
            'deletes': self.cache_stats['deletes'],
            'hit_rate': hit_rate,
            'memory_cache_size': len(self.memory_cache),
            'redis_info': self.redis_client.info()
        }
    
    def clear_cache(self) -> bool:
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        try:
            # Redis ìºì‹œ ì‚­ì œ
            self.redis_client.flushdb()
            
            # ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ
            with self.lock:
                self.memory_cache.clear()
            
            logger.info("Cache cleared successfully")
            return True
            
        except Exception as e:
            logger.error(f"Cache clear error: {e}")
            return False
    
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        with self.lock:
            current_size = len(self.memory_cache)
            max_size = 10000  # ìµœëŒ€ ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸°
            
            if current_size > max_size:
                # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                sorted_items = sorted(
                    self.memory_cache.items(),
                    key=lambda x: x[1]['timestamp']
                )
                
                items_to_remove = current_size - max_size
                for i in range(items_to_remove):
                    key, _ = sorted_items[i]
                    del self.memory_cache[key]
                
                logger.info(f"Memory cache optimized: removed {items_to_remove} items")

class DistributedCacheManager:
    """ë¶„ì‚° ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self, nodes: List[str], config: CacheConfig):
        self.nodes = nodes
        self.config = config
        self.connections = {}
        self.current_node = 0
        
        # ê° ë…¸ë“œì— ì—°ê²°
        for node in nodes:
            host, port = node.split(':')
            self.connections[node] = redis.Redis(
                host=host,
                port=int(port),
                db=config.redis_db,
                decode_responses=False
            )
    
    def get(self, key: str, default: Any = None) -> Any:
        """ë¶„ì‚° ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        # ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ë…¸ë“œ ì„ íƒ
        node = self.nodes[self.current_node]
        connection = self.connections[node]
        
        try:
            cached_value = connection.get(key)
            if cached_value is not None:
                return pickle.loads(cached_value)
            else:
                return default
        except Exception as e:
            logger.error(f"Distributed cache get error: {e}")
            return default
        finally:
            # ë‹¤ìŒ ë…¸ë“œë¡œ ì´ë™
            self.current_node = (self.current_node + 1) % len(self.nodes)
    
    def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """ë¶„ì‚° ìºì‹œì— ê°’ ì €ì¥"""
        ttl = ttl or self.config.default_ttl
        serialized_value = pickle.dumps(value)
        
        success_count = 0
        for node, connection in self.connections.items():
            try:
                success = connection.setex(key, ttl, serialized_value)
                if success:
                    success_count += 1
            except Exception as e:
                logger.error(f"Failed to set cache on node {node}: {e}")
        
        return success_count > 0
```

### 2. í† í° ìºì‹œ ê´€ë¦¬ì

```python
class TokenCacheManager:
    """í† í° ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self, cache_manager: CacheManager):
        self.cache_manager = cache_manager
        self.token_cache_prefix = "token:"
        self.indicator_cache_prefix = "indicator:"
        self.price_cache_prefix = "price:"
    
    def cache_token_data(self, token: str, data: Dict[str, Any], ttl: int = 3600) -> bool:
        """í† í° ë°ì´í„° ìºì‹±"""
        key = f"{self.token_cache_prefix}{token}"
        return self.cache_manager.set(key, data, ttl)
    
    def get_token_data(self, token: str) -> Optional[Dict[str, Any]]:
        """í† í° ë°ì´í„° ì¡°íšŒ"""
        key = f"{self.token_cache_prefix}{token}"
        return self.cache_manager.get(key)
    
    def cache_indicator(self, token: str, indicator: str, data: Any, ttl: int = 1800) -> bool:
        """ì§€í‘œ ë°ì´í„° ìºì‹±"""
        key = f"{self.indicator_cache_prefix}{token}:{indicator}"
        return self.cache_manager.set(key, data, ttl)
    
    def get_indicator(self, token: str, indicator: str) -> Optional[Any]:
        """ì§€í‘œ ë°ì´í„° ì¡°íšŒ"""
        key = f"{self.indicator_cache_prefix}{token}:{indicator}"
        return self.cache_manager.get(key)
    
    def cache_price_data(self, token: str, timeframe: str, data: pd.DataFrame, ttl: int = 300) -> bool:
        """ê°€ê²© ë°ì´í„° ìºì‹±"""
        key = f"{self.price_cache_prefix}{token}:{timeframe}"
        return self.cache_manager.set(key, data, ttl)
    
    def get_price_data(self, token: str, timeframe: str) -> Optional[pd.DataFrame]:
        """ê°€ê²© ë°ì´í„° ì¡°íšŒ"""
        key = f"{self.price_cache_prefix}{token}:{timeframe}"
        return self.cache_manager.get(key)
    
    def invalidate_token_cache(self, token: str) -> bool:
        """í† í° ìºì‹œ ë¬´íš¨í™”"""
        patterns = [
            f"{self.token_cache_prefix}{token}",
            f"{self.indicator_cache_prefix}{token}:*",
            f"{self.price_cache_prefix}{token}:*"
        ]
        
        success = True
        for pattern in patterns:
            try:
                # Redisì—ì„œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì‚­ì œ
                keys = self.cache_manager.redis_client.keys(pattern)
                if keys:
                    self.cache_manager.redis_client.delete(*keys)
            except Exception as e:
                logger.error(f"Failed to invalidate cache for pattern {pattern}: {e}")
                success = False
        
        return success

class IndicatorCache:
    """ì§€í‘œ ìºì‹œ"""
    
    def __init__(self, cache_manager: CacheManager):
        self.cache_manager = cache_manager
        self.calculation_cache = {}
    
    def get_cached_indicator(self, token: str, indicator: str, params: Dict[str, Any]) -> Optional[Any]:
        """ìºì‹œëœ ì§€í‘œ ì¡°íšŒ"""
        # íŒŒë¼ë¯¸í„°ë¥¼ í¬í•¨í•œ ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_indicator_key(token, indicator, params)
        return self.cache_manager.get(cache_key)
    
    def cache_indicator(self, token: str, indicator: str, params: Dict[str, Any], 
                       result: Any, ttl: int = 1800) -> bool:
        """ì§€í‘œ ê²°ê³¼ ìºì‹±"""
        cache_key = self._generate_indicator_key(token, indicator, params)
        return self.cache_manager.set(cache_key, result, ttl)
    
    def _generate_indicator_key(self, token: str, indicator: str, params: Dict[str, Any]) -> str:
        """ì§€í‘œ ìºì‹œ í‚¤ ìƒì„±"""
        # íŒŒë¼ë¯¸í„°ë¥¼ ì •ë ¬ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
        param_str = json.dumps(params, sort_keys=True)
        
        # í•´ì‹œ ìƒì„±
        hash_input = f"{token}:{indicator}:{param_str}"
        hash_value = hashlib.md5(hash_input.encode()).hexdigest()
        
        return f"indicator:{hash_value}"
    
    def calculate_with_cache(self, token: str, indicator: str, params: Dict[str, Any], 
                           calculation_func: callable) -> Any:
        """ìºì‹œë¥¼ í™œìš©í•œ ì§€í‘œ ê³„ì‚°"""
        # ìºì‹œì—ì„œ ì¡°íšŒ
        cached_result = self.get_cached_indicator(token, indicator, params)
        if cached_result is not None:
            logger.debug(f"Indicator cache hit: {indicator} for {token}")
            return cached_result
        
        # ê³„ì‚° ì‹¤í–‰
        result = calculation_func(token, params)
        
        # ê²°ê³¼ ìºì‹±
        self.cache_indicator(token, indicator, params, result)
        
        logger.debug(f"Indicator calculated and cached: {indicator} for {token}")
        return result

class PriceCache:
    """ê°€ê²© ë°ì´í„° ìºì‹œ"""
    
    def __init__(self, cache_manager: CacheManager):
        self.cache_manager = cache_manager
        self.price_cache_prefix = "price:"
    
    def get_cached_prices(self, token: str, timeframe: str, limit: int = 1000) -> Optional[pd.DataFrame]:
        """ìºì‹œëœ ê°€ê²© ë°ì´í„° ì¡°íšŒ"""
        key = f"{self.price_cache_prefix}{token}:{timeframe}"
        cached_data = self.cache_manager.get(key)
        
        if cached_data is not None:
            # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
            if len(cached_data) >= limit:
                return cached_data.tail(limit)
            else:
                return cached_data
        
        return None
    
    def cache_prices(self, token: str, timeframe: str, prices: pd.DataFrame, ttl: int = 300) -> bool:
        """ê°€ê²© ë°ì´í„° ìºì‹±"""
        key = f"{self.price_cache_prefix}{token}:{timeframe}"
        return self.cache_manager.set(key, prices, ttl)
    
    def update_price_cache(self, token: str, timeframe: str, new_prices: pd.DataFrame) -> bool:
        """ê°€ê²© ìºì‹œ ì—…ë°ì´íŠ¸"""
        key = f"{self.price_cache_prefix}{token}:{timeframe}"
        
        # ê¸°ì¡´ ìºì‹œ ë°ì´í„° ì¡°íšŒ
        existing_data = self.cache_manager.get(key)
        
        if existing_data is not None:
            # ìƒˆë¡œìš´ ë°ì´í„°ì™€ ë³‘í•©
            updated_data = pd.concat([existing_data, new_prices]).drop_duplicates()
        else:
            updated_data = new_prices
        
        # ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ìºì‹±
        return self.cache_manager.set(key, updated_data, ttl=300)
```

### 3. ì§€ì—° ì‹œê°„ ìµœì í™”

```python
class CachePredictor:
    """ìºì‹œ ì˜ˆì¸¡ê¸°"""
    
    def __init__(self, cache_manager: CacheManager):
        self.cache_manager = cache_manager
        self.access_patterns = {}
        self.prediction_model = None
    
    def record_access_pattern(self, key: str, access_time: datetime):
        """ì ‘ê·¼ íŒ¨í„´ ê¸°ë¡"""
        if key not in self.access_patterns:
            self.access_patterns[key] = []
        
        self.access_patterns[key].append(access_time)
        
        # íŒ¨í„´ ë°ì´í„° í¬ê¸° ì œí•œ
        if len(self.access_patterns[key]) > 1000:
            self.access_patterns[key] = self.access_patterns[key][-500:]
    
    def predict_next_access(self, key: str) -> Optional[datetime]:
        """ë‹¤ìŒ ì ‘ê·¼ ì‹œê°„ ì˜ˆì¸¡"""
        if key not in self.access_patterns or len(self.access_patterns[key]) < 3:
            return None
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ë¶„ì„
        access_times = self.access_patterns[key]
        intervals = []
        
        for i in range(1, len(access_times)):
            interval = (access_times[i] - access_times[i-1]).total_seconds()
            intervals.append(interval)
        
        if intervals:
            avg_interval = sum(intervals) / len(intervals)
            last_access = access_times[-1]
            predicted_next = last_access + timedelta(seconds=avg_interval)
            return predicted_next
        
        return None
    
    def get_hot_keys(self, threshold: int = 10) -> List[str]:
        """ìì£¼ ì ‘ê·¼ë˜ëŠ” í‚¤ ì¡°íšŒ"""
        hot_keys = []
        
        for key, access_times in self.access_patterns.items():
            if len(access_times) >= threshold:
                hot_keys.append(key)
        
        # ì ‘ê·¼ ë¹ˆë„ìˆœ ì •ë ¬
        hot_keys.sort(key=lambda k: len(self.access_patterns[k]), reverse=True)
        return hot_keys

class PrefetchManager:
    """í”„ë¦¬í˜ì¹˜ ê´€ë¦¬ì"""
    
    def __init__(self, cache_manager: CacheManager, predictor: CachePredictor):
        self.cache_manager = cache_manager
        self.predictor = predictor
        self.prefetch_queue = []
        self.prefetch_thread = None
        self.running = False
    
    def start_prefetch_service(self):
        """í”„ë¦¬í˜ì¹˜ ì„œë¹„ìŠ¤ ì‹œì‘"""
        self.running = True
        self.prefetch_thread = threading.Thread(target=self._prefetch_worker)
        self.prefetch_thread.daemon = True
        self.prefetch_thread.start()
    
    def stop_prefetch_service(self):
        """í”„ë¦¬í˜ì¹˜ ì„œë¹„ìŠ¤ ì¤‘ì§€"""
        self.running = False
        if self.prefetch_thread:
            self.prefetch_thread.join()
    
    def _prefetch_worker(self):
        """í”„ë¦¬í˜ì¹˜ ì›Œì»¤"""
        while self.running:
            try:
                # ì˜ˆì¸¡ëœ ì ‘ê·¼ íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ í”„ë¦¬í˜ì¹˜
                hot_keys = self.predictor.get_hot_keys()
                
                for key in hot_keys[:10]:  # ìƒìœ„ 10ê°œë§Œ ì²˜ë¦¬
                    predicted_time = self.predictor.predict_next_access(key)
                    
                    if predicted_time and (predicted_time - datetime.now()).total_seconds() < 60:
                        # 1ë¶„ ì´ë‚´ì— ì ‘ê·¼ ì˜ˆìƒë˜ëŠ” ë°ì´í„° í”„ë¦¬í˜ì¹˜
                        self._prefetch_key(key)
                
                time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì‹¤í–‰
                
            except Exception as e:
                logger.error(f"Prefetch worker error: {e}")
                time.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ ëŒ€ê¸°
    
    def _prefetch_key(self, key: str):
        """í‚¤ í”„ë¦¬í˜ì¹˜"""
        try:
            # ìºì‹œì—ì„œ ì¡°íšŒí•˜ì—¬ ë©”ëª¨ë¦¬ ìºì‹œë¡œ ë¡œë“œ
            value = self.cache_manager.get(key)
            if value is not None:
                logger.debug(f"Prefetched key: {key}")
        except Exception as e:
            logger.error(f"Prefetch error for key {key}: {e}")
    
    def add_to_prefetch_queue(self, key: str, priority: int = 1):
        """í”„ë¦¬í˜ì¹˜ íì— ì¶”ê°€"""
        self.prefetch_queue.append((key, priority))
        self.prefetch_queue.sort(key=lambda x: x[1], reverse=True)  # ìš°ì„ ìˆœìœ„ ì •ë ¬

class EvictionStrategy:
    """ìºì‹œ ì œê±° ì „ëµ"""
    
    def __init__(self, strategy: str = 'lru'):
        self.strategy = strategy
        self.access_times = {}
        self.access_counts = {}
    
    def record_access(self, key: str):
        """ì ‘ê·¼ ê¸°ë¡"""
        current_time = time.time()
        self.access_times[key] = current_time
        self.access_counts[key] = self.access_counts.get(key, 0) + 1
    
    def get_keys_to_evict(self, cache_size: int, target_size: int) -> List[str]:
        """ì œê±°í•  í‚¤ ëª©ë¡ ì¡°íšŒ"""
        if cache_size <= target_size:
            return []
        
        keys_to_remove = cache_size - target_size
        
        if self.strategy == 'lru':
            return self._lru_eviction(keys_to_remove)
        elif self.strategy == 'lfu':
            return self._lfu_eviction(keys_to_remove)
        elif self.strategy == 'fifo':
            return self._fifo_eviction(keys_to_remove)
        else:
            return self._lru_eviction(keys_to_remove)
    
    def _lru_eviction(self, count: int) -> List[str]:
        """LRU ì œê±° ì „ëµ"""
        # ê°€ì¥ ì˜¤ë˜ëœ ì ‘ê·¼ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_keys = sorted(self.access_times.items(), key=lambda x: x[1])
        return [key for key, _ in sorted_keys[:count]]
    
    def _lfu_eviction(self, count: int) -> List[str]:
        """LFU ì œê±° ì „ëµ"""
        # ê°€ì¥ ì ì€ ì ‘ê·¼ íšŸìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_keys = sorted(self.access_counts.items(), key=lambda x: x[1])
        return [key for key, _ in sorted_keys[:count]]
    
    def _fifo_eviction(self, count: int) -> List[str]:
        """FIFO ì œê±° ì „ëµ"""
        # ìºì‹œì— ì¶”ê°€ëœ ìˆœì„œëŒ€ë¡œ ì œê±° (ì ‘ê·¼ ì‹œê°„ì´ ì¶”ê°€ ì‹œê°„ê³¼ ê°™ë‹¤ê³  ê°€ì •)
        sorted_keys = sorted(self.access_times.items(), key=lambda x: x[1])
        return [key for key, _ in sorted_keys[:count]]
```

### 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
class CacheMetrics:
    """ìºì‹œ ë©”íŠ¸ë¦­"""
    
    def __init__(self):
        self.metrics = {
            'hit_rate': 0.0,
            'response_time': 0.0,
            'memory_usage': 0.0,
            'throughput': 0.0
        }
        self.history = []
    
    def update_metrics(self, cache_stats: Dict[str, Any], response_time: float):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        # íˆíŠ¸ìœ¨ ê³„ì‚°
        total_requests = cache_stats['hits'] + cache_stats['misses']
        hit_rate = cache_stats['hits'] / total_requests if total_requests > 0 else 0
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (Redis infoì—ì„œ ì¶”ì¶œ)
        memory_usage = cache_stats.get('redis_info', {}).get('used_memory_human', '0B')
        
        # ì²˜ë¦¬ëŸ‰ (ì´ˆë‹¹ ìš”ì²­ ìˆ˜)
        throughput = total_requests / 60  # 1ë¶„ ê¸°ì¤€
        
        self.metrics.update({
            'hit_rate': hit_rate,
            'response_time': response_time,
            'memory_usage': memory_usage,
            'throughput': throughput
        })
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.history.append({
            'timestamp': datetime.now(),
            'metrics': self.metrics.copy()
        })
        
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        if len(self.history) > 1000:
            self.history = self.history[-500:]
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ìš”ì•½"""
        if not self.history:
            return self.metrics
        
        recent_metrics = self.history[-100:]  # ìµœê·¼ 100ê°œ
        
        avg_hit_rate = sum(m['metrics']['hit_rate'] for m in recent_metrics) / len(recent_metrics)
        avg_response_time = sum(m['metrics']['response_time'] for m in recent_metrics) / len(recent_metrics)
        avg_throughput = sum(m['metrics']['throughput'] for m in recent_metrics) / len(recent_metrics)
        
        return {
            'current': self.metrics,
            'average': {
                'hit_rate': avg_hit_rate,
                'response_time': avg_response_time,
                'throughput': avg_throughput
            },
            'trend': self._calculate_trend()
        }
    
    def _calculate_trend(self) -> str:
        """íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(self.history) < 10:
            return 'insufficient_data'
        
        recent_hit_rates = [m['metrics']['hit_rate'] for m in self.history[-10:]]
        
        # ì„ í˜• íŠ¸ë Œë“œ ê³„ì‚°
        x = np.arange(len(recent_hit_rates))
        slope = np.polyfit(x, recent_hit_rates, 1)[0]
        
        if slope > 0.01:
            return 'improving'
        elif slope < -0.01:
            return 'declining'
        else:
            return 'stable'

class LatencyMonitor:
    """ì§€ì—° ì‹œê°„ ëª¨ë‹ˆí„°"""
    
    def __init__(self):
        self.latency_history = []
        self.alert_threshold = 100  # 100ms
    
    def record_latency(self, operation: str, latency_ms: float):
        """ì§€ì—° ì‹œê°„ ê¸°ë¡"""
        record = {
            'timestamp': datetime.now(),
            'operation': operation,
            'latency_ms': latency_ms
        }
        
        self.latency_history.append(record)
        
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        if len(self.latency_history) > 10000:
            self.latency_history = self.latency_history[-5000:]
        
        # ì•Œë¦¼ ì²´í¬
        if latency_ms > self.alert_threshold:
            logger.warning(f"High latency detected: {operation} took {latency_ms}ms")
    
    def get_latency_stats(self, operation: str = None, minutes: int = 60) -> Dict[str, Any]:
        """ì§€ì—° ì‹œê°„ í†µê³„"""
        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        
        filtered_history = [
            record for record in self.latency_history
            if record['timestamp'] >= cutoff_time
        ]
        
        if operation:
            filtered_history = [
                record for record in filtered_history
                if record['operation'] == operation
            ]
        
        if not filtered_history:
            return {'error': 'No data available'}
        
        latencies = [record['latency_ms'] for record in filtered_history]
        
        return {
            'count': len(latencies),
            'avg_latency': np.mean(latencies),
            'p95_latency': np.percentile(latencies, 95),
            'p99_latency': np.percentile(latencies, 99),
            'max_latency': np.max(latencies),
            'min_latency': np.min(latencies)
        }

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ìµœì í™”"""
    
    def __init__(self, cache_manager: CacheManager):
        self.cache_manager = cache_manager
        self.memory_threshold = 0.8  # 80%
        self.optimization_history = []
    
    def check_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
        try:
            redis_info = self.cache_manager.redis_client.info()
            
            used_memory = redis_info.get('used_memory', 0)
            max_memory = redis_info.get('maxmemory', 0)
            
            if max_memory > 0:
                memory_usage_ratio = used_memory / max_memory
            else:
                memory_usage_ratio = 0
            
            return {
                'used_memory': used_memory,
                'max_memory': max_memory,
                'usage_ratio': memory_usage_ratio,
                'needs_optimization': memory_usage_ratio > self.memory_threshold
            }
            
        except Exception as e:
            logger.error(f"Memory usage check error: {e}")
            return {'error': str(e)}
    
    def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        memory_info = self.check_memory_usage()
        
        if memory_info.get('needs_optimization', False):
            # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
            optimization_result = self._perform_lru_cleanup()
            
            # ìµœì í™” íˆìŠ¤í† ë¦¬ ê¸°ë¡
            self.optimization_history.append({
                'timestamp': datetime.now(),
                'memory_before': memory_info['used_memory'],
                'memory_after': self.check_memory_usage().get('used_memory', 0),
                'keys_removed': optimization_result.get('keys_removed', 0)
            })
            
            return optimization_result
        else:
            return {'status': 'no_optimization_needed'}
    
    def _perform_lru_cleanup(self) -> Dict[str, Any]:
        """LRU ì •ë¦¬ ìˆ˜í–‰"""
        try:
            # Redisì˜ LRU ì •ë¦¬ ìˆ˜í–‰
            keys_removed = 0
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì€ ê²½ìš° ê°•ì œ ì •ë¦¬
            if self.check_memory_usage().get('needs_optimization', False):
                # ì„ì‹œë¡œ ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
                self.cache_manager.optimize_memory()
                keys_removed = 100  # ì˜ˆì‹œ ê°’
            
            return {
                'status': 'success',
                'keys_removed': keys_removed,
                'optimization_type': 'lru_cleanup'
            }
            
        except Exception as e:
            logger.error(f"Memory optimization error: {e}")
            return {
                'status': 'error',
                'error': str(e)
            }
```

## ğŸ“ˆ ì„±ê³¼ ì§€í‘œ

### ìºì‹± ì„±ê³¼
- **ìºì‹œ íˆíŠ¸ìœ¨**: > 90%
- **ì‘ë‹µ ì‹œê°„**: < 10ms (ìºì‹œ íˆíŠ¸ ì‹œ)
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: < 80% ì‚¬ìš©ë¥ 
- **ìºì‹œ ë™ê¸°í™”**: < 100ms

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- **ì§€ì—° ì‹œê°„ P95**: < 50ms
- **ì²˜ë¦¬ëŸ‰**: > 10,000 req/s
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: < 2GB
- **ê°€ìš©ì„±**: > 99.9%

### ìµœì í™” ì„±ê³¼
- **í”„ë¦¬í˜ì¹˜ ì •í™•ë„**: > 70%
- **ë©”ëª¨ë¦¬ ì •ë¦¬ íš¨ìœ¨ì„±**: > 80%
- **ë¶„ì‚° ìºì‹œ ì¼ê´€ì„±**: > 95%
- **ì¥ì•  ë³µêµ¬ ì‹œê°„**: < 30ì´ˆ

## ğŸ”„ ê°œë°œ ë¡œë“œë§µ

### 1ë‹¨ê³„: ê¸°ë³¸ ìºì‹± ì‹œìŠ¤í…œ (2025-09-01 ~ 2025-09-15)
- [x] Redis ìºì‹œ ê´€ë¦¬ì êµ¬í˜„
- [x] ë©”ëª¨ë¦¬ ìºì‹œ êµ¬í˜„
- [ ] ë¶„ì‚° ìºì‹œ êµ¬í˜„
- [ ] ê¸°ë³¸ ìºì‹± ì „ëµ

### 2ë‹¨ê³„: í† í°/ì§€í‘œ ìºì‹± (2025-09-16 ~ 2025-09-30)
- [x] í† í° ìºì‹œ ê´€ë¦¬ì êµ¬í˜„
- [x] ì§€í‘œ ìºì‹œ êµ¬í˜„
- [x] ê°€ê²© ìºì‹œ êµ¬í˜„
- [ ] ìºì‹œ ë¬´íš¨í™” ì „ëµ

### 3ë‹¨ê³„: ì§€ì—° ì‹œê°„ ìµœì í™” (2025-10-01 ~ 2025-10-15)
- [x] ìºì‹œ ì˜ˆì¸¡ê¸° êµ¬í˜„
- [x] í”„ë¦¬í˜ì¹˜ ê´€ë¦¬ì êµ¬í˜„
- [x] ì œê±° ì „ëµ êµ¬í˜„
- [ ] ê³ ê¸‰ ìµœì í™” ì•Œê³ ë¦¬ì¦˜

### 4ë‹¨ê³„: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (2025-10-16 ~ 2025-10-31)
- [x] ìºì‹œ ë©”íŠ¸ë¦­ êµ¬í˜„
- [x] ì§€ì—° ì‹œê°„ ëª¨ë‹ˆí„° êµ¬í˜„
- [x] ë©”ëª¨ë¦¬ ìµœì í™” êµ¬í˜„
- [ ] ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ

### 5ë‹¨ê³„: í†µí•© ë° ìµœì í™” (2025-11-01 ~ 2025-11-15)
- [ ] ëª¨ë“  ëª¨ë“ˆ í†µí•©
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ
- [ë°ì´í„° ê±°ë²„ë„ŒìŠ¤](3.5.8_DATA_GOVERNANCE.md)
- [í†µí•© ìµœì í™”](3.5.12_INTEGRATION_OPTIMIZATION.md)
- [í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”](3.5.10_PORTFOLIO_OPTIMIZATION.md) 