# ğŸ“° Phase 3.5.3: ë‰´ìŠ¤ ì´ë²¤íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ (ë‰´ìŠ¤ + ê°ì • í†µí•©)

## ğŸ“‹ **ê°œìš”**

### ğŸ¯ **ëª©í‘œ**
- **ì´ë²¤íŠ¸ ê°ì§€ ì‹œìŠ¤í…œ**: ê²½ì œì§€í‘œ, ê¸°ì—… ì‹¤ì , ì •ë¶€ ì •ì±… ë“± ì¤‘ìš” ì´ë²¤íŠ¸ ê°ì§€
- **ê²½ì œ ìº˜ë¦°ë” ì—°ë™**: ì£¼ìš” ê²½ì œ ì´ë²¤íŠ¸ ìŠ¤ì¼€ì¤„ë§ ë° ì•Œë¦¼
- **ë‰´ìŠ¤ ì˜í–¥ë„ ë¶„ì„**: ë‰´ìŠ¤ì˜ ì‹œì¥ ì˜í–¥ë„ í‰ê°€ ë° ì˜ˆì¸¡
- **íŒ©íŠ¸ì²´í‚¹ ì‹œìŠ¤í…œ**: ë‰´ìŠ¤ ì‚¬ì‹¤ ê²€ì¦ ë° ì‹ ë¢°ë„ í‰ê°€
- **ë‰´ìŠ¤ ì‹ ë¢°ë„ í‰ê°€**: ì¶œì²˜ ì‹ ë¢°ë„, ì¼ê´€ì„±, ê²€ì¦ ê°€ëŠ¥ì„± í‰ê°€
- **ì†Œì…œ ë¯¸ë””ì–´ ê°ì • ë¶„ì„**: Twitter, Reddit, ë‰´ìŠ¤ ëŒ“ê¸€ ì‹¤ì‹œê°„ ê°ì • ë¶„ì„
- **ë ˆê·¤ë ˆì´ì…˜ ë‰´ìŠ¤ íŒŒì‹±**: ê·œì œ ê´€ë ¨ ë‰´ìŠ¤ ìë™ íŒŒì‹± ë° ë¶„ì„
- **ë‰´ìŠ¤ API ì—°ë™**: ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì†ŒìŠ¤ API í†µí•©
- **ì†Œì…œ ë¯¸ë””ì–´ API ì—°ë™**: Twitter, Reddit, ê¸°íƒ€ ì†Œì…œ í”Œë«í¼ API í†µí•©
- **í‚¤ì›Œë“œ ê¸°ë°˜ ì¤‘ìš”ë„ ìŠ¤ì½”ì–´ë§**: í‚¤ì›Œë“œ ë¶„ì„ì„ í†µí•œ ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€
- **íŒ©íŠ¸ ì²´í¬ ì•Œê³ ë¦¬ì¦˜**: ìë™í™”ëœ ì‚¬ì‹¤ ê²€ì¦ ì‹œìŠ¤í…œ
- **ë‹¤êµ­ì–´ ëª¨ë¸ í•™ìŠµÂ·ì„œë¹™**: í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë“± ë‹¤êµ­ì–´ ì§€ì›
- **ë‰´ìŠ¤ ê¸°ë°˜ ìë™ í¬ì§€ì…˜ ê´€ë¦¬**: ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ìë™ ê±°ë˜
- **ë¦¬ìŠ¤í¬ ì¡°ì • ê¸°ëŠ¥**: ë‰´ìŠ¤ ê¸°ë°˜ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë° í¬ì§€ì…˜ ì¡°ì •
- **ê°ì • ê¸°ë°˜ ê±°ë˜ ì‹ í˜¸**: ê°ì • ì ìˆ˜ ê¸°ë°˜ ê±°ë˜ ì‹ í˜¸ ìƒì„±
- **ì´ë²¤íŠ¸ ê¸°ë°˜ ê±°ë˜ íŠ¸ë¦¬ê±°**: ì¤‘ìš” ì´ë²¤íŠ¸ ë°œìƒ ì‹œ ìë™ ê±°ë˜ ì‹ í˜¸ ìƒì„±
- **ì‹¤ì‹œê°„ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§**: 24/7 ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¼ ë¶„ì„

### ğŸ“Š **ì„±ëŠ¥ ëª©í‘œ**
- **ì´ë²¤íŠ¸ ê°ì§€ ì†ë„**: < 1ì´ˆ ì¤‘ìš” ì´ë²¤íŠ¸ ê°ì§€
- **ë‰´ìŠ¤ ë¶„ì„ ì •í™•ë„**: > 85% ì˜í–¥ë„ ì˜ˆì¸¡ ì •í™•ë„
- **ê°ì • ë¶„ì„ ì •í™•ë„**: > 75% ê°ì • ë¶„ì„ ì •í™•ë„
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: < 500ms ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ
- **ì†Œì…œ ë¯¸ë””ì–´ ì²˜ë¦¬**: < 200ms ê°ì • ë¶„ì„ ì™„ë£Œ
- **ë‹¤êµ­ì–´ ì§€ì›**: ì˜ì–´, í•œêµ­ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´
- **ì´ë²¤íŠ¸ ì˜ˆì¸¡**: > 70% ì´ë²¤íŠ¸ ë°œìƒ ì˜ˆì¸¡ ì •í™•ë„

## ğŸ—ï¸ **ë‰´ìŠ¤ ì´ë²¤íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

### ğŸ“ **ì‹œìŠ¤í…œ êµ¬ì¡°**
```
news-event-analysis/
â”œâ”€â”€ event-detection/                      # ì´ë²¤íŠ¸ ê°ì§€
â”‚   â”œâ”€â”€ economic-indicators/             # ê²½ì œ ì§€í‘œ ê°ì§€
â”‚   â”œâ”€â”€ earnings-reports/                # ì‹¤ì  ë°œí‘œ ê°ì§€
â”‚   â”œâ”€â”€ policy-changes/                  # ì •ì±… ë³€ê²½ ê°ì§€
â”‚   â”œâ”€â”€ market-events/                   # ì‹œì¥ ì´ë²¤íŠ¸ ê°ì§€
â”‚   â””â”€â”€ breaking-news/                   # ê¸´ê¸‰ ë‰´ìŠ¤ ê°ì§€
â”œâ”€â”€ fact-checking/                        # íŒ©íŠ¸ì²´í‚¹ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ fact-checker/                    # íŒ©íŠ¸ ì²´ì»¤
â”‚   â”œâ”€â”€ source-verification/             # ì¶œì²˜ ê²€ì¦
â”‚   â”œâ”€â”€ consistency-checker/             # ì¼ê´€ì„± ê²€ì‚¬
â”‚   â””â”€â”€ credibility-scorer/              # ì‹ ë¢°ë„ ì ìˆ˜
â”œâ”€â”€ news-reliability/                     # ë‰´ìŠ¤ ì‹ ë¢°ë„ í‰ê°€
â”‚   â”œâ”€â”€ source-reliability/              # ì¶œì²˜ ì‹ ë¢°ë„
â”‚   â”œâ”€â”€ content-consistency/             # ë‚´ìš© ì¼ê´€ì„±
â”‚   â”œâ”€â”€ verification-capability/         # ê²€ì¦ ê°€ëŠ¥ì„±
â”‚   â””â”€â”€ reliability-scorer/              # ì‹ ë¢°ë„ ìŠ¤ì½”ì–´ë§
â”œâ”€â”€ regulatory-news-parsing/              # ë ˆê·¤ë ˆì´ì…˜ ë‰´ìŠ¤ íŒŒì‹±
â”‚   â”œâ”€â”€ regulatory-keywords/             # ê·œì œ í‚¤ì›Œë“œ
â”‚   â”œâ”€â”€ policy-extractor/                # ì •ì±… ì¶”ì¶œê¸°
â”‚   â”œâ”€â”€ impact-analyzer/                 # ì˜í–¥ ë¶„ì„ê¸°
â”‚   â””â”€â”€ compliance-checker/              # ì¤€ìˆ˜ì„± ê²€ì‚¬
â”œâ”€â”€ api-integration/                      # API ì—°ë™
â”‚   â”œâ”€â”€ news-apis/                       # ë‰´ìŠ¤ API
â”‚   â”œâ”€â”€ social-media-apis/               # ì†Œì…œ ë¯¸ë””ì–´ API
â”‚   â”œâ”€â”€ calendar-apis/                   # ìº˜ë¦°ë” API
â”‚   â””â”€â”€ regulatory-apis/                 # ê·œì œ API
â”œâ”€â”€ keyword-importance-scoring/           # í‚¤ì›Œë“œ ì¤‘ìš”ë„ ìŠ¤ì½”ì–´ë§
â”‚   â”œâ”€â”€ keyword-extractor/               # í‚¤ì›Œë“œ ì¶”ì¶œê¸°
â”‚   â”œâ”€â”€ importance-calculator/           # ì¤‘ìš”ë„ ê³„ì‚°ê¸°
â”‚   â”œâ”€â”€ trend-analyzer/                  # íŠ¸ë Œë“œ ë¶„ì„ê¸°
â”‚   â””â”€â”€ scoring-engine/                  # ìŠ¤ì½”ì–´ë§ ì—”ì§„
â”œâ”€â”€ fact-check-algorithms/                # íŒ©íŠ¸ ì²´í¬ ì•Œê³ ë¦¬ì¦˜
â”‚   â”œâ”€â”€ claim-extractor/                 # ì£¼ì¥ ì¶”ì¶œê¸°
â”‚   â”œâ”€â”€ evidence-finder/                 # ì¦ê±° ì°¾ê¸°
â”‚   â”œâ”€â”€ verification-engine/             # ê²€ì¦ ì—”ì§„
â”‚   â””â”€â”€ fact-check-scorer/               # íŒ©íŠ¸ ì²´í¬ ìŠ¤ì½”ì–´
â”œâ”€â”€ multi-language-models/                # ë‹¤êµ­ì–´ ëª¨ë¸
â”‚   â”œâ”€â”€ language-detector/               # ì–¸ì–´ ê°ì§€ê¸°
â”‚   â”œâ”€â”€ model-trainer/                   # ëª¨ë¸ í›ˆë ¨ê¸°
â”‚   â”œâ”€â”€ model-servers/                   # ëª¨ë¸ ì„œë²„
â”‚   â””â”€â”€ translation-engine/              # ë²ˆì—­ ì—”ì§„
â”œâ”€â”€ news-based-position-management/       # ë‰´ìŠ¤ ê¸°ë°˜ í¬ì§€ì…˜ ê´€ë¦¬
â”‚   â”œâ”€â”€ position-calculator/             # í¬ì§€ì…˜ ê³„ì‚°ê¸°
â”‚   â”œâ”€â”€ risk-adjuster/                   # ë¦¬ìŠ¤í¬ ì¡°ì •ê¸°
â”‚   â”œâ”€â”€ trade-signal-generator/          # ê±°ë˜ ì‹ í˜¸ ìƒì„±ê¸°
â”‚   â””â”€â”€ position-monitor/                # í¬ì§€ì…˜ ëª¨ë‹ˆí„°
â”œâ”€â”€ economic-calendar/                    # ê²½ì œ ìº˜ë¦°ë”
â”‚   â”œâ”€â”€ calendar-api/                    # ìº˜ë¦°ë” API ì—°ë™
â”‚   â”œâ”€â”€ event-scheduler/                 # ì´ë²¤íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”œâ”€â”€ reminder-system/                 # ì•Œë¦¼ ì‹œìŠ¤í…œ
â”‚   â””â”€â”€ impact-prediction/               # ì˜í–¥ë„ ì˜ˆì¸¡
â”œâ”€â”€ news-impact-analysis/                 # ë‰´ìŠ¤ ì˜í–¥ ë¶„ì„
â”‚   â”œâ”€â”€ sentiment-analyzer/              # ê°ì • ë¶„ì„ê¸°
â”‚   â”œâ”€â”€ impact-scorer/                   # ì˜í–¥ë„ ìŠ¤ì½”ì–´ë§
â”‚   â”œâ”€â”€ market-correlation/              # ì‹œì¥ ìƒê´€ê´€ê³„
â”‚   â””â”€â”€ volatility-prediction/           # ë³€ë™ì„± ì˜ˆì¸¡
â”œâ”€â”€ event-triggers/                       # ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°
â”‚   â”œâ”€â”€ trigger-engine/                  # íŠ¸ë¦¬ê±° ì—”ì§„
â”‚   â”œâ”€â”€ signal-generator/                # ì‹ í˜¸ ìƒì„±ê¸°
â”‚   â”œâ”€â”€ risk-adjuster/                   # ë¦¬ìŠ¤í¬ ì¡°ì •ê¸°
â”‚   â””â”€â”€ position-manager/                # í¬ì§€ì…˜ ê´€ë¦¬ì
â”œâ”€â”€ real-time-monitoring/                 # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
â”‚   â”œâ”€â”€ news-streams/                    # ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¼
â”‚   â”œâ”€â”€ social-media/                    # ì†Œì…œë¯¸ë””ì–´
â”‚   â”œâ”€â”€ financial-news/                  # ê¸ˆìœµ ë‰´ìŠ¤
â”‚   â””â”€â”€ regulatory-news/                 # ê·œì œ ë‰´ìŠ¤
â”œâ”€â”€ data-sources/                         # ë°ì´í„° ì†ŒìŠ¤
    â”œâ”€â”€ news-apis/                       # ë‰´ìŠ¤ API
    â”œâ”€â”€ calendar-apis/                   # ìº˜ë¦°ë” API
    â”œâ”€â”€ social-apis/                     # ì†Œì…œ API
    â””â”€â”€ regulatory-apis/                 # ê·œì œ API
â”œâ”€â”€ sentiment-analysis/                   # ê°ì • ë¶„ì„
    â”œâ”€â”€ social-media-sentiment/          # ì†Œì…œ ë¯¸ë””ì–´ ê°ì •
    â”œâ”€â”€ news-sentiment/                  # ë‰´ìŠ¤ ê°ì • ë¶„ì„
    â”œâ”€â”€ comment-sentiment/               # ëŒ“ê¸€ ê°ì • ë¶„ì„
    â”œâ”€â”€ multi-language-sentiment/        # ë‹¤êµ­ì–´ ê°ì • ë¶„ì„
    â””â”€â”€ sentiment-signal-generator/      # ê°ì • ì‹ í˜¸ ìƒì„±ê¸°
```

## ğŸ”§ **ì´ë²¤íŠ¸ ê°ì§€ ì‹œìŠ¤í…œ**

### ğŸ“¦ **ê²½ì œ ì§€í‘œ ê°ì§€ê¸°**

```python
# news-event-analysis/event-detection/economic_indicator_detector.py
import asyncio
import aiohttp
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
import json
import re
from enum import Enum

logger = logging.getLogger(__name__)

class EventType(Enum):
    """ì´ë²¤íŠ¸ íƒ€ì…"""
    ECONOMIC_INDICATOR = "economic_indicator"
    EARNINGS_REPORT = "earnings_report"
    POLICY_CHANGE = "policy_change"
    MARKET_EVENT = "market_event"
    BREAKING_NEWS = "breaking_news"

class EventImportance(Enum):
    """ì´ë²¤íŠ¸ ì¤‘ìš”ë„"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class EconomicEvent:
    """ê²½ì œ ì´ë²¤íŠ¸"""
    event_id: str
    event_type: EventType
    title: str
    description: str
    importance: EventImportance
    scheduled_time: datetime
    actual_time: Optional[datetime] = None
    expected_value: Optional[float] = None
    actual_value: Optional[float] = None
    previous_value: Optional[float] = None
    currency: str = "USD"
    country: str = "US"
    impact_score: float = 0.0
    market_impact: Dict[str, float] = None

@dataclass
class EventAlert:
    """ì´ë²¤íŠ¸ ì•Œë¦¼"""
    event_id: str
    alert_type: str  # 'scheduled', 'actual', 'deviation'
    message: str
    timestamp: datetime
    importance: EventImportance
    trading_signals: List[Dict] = None

class EconomicIndicatorDetector:
    """ê²½ì œ ì§€í‘œ ê°ì§€ê¸°"""
    
    def __init__(self):
        self.economic_calendar = {}
        self.event_patterns = self._load_event_patterns()
        self.importance_weights = {
            EventImportance.LOW: 0.1,
            EventImportance.MEDIUM: 0.3,
            EventImportance.HIGH: 0.6,
            EventImportance.CRITICAL: 1.0
        }
        self.monitoring_task = None
    
    def _load_event_patterns(self) -> Dict[str, Dict]:
        """ì´ë²¤íŠ¸ íŒ¨í„´ ë¡œë“œ"""
        return {
            'fomc': {
                'keywords': ['FOMC', 'Federal Reserve', 'interest rate', 'monetary policy'],
                'importance': EventImportance.CRITICAL,
                'impact_assets': ['USD', 'US10Y', 'SPY', 'QQQ']
            },
            'non_farm_payrolls': {
                'keywords': ['non-farm payrolls', 'NFP', 'employment', 'jobs report'],
                'importance': EventImportance.HIGH,
                'impact_assets': ['USD', 'US10Y', 'SPY', 'QQQ']
            },
            'cpi': {
                'keywords': ['CPI', 'inflation', 'consumer price index'],
                'importance': EventImportance.HIGH,
                'impact_assets': ['USD', 'US10Y', 'SPY', 'QQQ']
            },
            'gdp': {
                'keywords': ['GDP', 'gross domestic product', 'economic growth'],
                'importance': EventImportance.HIGH,
                'impact_assets': ['USD', 'US10Y', 'SPY', 'QQQ']
            },
            'earnings': {
                'keywords': ['earnings', 'quarterly report', 'Q1', 'Q2', 'Q3', 'Q4'],
                'importance': EventImportance.MEDIUM,
                'impact_assets': ['individual_stocks']
            }
        }
    
    async def start_monitoring(self):
        """ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring_task = asyncio.create_task(self._monitor_events())
        logger.info("ê²½ì œ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    async def stop_monitoring(self):
        """ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if self.monitoring_task:
            self.monitoring_task.cancel()
            logger.info("ê²½ì œ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    async def _monitor_events(self):
        """ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while True:
            try:
                # ìŠ¤ì¼€ì¤„ëœ ì´ë²¤íŠ¸ í™•ì¸
                await self._check_scheduled_events()
                
                # ì‹¤ì‹œê°„ ë‰´ìŠ¤ í™•ì¸
                await self._check_breaking_news()
                
                # 1ë¶„ ëŒ€ê¸°
                await asyncio.sleep(60)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)
    
    async def _check_scheduled_events(self):
        """ìŠ¤ì¼€ì¤„ëœ ì´ë²¤íŠ¸ í™•ì¸"""
        try:
            current_time = datetime.now()
            
            for event_id, event in self.economic_calendar.items():
                # ì´ë²¤íŠ¸ ì‹œê°„ì´ ê°€ê¹Œì›Œì§€ë©´ ì•Œë¦¼
                time_diff = event.scheduled_time - current_time
                
                if timedelta(minutes=30) <= time_diff <= timedelta(minutes=31):
                    await self._send_event_alert(event, 'scheduled_30min')
                
                elif timedelta(minutes=5) <= time_diff <= timedelta(minutes=6):
                    await self._send_event_alert(event, 'scheduled_5min')
                
                elif timedelta(minutes=0) <= time_diff <= timedelta(minutes=1):
                    await self._send_event_alert(event, 'scheduled_now')
                
        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¤„ëœ ì´ë²¤íŠ¸ í™•ì¸ ì˜¤ë¥˜: {e}")
    
    async def _check_breaking_news(self):
        """ê¸´ê¸‰ ë‰´ìŠ¤ í™•ì¸"""
        try:
            # ë‰´ìŠ¤ APIì—ì„œ ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            breaking_news = await self._fetch_breaking_news()
            
            for news in breaking_news:
                # ì´ë²¤íŠ¸ íŒ¨í„´ ë§¤ì¹­
                matched_event = self._match_event_pattern(news)
                
                if matched_event:
                    await self._process_breaking_news(matched_event, news)
                
        except Exception as e:
            logger.error(f"ê¸´ê¸‰ ë‰´ìŠ¤ í™•ì¸ ì˜¤ë¥˜: {e}")
    
    async def _fetch_breaking_news(self) -> List[Dict]:
        """ê¸´ê¸‰ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‰´ìŠ¤ API ì‚¬ìš©
        # ì˜ˆ: Alpha Vantage, NewsAPI, Reuters ë“±
        
        # ì˜ˆì‹œ ë°ì´í„°
        return [
            {
                'title': 'Federal Reserve announces interest rate decision',
                'content': 'The Federal Reserve has decided to raise interest rates by 25 basis points...',
                'timestamp': datetime.now(),
                'source': 'Reuters',
                'url': 'https://example.com/news/1'
            }
        ]
    
    def _match_event_pattern(self, news: Dict) -> Optional[Dict]:
        """ë‰´ìŠ¤ì™€ ì´ë²¤íŠ¸ íŒ¨í„´ ë§¤ì¹­"""
        title = news.get('title', '').lower()
        content = news.get('content', '').lower()
        text = f"{title} {content}"
        
        for pattern_name, pattern in self.event_patterns.items():
            keywords = pattern['keywords']
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
            matches = sum(1 for keyword in keywords if keyword.lower() in text)
            
            if matches >= len(keywords) * 0.5:  # 50% ì´ìƒ ë§¤ì¹­
                return {
                    'pattern_name': pattern_name,
                    'pattern': pattern,
                    'news': news,
                    'match_score': matches / len(keywords)
                }
        
        return None
    
    async def _process_breaking_news(self, matched_event: Dict, news: Dict):
        """ê¸´ê¸‰ ë‰´ìŠ¤ ì²˜ë¦¬"""
        try:
            pattern = matched_event['pattern']
            news_data = matched_event['news']
            
            # ì´ë²¤íŠ¸ ìƒì„±
            event = EconomicEvent(
                event_id=f"breaking_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                event_type=EventType.BREAKING_NEWS,
                title=news_data['title'],
                description=news_data['content'],
                importance=pattern['importance'],
                scheduled_time=datetime.now(),
                actual_time=datetime.now(),
                impact_score=self._calculate_impact_score(pattern, news_data)
            )
            
            # ì•Œë¦¼ ë°œì†¡
            await self._send_event_alert(event, 'breaking_news')
            
            # ê±°ë˜ ì‹ í˜¸ ìƒì„±
            trading_signals = self._generate_trading_signals(event)
            if trading_signals:
                await self._send_trading_signals(trading_signals)
            
        except Exception as e:
            logger.error(f"ê¸´ê¸‰ ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def _calculate_impact_score(self, pattern: Dict, news: Dict) -> float:
        """ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚°"""
        base_score = self.importance_weights[pattern['importance']]
        
        # ë‰´ìŠ¤ ì†ŒìŠ¤ ì‹ ë¢°ë„
        source_credibility = {
            'Reuters': 0.9,
            'Bloomberg': 0.9,
            'CNBC': 0.8,
            'MarketWatch': 0.7
        }
        
        source = news.get('source', '')
        credibility = source_credibility.get(source, 0.5)
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        impact_score = base_score * credibility
        
        return min(1.0, impact_score)
    
    def _generate_trading_signals(self, event: EconomicEvent) -> List[Dict]:
        """ê±°ë˜ ì‹ í˜¸ ìƒì„±"""
        signals = []
        
        # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì‹ í˜¸ ìƒì„±
        if event.event_type == EventType.ECONOMIC_INDICATOR:
            signals.extend(self._generate_economic_signals(event))
        elif event.event_type == EventType.EARNINGS_REPORT:
            signals.extend(self._generate_earnings_signals(event))
        elif event.event_type == EventType.POLICY_CHANGE:
            signals.extend(self._generate_policy_signals(event))
        
        return signals
    
    def _generate_economic_signals(self, event: EconomicEvent) -> List[Dict]:
        """ê²½ì œ ì§€í‘œ ì‹ í˜¸ ìƒì„±"""
        signals = []
        
        # FOMC ì´ë²¤íŠ¸
        if 'fomc' in event.title.lower():
            if 'raise' in event.description.lower():
                signals.append({
                    'type': 'SELL',
                    'asset': 'US10Y',
                    'reason': 'FOMC rate hike expected',
                    'confidence': 0.8
                })
            elif 'cut' in event.description.lower():
                signals.append({
                    'type': 'BUY',
                    'asset': 'US10Y',
                    'reason': 'FOMC rate cut expected',
                    'confidence': 0.8
                })
        
        # NFP ì´ë²¤íŠ¸
        elif 'non-farm payrolls' in event.title.lower():
            if event.actual_value and event.expected_value:
                if event.actual_value > event.expected_value:
                    signals.append({
                        'type': 'BUY',
                        'asset': 'USD',
                        'reason': 'Strong NFP data',
                        'confidence': 0.7
                    })
                else:
                    signals.append({
                        'type': 'SELL',
                        'asset': 'USD',
                        'reason': 'Weak NFP data',
                        'confidence': 0.7
                    })
        
        return signals
    
    async def _send_event_alert(self, event: EconomicEvent, alert_type: str):
        """ì´ë²¤íŠ¸ ì•Œë¦¼ ë°œì†¡"""
        try:
            alert = EventAlert(
                event_id=event.event_id,
                alert_type=alert_type,
                message=f"{event.title} - {event.description}",
                timestamp=datetime.now(),
                importance=event.importance
            )
            
            # ì•Œë¦¼ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡
            await self._send_alert(alert)
            
            logger.info(f"ì´ë²¤íŠ¸ ì•Œë¦¼ ë°œì†¡: {alert_type} - {event.title}")
            
        except Exception as e:
            logger.error(f"ì´ë²¤íŠ¸ ì•Œë¦¼ ë°œì†¡ ì˜¤ë¥˜: {e}")
    
    async def _send_trading_signals(self, signals: List[Dict]):
        """ê±°ë˜ ì‹ í˜¸ ë°œì†¡"""
        try:
            # ê±°ë˜ ì‹œìŠ¤í…œìœ¼ë¡œ ì‹ í˜¸ ì „ì†¡
            for signal in signals:
                await self._send_signal(signal)
            
            logger.info(f"ê±°ë˜ ì‹ í˜¸ ë°œì†¡: {len(signals)}ê°œ ì‹ í˜¸")
            
        except Exception as e:
            logger.error(f"ê±°ë˜ ì‹ í˜¸ ë°œì†¡ ì˜¤ë¥˜: {e}")
    
    async def _send_alert(self, alert: EventAlert):
        """ì•Œë¦¼ ì „ì†¡ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì•Œë¦¼ ì‹œìŠ¤í…œ ì‚¬ìš©)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ë©”ì¼, SMS, ì›¹í›… ë“± ì‚¬ìš©
        pass
    
    async def _send_signal(self, signal: Dict):
        """ì‹ í˜¸ ì „ì†¡ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê±°ë˜ ì‹œìŠ¤í…œ ì‚¬ìš©)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê±°ë˜ ì‹œìŠ¤í…œ API í˜¸ì¶œ
        pass
```

## ğŸ”§ **ê²½ì œ ìº˜ë¦°ë” ì‹œìŠ¤í…œ**

### ğŸ“¦ **ê²½ì œ ìº˜ë¦°ë” ê´€ë¦¬ì**

```python
# news-event-analysis/economic-calendar/calendar_manager.py
import asyncio
import aiohttp
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
import json
import icalendar
from enum import Enum

logger = logging.getLogger(__name__)

class CalendarSource(Enum):
    """ìº˜ë¦°ë” ì†ŒìŠ¤"""
    INVESTING_COM = "investing.com"
    FOREX_FACTORY = "forexfactory.com"
    FX_STREET = "fxstreet.com"
    CUSTOM = "custom"

@dataclass
class CalendarEvent:
    """ìº˜ë¦°ë” ì´ë²¤íŠ¸"""
    event_id: str
    title: str
    description: str
    scheduled_time: datetime
    currency: str
    country: str
    importance: str
    expected_value: Optional[str] = None
    previous_value: Optional[str] = None
    source: str = ""
    url: str = ""

class EconomicCalendarManager:
    """ê²½ì œ ìº˜ë¦°ë” ê´€ë¦¬ì"""
    
    def __init__(self):
        self.calendar_events = {}
        self.calendar_sources = {
            CalendarSource.INVESTING_COM: "https://www.investing.com/economic-calendar/",
            CalendarSource.FOREX_FACTORY: "https://www.forexfactory.com/calendar",
            CalendarSource.FX_STREET: "https://www.fxstreet.com/economic-calendar"
        }
        self.update_interval = 3600  # 1ì‹œê°„ë§ˆë‹¤ ì—…ë°ì´íŠ¸
        self.update_task = None
    
    async def start_calendar_updates(self):
        """ìº˜ë¦°ë” ì—…ë°ì´íŠ¸ ì‹œì‘"""
        self.update_task = asyncio.create_task(self._calendar_update_loop())
        logger.info("ê²½ì œ ìº˜ë¦°ë” ì—…ë°ì´íŠ¸ ì‹œì‘")
    
    async def stop_calendar_updates(self):
        """ìº˜ë¦°ë” ì—…ë°ì´íŠ¸ ì¤‘ì§€"""
        if self.update_task:
            self.update_task.cancel()
            logger.info("ê²½ì œ ìº˜ë¦°ë” ì—…ë°ì´íŠ¸ ì¤‘ì§€")
    
    async def _calendar_update_loop(self):
        """ìº˜ë¦°ë” ì—…ë°ì´íŠ¸ ë£¨í”„"""
        while True:
            try:
                # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ìº˜ë¦°ë” ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                await self._update_calendar_data()
                
                # ìŠ¤ì¼€ì¤„ëœ ì‹œê°„ë§Œí¼ ëŒ€ê¸°
                await asyncio.sleep(self.update_interval)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ìº˜ë¦°ë” ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(self.update_interval)
    
    async def _update_calendar_data(self):
        """ìº˜ë¦°ë” ë°ì´í„° ì—…ë°ì´íŠ¸"""
        try:
            for source, url in self.calendar_sources.items():
                events = await self._fetch_calendar_events(source, url)
                
                if events:
                    self.calendar_events[source] = events
                    logger.info(f"ìº˜ë¦°ë” ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {source.value} - {len(events)}ê°œ ì´ë²¤íŠ¸")
                
        except Exception as e:
            logger.error(f"ìº˜ë¦°ë” ë°ì´í„° ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    async def _fetch_calendar_events(self, source: CalendarSource, url: str) -> List[CalendarEvent]:
        """ìº˜ë¦°ë” ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê° ì‚¬ì´íŠ¸ì˜ API ë˜ëŠ” ì›¹ ìŠ¤í¬ë˜í•‘ ì‚¬ìš©
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ë°ì´í„° ë°˜í™˜
            
            if source == CalendarSource.INVESTING_COM:
                return await self._fetch_investing_calendar()
            elif source == CalendarSource.FOREX_FACTORY:
                return await self._fetch_forex_factory_calendar()
            elif source == CalendarSource.FX_STREET:
                return await self._fetch_fxstreet_calendar()
            
            return []
            
        except Exception as e:
            logger.error(f"ìº˜ë¦°ë” ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜ {source.value}: {e}")
            return []
    
    async def _fetch_investing_calendar(self) -> List[CalendarEvent]:
        """Investing.com ìº˜ë¦°ë” ê°€ì ¸ì˜¤ê¸°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Investing.com API ë˜ëŠ” ì›¹ ìŠ¤í¬ë˜í•‘
        events = []
        
        # ì˜ˆì‹œ ë°ì´í„°
        events.append(CalendarEvent(
            event_id="investing_001",
            title="Non-Farm Payrolls",
            description="Employment change in the non-farm sector",
            scheduled_time=datetime.now() + timedelta(days=1, hours=8),
            currency="USD",
            country="US",
            importance="High",
            expected_value="180K",
            previous_value="175K",
            source="investing.com"
        ))
        
        return events
    
    async def _fetch_forex_factory_calendar(self) -> List[CalendarEvent]:
        """Forex Factory ìº˜ë¦°ë” ê°€ì ¸ì˜¤ê¸°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Forex Factory ì›¹ ìŠ¤í¬ë˜í•‘
        events = []
        
        # ì˜ˆì‹œ ë°ì´í„°
        events.append(CalendarEvent(
            event_id="forex_001",
            title="FOMC Interest Rate Decision",
            description="Federal Reserve interest rate decision",
            scheduled_time=datetime.now() + timedelta(days=2, hours=14),
            currency="USD",
            country="US",
            importance="High",
            expected_value="5.50%",
            previous_value="5.25%",
            source="forexfactory.com"
        ))
        
        return events
    
    async def _fetch_fxstreet_calendar(self) -> List[CalendarEvent]:
        """FX Street ìº˜ë¦°ë” ê°€ì ¸ì˜¤ê¸°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” FX Street API ë˜ëŠ” ì›¹ ìŠ¤í¬ë˜í•‘
        events = []
        
        # ì˜ˆì‹œ ë°ì´í„°
        events.append(CalendarEvent(
            event_id="fxstreet_001",
            title="CPI (YoY)",
            description="Consumer Price Index year-over-year change",
            scheduled_time=datetime.now() + timedelta(days=3, hours=8),
            currency="USD",
            country="US",
            importance="High",
            expected_value="3.1%",
            previous_value="3.2%",
            source="fxstreet.com"
        ))
        
        return events
    
    def get_upcoming_events(self, hours: int = 24) -> List[CalendarEvent]:
        """ë‹¤ê°€ì˜¤ëŠ” ì´ë²¤íŠ¸ ì¡°íšŒ"""
        upcoming_events = []
        current_time = datetime.now()
        end_time = current_time + timedelta(hours=hours)
        
        for source_events in self.calendar_events.values():
            for event in source_events:
                if current_time <= event.scheduled_time <= end_time:
                    upcoming_events.append(event)
        
        # ì‹œê°„ìˆœ ì •ë ¬
        upcoming_events.sort(key=lambda x: x.scheduled_time)
        
        return upcoming_events
    
    def get_events_by_importance(self, importance: str) -> List[CalendarEvent]:
        """ì¤‘ìš”ë„ë³„ ì´ë²¤íŠ¸ ì¡°íšŒ"""
        filtered_events = []
        
        for source_events in self.calendar_events.values():
            for event in source_events:
                if event.importance.lower() == importance.lower():
                    filtered_events.append(event)
        
        return filtered_events
    
    def get_events_by_currency(self, currency: str) -> List[CalendarEvent]:
        """í†µí™”ë³„ ì´ë²¤íŠ¸ ì¡°íšŒ"""
        filtered_events = []
        
        for source_events in self.calendar_events.values():
            for event in source_events:
                if event.currency.upper() == currency.upper():
                    filtered_events.append(event)
        
        return filtered_events
    
    def add_custom_event(self, event: CalendarEvent):
        """ì»¤ìŠ¤í…€ ì´ë²¤íŠ¸ ì¶”ê°€"""
        if CalendarSource.CUSTOM not in self.calendar_events:
            self.calendar_events[CalendarSource.CUSTOM] = []
        
        self.calendar_events[CalendarSource.CUSTOM].append(event)
        logger.info(f"ì»¤ìŠ¤í…€ ì´ë²¤íŠ¸ ì¶”ê°€: {event.title}")
    
    def export_calendar_ics(self, file_path: str):
        """ìº˜ë¦°ë”ë¥¼ iCal í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            cal = icalendar.Calendar()
            cal.add('prodid', '-//Economic Calendar//AutoGrowthTradingSystem//')
            cal.add('version', '2.0')
            
            for source_events in self.calendar_events.values():
                for event in source_events:
                    ical_event = icalendar.Event()
                    ical_event.add('summary', event.title)
                    ical_event.add('description', event.description)
                    ical_event.add('dtstart', event.scheduled_time)
                    ical_event.add('dtend', event.scheduled_time + timedelta(hours=1))
                    ical_event.add('location', f"{event.country} - {event.currency}")
                    
                    cal.add_component(ical_event)
            
            with open(file_path, 'wb') as f:
                f.write(cal.to_ical())
            
            logger.info(f"ìº˜ë¦°ë” ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {file_path}")
            
        except Exception as e:
            logger.error(f"ìº˜ë¦°ë” ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}")
```

## ğŸ”§ **ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ**

### ğŸ“¦ **ì†Œì…œ ë¯¸ë””ì–´ ê°ì • ë¶„ì„ê¸°**

```python
# news-event-analysis/sentiment-analysis/social_media_sentiment_analyzer.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import logging
from textblob import TextBlob
import tweepy
import praw
import requests
from transformers import pipeline

logger = logging.getLogger(__name__)

@dataclass
class SentimentData:
    """ê°ì • ë°ì´í„°"""
    source: str  # 'twitter', 'reddit', 'news_comments'
    text: str
    sentiment_score: float  # -1.0 ~ 1.0
    confidence: float
    language: str
    timestamp: datetime
    user_id: Optional[str] = None
    engagement: int = 0  # ì¢‹ì•„ìš”, ë¦¬íŠ¸ìœ—, ëŒ“ê¸€ ìˆ˜

@dataclass
class SentimentSignal:
    """ê°ì • ì‹ í˜¸"""
    symbol: str
    sentiment_score: float
    confidence: float
    volume: int
    source_breakdown: Dict[str, float]
    timestamp: datetime
    trading_signal: str  # 'BUY', 'SELL', 'HOLD'

class SocialMediaSentimentAnalyzer:
    """ì†Œì…œ ë¯¸ë””ì–´ ê°ì • ë¶„ì„ê¸°"""
    
    def __init__(self, twitter_api_key: str = None, reddit_api_key: str = None):
        self.twitter_api_key = twitter_api_key
        self.reddit_api_key = reddit_api_key
        self.sentiment_pipeline = pipeline("sentiment-analysis", model="cardiffnlp/twitter-roberta-base-sentiment")
        self.sentiment_history = []
        self.sentiment_threshold = 0.3
    
    def analyze_twitter_sentiment(self, symbol: str, keywords: List[str], 
                                count: int = 100) -> List[SentimentData]:
        """Twitter ê°ì • ë¶„ì„"""
        try:
            sentiment_data = []
            
            # Twitter API í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” tweepy ì‚¬ìš©)
            # tweets = self.twitter_api.search_tweets(q=f"${symbol} {' '.join(keywords)}", count=count)
            
            # ì˜ˆì‹œ ë°ì´í„°
            tweets = [
                {"text": f"${symbol} is going to the moon! ğŸš€", "created_at": datetime.now(), "user_id": "user1", "favorite_count": 10, "retweet_count": 5},
                {"text": f"${symbol} earnings look terrible", "created_at": datetime.now(), "user_id": "user2", "favorite_count": 2, "retweet_count": 1},
                {"text": f"${symbol} new product announcement", "created_at": datetime.now(), "user_id": "user3", "favorite_count": 15, "retweet_count": 8}
            ]
            
            for tweet in tweets:
                # ê°ì • ë¶„ì„
                sentiment_result = self.sentiment_pipeline(tweet['text'])[0]
                
                # ê°ì • ì ìˆ˜ ë³€í™˜ (-1.0 ~ 1.0)
                if sentiment_result['label'] == 'POSITIVE':
                    sentiment_score = sentiment_result['score']
                elif sentiment_result['label'] == 'NEGATIVE':
                    sentiment_score = -sentiment_result['score']
                else:
                    sentiment_score = 0.0
                
                # ì°¸ì—¬ë„ ê³„ì‚°
                engagement = tweet.get('favorite_count', 0) + tweet.get('retweet_count', 0) * 2
                
                sentiment_data.append(SentimentData(
                    source='twitter',
                    text=tweet['text'],
                    sentiment_score=sentiment_score,
                    confidence=sentiment_result['score'],
                    language='en',
                    timestamp=tweet['created_at'],
                    user_id=tweet['user_id'],
                    engagement=engagement
                ))
            
            return sentiment_data
            
        except Exception as e:
            logger.error(f"Twitter ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return []
    
    def analyze_reddit_sentiment(self, symbol: str, subreddits: List[str], 
                               limit: int = 100) -> List[SentimentData]:
        """Reddit ê°ì • ë¶„ì„"""
        try:
            sentiment_data = []
            
            # Reddit API í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” praw ì‚¬ìš©)
            # for subreddit_name in subreddits:
            #     subreddit = self.reddit.subreddit(subreddit_name)
            #     posts = subreddit.search(f"{symbol}", limit=limit//len(subreddits))
            
            # ì˜ˆì‹œ ë°ì´í„°
            posts = [
                {"title": f"{symbol} DD - Bullish case", "selftext": "Great fundamentals", "score": 100, "created_utc": datetime.now().timestamp()},
                {"title": f"{symbol} is overvalued", "selftext": "Bubble will pop", "score": 50, "created_utc": datetime.now().timestamp()}
            ]
            
            for post in posts:
                # ì œëª©ê³¼ ë³¸ë¬¸ ê²°í•©
                full_text = f"{post['title']} {post['selftext']}"
                
                # ê°ì • ë¶„ì„
                sentiment_result = self.sentiment_pipeline(full_text)[0]
                
                # ê°ì • ì ìˆ˜ ë³€í™˜
                if sentiment_result['label'] == 'POSITIVE':
                    sentiment_score = sentiment_result['score']
                elif sentiment_result['label'] == 'NEGATIVE':
                    sentiment_score = -sentiment_result['score']
                else:
                    sentiment_score = 0.0
                
                sentiment_data.append(SentimentData(
                    source='reddit',
                    text=full_text,
                    sentiment_score=sentiment_score,
                    confidence=sentiment_result['score'],
                    language='en',
                    timestamp=datetime.fromtimestamp(post['created_utc']),
                    engagement=post['score']
                ))
            
            return sentiment_data
            
        except Exception as e:
            logger.error(f"Reddit ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return []
    
    def generate_sentiment_signal(self, sentiment_data: List[SentimentData], 
                                symbol: str) -> SentimentSignal:
        """ê°ì • ê¸°ë°˜ ê±°ë˜ ì‹ í˜¸ ìƒì„±"""
        try:
            if not sentiment_data:
                return SentimentSignal(
                    symbol=symbol,
                    sentiment_score=0.0,
                    confidence=0.0,
                    volume=0,
                    source_breakdown={},
                    timestamp=datetime.now(),
                    trading_signal='HOLD'
                )
            
            # ì†ŒìŠ¤ë³„ ê°ì • ì ìˆ˜ ê³„ì‚°
            source_sentiments = {}
            total_weighted_sentiment = 0.0
            total_weight = 0.0
            
            for data in sentiment_data:
                # ê°€ì¤‘ì¹˜ ê³„ì‚° (ì°¸ì—¬ë„ ê¸°ë°˜)
                weight = data.engagement + 1
                
                if data.source not in source_sentiments:
                    source_sentiments[data.source] = {
                        'total_sentiment': 0.0,
                        'total_weight': 0.0,
                        'count': 0
                    }
                
                source_sentiments[data.source]['total_sentiment'] += data.sentiment_score * weight
                source_sentiments[data.source]['total_weight'] += weight
                source_sentiments[data.source]['count'] += 1
                
                total_weighted_sentiment += data.sentiment_score * weight
                total_weight += weight
            
            # ì „ì²´ í‰ê·  ê°ì • ì ìˆ˜
            if total_weight > 0:
                overall_sentiment = total_weighted_sentiment / total_weight
            else:
                overall_sentiment = 0.0
            
            # ì†ŒìŠ¤ë³„ í‰ê·  ê³„ì‚°
            source_breakdown = {}
            for source, data in source_sentiments.items():
                if data['total_weight'] > 0:
                    source_breakdown[source] = data['total_sentiment'] / data['total_weight']
                else:
                    source_breakdown[source] = 0.0
            
            # ê±°ë˜ ì‹ í˜¸ ê²°ì •
            if overall_sentiment > self.sentiment_threshold:
                trading_signal = 'BUY'
            elif overall_sentiment < -self.sentiment_threshold:
                trading_signal = 'SELL'
            else:
                trading_signal = 'HOLD'
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = min(1.0, len(sentiment_data) / 100.0)  # ë°ì´í„° ì–‘ ê¸°ë°˜
            
            return SentimentSignal(
                symbol=symbol,
                sentiment_score=overall_sentiment,
                confidence=confidence,
                volume=len(sentiment_data),
                source_breakdown=source_breakdown,
                timestamp=datetime.now(),
                trading_signal=trading_signal
            )
            
        except Exception as e:
            logger.error(f"ê°ì • ì‹ í˜¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return SentimentSignal(
                symbol=symbol,
                sentiment_score=0.0,
                confidence=0.0,
                volume=0,
                source_breakdown={},
                timestamp=datetime.now(),
                trading_signal='HOLD'
            )
```

### ğŸ“¦ **ë‹¤êµ­ì–´ ê°ì • ë¶„ì„ê¸°**

```python
# news-event-analysis/sentiment-analysis/multi_language_sentiment_analyzer.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import logging
from transformers import pipeline
import langdetect

logger = logging.getLogger(__name__)

@dataclass
class MultiLanguageSentiment:
    """ë‹¤êµ­ì–´ ê°ì • ë¶„ì„ ê²°ê³¼"""
    text: str
    detected_language: str
    sentiment_score: float
    confidence: float
    translated_text: Optional[str] = None

class MultiLanguageSentimentAnalyzer:
    """ë‹¤êµ­ì–´ ê°ì • ë¶„ì„ê¸°"""
    
    def __init__(self):
        # ì–¸ì–´ë³„ ê°ì • ë¶„ì„ ëª¨ë¸
        self.sentiment_models = {
            'en': pipeline("sentiment-analysis", model="cardiffnlp/twitter-roberta-base-sentiment"),
            'ko': pipeline("sentiment-analysis", model="klue/roberta-base"),
            'zh': pipeline("sentiment-analysis", model="uer/roberta-base-finetuned-jd-binary-chinese"),
            'ja': pipeline("sentiment-analysis", model="cl-tohoku/bert-base-japanese-v3")
        }
        
        self.language_mapping = {
            'en': 'english',
            'ko': 'korean', 
            'zh': 'chinese',
            'ja': 'japanese'
        }
    
    def detect_language(self, text: str) -> str:
        """ì–¸ì–´ ê°ì§€"""
        try:
            detected = langdetect.detect(text)
            return detected
        except:
            return 'en'  # ê¸°ë³¸ê°’
    
    def analyze_sentiment(self, text: str, target_language: str = None) -> MultiLanguageSentiment:
        """ë‹¤êµ­ì–´ ê°ì • ë¶„ì„"""
        try:
            # ì–¸ì–´ ê°ì§€
            detected_lang = self.detect_language(text)
            
            # ë¶„ì„í•  ì–¸ì–´ ê²°ì •
            if target_language and target_language in self.sentiment_models:
                analysis_lang = target_language
            elif detected_lang in self.sentiment_models:
                analysis_lang = detected_lang
            else:
                analysis_lang = 'en'  # ê¸°ë³¸ê°’
            
            # ê°ì • ë¶„ì„ ìˆ˜í–‰
            model = self.sentiment_models[analysis_lang]
            result = model(text)[0]
            
            # ê°ì • ì ìˆ˜ ë³€í™˜
            if result['label'] in ['POSITIVE', 'positive', 'ê¸ì •']:
                sentiment_score = result['score']
            elif result['label'] in ['NEGATIVE', 'negative', 'ë¶€ì •']:
                sentiment_score = -result['score']
            else:
                sentiment_score = 0.0
            
            return MultiLanguageSentiment(
                text=text,
                detected_language=detected_lang,
                sentiment_score=sentiment_score,
                confidence=result['score']
            )
            
        except Exception as e:
            logger.error(f"ë‹¤êµ­ì–´ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return MultiLanguageSentiment(
                text=text,
                detected_language='en',
                sentiment_score=0.0,
                confidence=0.0
            )
    
    def analyze_batch(self, texts: List[str]) -> List[MultiLanguageSentiment]:
        """ë°°ì¹˜ ê°ì • ë¶„ì„"""
        results = []
        
        for text in texts:
            result = self.analyze_sentiment(text)
            results.append(result)
        
        return results
```

## ğŸ“Š **ì„±ê³¼ ì§€í‘œ**

### **ëª©í‘œ ì„±ê³¼**
- **ì´ë²¤íŠ¸ ê°ì§€ ì •í™•ë„**: 90% ì´ìƒ
- **ë‰´ìŠ¤ ë¶„ì„ ì •í™•ë„**: 85% ì´ìƒ
- **ê°ì • ë¶„ì„ ì •í™•ë„**: 75% ì´ìƒ
- **ì´ë²¤íŠ¸ ì˜ˆì¸¡ ì •í™•ë„**: 70% ì´ìƒ
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: < 1ì´ˆ
- **ë‹¤êµ­ì–´ ì§€ì›**: 4ê°œ ì–¸ì–´

### **ì„±ëŠ¥ ì§€í‘œ**
- **ì´ë²¤íŠ¸ ê°ì§€ ì†ë„**: < 1ì´ˆ
- **ë‰´ìŠ¤ ë¶„ì„ ì‹œê°„**: < 500ms
- **ê°ì • ë¶„ì„ ì‹œê°„**: < 200ms
- **ìº˜ë¦°ë” ì—…ë°ì´íŠ¸**: < 1ì‹œê°„
- **ì•Œë¦¼ ë°œì†¡ ì‹œê°„**: < 100ms
- **ì‹œìŠ¤í…œ ê°€ë™ë¥ **: > 99.5%

## ğŸ”— **ê´€ë ¨ ë¬¸ì„œ**

- [Phase 3.5.1: ê¸°ìˆ ì  ì§€í‘œ ë¶„ì„](3.5.1_TECHNICAL_ANALYSIS.md)
- [Phase 3.5.2: ê±°ë˜ ì „ëµ ë¼ì´ë¸ŒëŸ¬ë¦¬](3.5.2_TRADING_STRATEGIES.md)
- [Phase 3.5.4: ì˜¨ë¼ì¸ í•™ìŠµ](3.5.4_ONLINE_LEARNING.md)
- [Phase 3.5.5: ì„¤ëª… ê°€ëŠ¥í•œ AI](3.5.5_EXPLAINABLE_AI.md)

## ğŸ”§ **ìƒˆë¡œìš´ ëª¨ë“ˆ êµ¬í˜„ ì½”ë“œ**

### ğŸ” **íŒ©íŠ¸ì²´í‚¹ ì‹œìŠ¤í…œ**

```python
# news-event-analysis/fact-checking/fact_checker.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import logging
import re
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)

@dataclass
class FactCheckResult:
    """íŒ©íŠ¸ì²´í¬ ê²°ê³¼"""
    claim: str
    verification_score: float
    evidence_sources: List[str]
    consistency_score: float
    credibility_score: float
    final_verdict: str

class FactChecker:
    """íŒ©íŠ¸ì²´ì»¤"""
    
    def __init__(self):
        self.trusted_sources = [
            'reuters.com', 'bloomberg.com', 'wsj.com', 
            'ft.com', 'cnbc.com', 'marketwatch.com'
        ]
        self.verification_threshold = 0.7
        
    def extract_claims(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì¥ ì¶”ì¶œ"""
        try:
            claims = []
            
            # ì£¼ì¥ íŒ¨í„´ ë§¤ì¹­
            claim_patterns = [
                r'([A-Z][^.]*(?:ì¦ê°€|ê°ì†Œ|ìƒìŠ¹|í•˜ë½|ë³€í™”)[^.]*\.)',
                r'([A-Z][^.]*(?:ë°œí‘œ|ê³µê°œ|ë°œí‘œí–ˆë‹¤|ê³µê°œí–ˆë‹¤)[^.]*\.)',
                r'([A-Z][^.]*(?:ì˜ˆìƒ|ì „ë§|ì˜ˆì¸¡)[^.]*\.)',
                r'([A-Z][^.]*(?:í™•ì¸|ê²€ì¦|ì…ì¦)[^.]*\.)'
            ]
            
            for pattern in claim_patterns:
                matches = re.findall(pattern, text)
                claims.extend(matches)
            
            return list(set(claims))  # ì¤‘ë³µ ì œê±°
            
        except Exception as e:
            logger.error(f"ì£¼ì¥ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def find_evidence(self, claim: str, news_sources: List[Dict]) -> List[Dict]:
        """ì¦ê±° ì°¾ê¸°"""
        try:
            evidence = []
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(claim)
            
            for source in news_sources:
                relevance_score = self._calculate_relevance(claim, source['content'], keywords)
                
                if relevance_score > 0.3:  # ì„ê³„ê°’
                    evidence.append({
                        'source': source['source'],
                        'content': source['content'],
                        'relevance_score': relevance_score,
                        'timestamp': source['timestamp']
                    })
            
            # ê´€ë ¨ì„± ì ìˆ˜ë³„ ì •ë ¬
            evidence.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            return evidence[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
            
        except Exception as e:
            logger.error(f"ì¦ê±° ì°¾ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    def verify_claim(self, claim: str, evidence: List[Dict]) -> FactCheckResult:
        """ì£¼ì¥ ê²€ì¦"""
        try:
            if not evidence:
                return FactCheckResult(
                    claim=claim,
                    verification_score=0.0,
                    evidence_sources=[],
                    consistency_score=0.0,
                    credibility_score=0.0,
                    final_verdict='insufficient_evidence'
                )
            
            # ê²€ì¦ ì ìˆ˜ ê³„ì‚°
            verification_score = self._calculate_verification_score(claim, evidence)
            
            # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
            consistency_score = self._calculate_consistency_score(evidence)
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            credibility_score = self._calculate_credibility_score(evidence)
            
            # ìµœì¢… íŒì •
            final_verdict = self._determine_verdict(verification_score, consistency_score, credibility_score)
            
            return FactCheckResult(
                claim=claim,
                verification_score=verification_score,
                evidence_sources=[e['source'] for e in evidence],
                consistency_score=consistency_score,
                credibility_score=credibility_score,
                final_verdict=final_verdict
            )
            
        except Exception as e:
            logger.error(f"ì£¼ì¥ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return FactCheckResult(
                claim=claim,
                verification_score=0.0,
                evidence_sources=[],
                consistency_score=0.0,
                credibility_score=0.0,
                final_verdict='error'
            )
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
        words = text.split()
        keywords = [word for word in words if len(word) > 2]
        return keywords[:10]  # ìƒìœ„ 10ê°œ
    
    def _calculate_relevance(self, claim: str, content: str, keywords: List[str]) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
            similarity = SequenceMatcher(None, claim, content).ratio()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            keyword_matches = sum(1 for keyword in keywords if keyword.lower() in content.lower())
            keyword_score = keyword_matches / len(keywords) if keywords else 0
            
            # ì¢…í•© ì ìˆ˜
            relevance_score = 0.6 * similarity + 0.4 * keyword_score
            
            return min(1.0, relevance_score)
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _calculate_verification_score(self, claim: str, evidence: List[Dict]) -> float:
        """ê²€ì¦ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not evidence:
                return 0.0
            
            # ì¦ê±°ì˜ í‰ê·  ê´€ë ¨ì„± ì ìˆ˜
            avg_relevance = np.mean([e['relevance_score'] for e in evidence])
            
            # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ì˜ ë¹„ìœ¨
            trusted_count = sum(1 for e in evidence if any(source in e['source'] for source in self.trusted_sources))
            trusted_ratio = trusted_count / len(evidence)
            
            # ê²€ì¦ ì ìˆ˜
            verification_score = 0.7 * avg_relevance + 0.3 * trusted_ratio
            
            return min(1.0, verification_score)
            
        except Exception as e:
            logger.error(f"ê²€ì¦ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _calculate_consistency_score(self, evidence: List[Dict]) -> float:
        """ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if len(evidence) < 2:
                return 1.0
            
            # ì¦ê±° ê°„ ìœ ì‚¬ì„± ê³„ì‚°
            similarities = []
            for i in range(len(evidence)):
                for j in range(i + 1, len(evidence)):
                    similarity = SequenceMatcher(None, evidence[i]['content'], evidence[j]['content']).ratio()
                    similarities.append(similarity)
            
            if similarities:
                consistency_score = np.mean(similarities)
            else:
                consistency_score = 1.0
            
            return consistency_score
            
        except Exception as e:
            logger.error(f"ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _calculate_credibility_score(self, evidence: List[Dict]) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not evidence:
                return 0.0
            
            credibility_scores = []
            
            for e in evidence:
                # ì¶œì²˜ ì‹ ë¢°ë„
                source_credibility = 1.0 if any(source in e['source'] for source in self.trusted_sources) else 0.5
                
                # ê´€ë ¨ì„± ì ìˆ˜
                relevance_score = e['relevance_score']
                
                # ì¢…í•© ì‹ ë¢°ë„
                credibility = 0.6 * source_credibility + 0.4 * relevance_score
                credibility_scores.append(credibility)
            
            return np.mean(credibility_scores)
            
        except Exception as e:
            logger.error(f"ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _determine_verdict(self, verification_score: float, consistency_score: float, credibility_score: float) -> str:
        """ìµœì¢… íŒì •"""
        try:
            # ì¢…í•© ì ìˆ˜
            overall_score = (verification_score + consistency_score + credibility_score) / 3
            
            if overall_score >= self.verification_threshold:
                return 'verified'
            elif overall_score >= 0.5:
                return 'partially_verified'
            elif overall_score >= 0.3:
                return 'unverified'
            else:
                return 'false'
                
        except Exception as e:
            logger.error(f"ìµœì¢… íŒì • ì˜¤ë¥˜: {e}")
            return 'error'
```

### ğŸ“° **ë‰´ìŠ¤ ì‹ ë¢°ë„ í‰ê°€ ì‹œìŠ¤í…œ**

```python
# news-event-analysis/news-reliability/news_reliability_scorer.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import logging
import re

logger = logging.getLogger(__name__)

@dataclass
class ReliabilityScore:
    """ì‹ ë¢°ë„ ì ìˆ˜"""
    source_reliability: float
    content_consistency: float
    verification_capability: float
    overall_reliability: float
    confidence_level: str

class NewsReliabilityScorer:
    """ë‰´ìŠ¤ ì‹ ë¢°ë„ í‰ê°€ê¸°"""
    
    def __init__(self):
        self.trusted_sources = {
            'reuters.com': 0.95,
            'bloomberg.com': 0.93,
            'wsj.com': 0.92,
            'ft.com': 0.91,
            'cnbc.com': 0.88,
            'marketwatch.com': 0.87
        }
        self.medium_sources = {
            'yahoo.com': 0.75,
            'cnn.com': 0.72,
            'bbc.com': 0.78
        }
        
    def evaluate_source_reliability(self, source_url: str, source_history: Dict) -> float:
        """ì¶œì²˜ ì‹ ë¢°ë„ í‰ê°€"""
        try:
            # ê¸°ë³¸ ì‹ ë¢°ë„ ì ìˆ˜
            if source_url in self.trusted_sources:
                base_score = self.trusted_sources[source_url]
            elif source_url in self.medium_sources:
                base_score = self.medium_sources[source_url]
            else:
                base_score = 0.5  # ê¸°ë³¸ê°’
            
            # íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì¡°ì •
            if source_history:
                accuracy_rate = source_history.get('accuracy_rate', 0.5)
                consistency_rate = source_history.get('consistency_rate', 0.5)
                
                # ì¡°ì •ëœ ì ìˆ˜
                adjusted_score = base_score * 0.6 + accuracy_rate * 0.25 + consistency_rate * 0.15
            else:
                adjusted_score = base_score
            
            return min(1.0, adjusted_score)
            
        except Exception as e:
            logger.error(f"ì¶œì²˜ ì‹ ë¢°ë„ í‰ê°€ ì˜¤ë¥˜: {e}")
            return 0.5
    
    def evaluate_content_consistency(self, content: str, similar_articles: List[Dict]) -> float:
        """ë‚´ìš© ì¼ê´€ì„± í‰ê°€"""
        try:
            if not similar_articles:
                return 0.5  # ë¹„êµ ëŒ€ìƒì´ ì—†ìœ¼ë©´ ì¤‘ê°„ê°’
            
            consistency_scores = []
            
            for article in similar_articles:
                # í‚¤ì›Œë“œ ì¼ì¹˜ë„
                content_keywords = self._extract_keywords(content)
                article_keywords = self._extract_keywords(article['content'])
                
                keyword_overlap = len(set(content_keywords) & set(article_keywords))
                keyword_consistency = keyword_overlap / max(len(content_keywords), len(article_keywords)) if content_keywords and article_keywords else 0
                
                # ê°ì • ì¼ê´€ì„±
                sentiment_consistency = 1.0 - abs(content.get('sentiment', 0) - article.get('sentiment', 0))
                
                # ì¢…í•© ì¼ê´€ì„±
                consistency = (keyword_consistency + sentiment_consistency) / 2
                consistency_scores.append(consistency)
            
            return np.mean(consistency_scores)
            
        except Exception as e:
            logger.error(f"ë‚´ìš© ì¼ê´€ì„± í‰ê°€ ì˜¤ë¥˜: {e}")
            return 0.5
    
    def evaluate_verification_capability(self, content: str, metadata: Dict) -> float:
        """ê²€ì¦ ê°€ëŠ¥ì„± í‰ê°€"""
        try:
            verification_score = 0.5  # ê¸°ë³¸ê°’
            
            # ì¸ìš© ë° ì¶œì²˜
            if metadata.get('citations'):
                verification_score += 0.2
            
            # ë§í¬ ë° ì°¸ì¡°
            if metadata.get('references'):
                verification_score += 0.15
            
            # ì‘ì„±ì ì •ë³´
            if metadata.get('author') and metadata.get('author_credentials'):
                verification_score += 0.1
            
            # ë‚ ì§œ ë° ì‹œê°„
            if metadata.get('timestamp'):
                verification_score += 0.05
            
            # ìˆ˜ì¹˜ ë° ë°ì´í„°
            if self._has_numerical_data(content):
                verification_score += 0.1
            
            return min(1.0, verification_score)
            
        except Exception as e:
            logger.error(f"ê²€ì¦ ê°€ëŠ¥ì„± í‰ê°€ ì˜¤ë¥˜: {e}")
            return 0.5
    
    def calculate_overall_reliability(self, source_reliability: float, 
                                    content_consistency: float, 
                                    verification_capability: float) -> ReliabilityScore:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            # ê°€ì¤‘ í‰ê· 
            overall_reliability = (
                source_reliability * 0.4 +
                content_consistency * 0.35 +
                verification_capability * 0.25
            )
            
            # ì‹ ë¢°ë„ ë ˆë²¨ ê²°ì •
            if overall_reliability >= 0.8:
                confidence_level = 'high'
            elif overall_reliability >= 0.6:
                confidence_level = 'medium'
            elif overall_reliability >= 0.4:
                confidence_level = 'low'
            else:
                confidence_level = 'very_low'
            
            return ReliabilityScore(
                source_reliability=source_reliability,
                content_consistency=content_consistency,
                verification_capability=verification_capability,
                overall_reliability=overall_reliability,
                confidence_level=confidence_level
            )
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return ReliabilityScore(0.5, 0.5, 0.5, 0.5, 'low')
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        words = re.findall(r'\b\w+\b', text.lower())
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        keywords = [word for word in words if word not in stop_words and len(word) > 3]
        return keywords[:20]  # ìƒìœ„ 20ê°œ
    
    def _has_numerical_data(self, content: str) -> bool:
        """ìˆ˜ì¹˜ ë°ì´í„° í¬í•¨ ì—¬ë¶€"""
        # ìˆ«ì íŒ¨í„´ ì°¾ê¸°
        number_patterns = [
            r'\d+\.\d+%',  # í¼ì„¼íŠ¸
            r'\$\d+',      # ë‹¬ëŸ¬
            r'\d+\.\d+',   # ì†Œìˆ˜ì 
            r'\d+',        # ì •ìˆ˜
        ]
        
        for pattern in number_patterns:
            if re.search(pattern, content):
                return True
        
        return False
```

### ğŸ“‹ **ë ˆê·¤ë ˆì´ì…˜ ë‰´ìŠ¤ íŒŒì‹± ì‹œìŠ¤í…œ**

```python
# news-event-analysis/regulatory-news-parsing/regulatory_news_parser.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import logging
import re

logger = logging.getLogger(__name__)

@dataclass
class RegulatoryEvent:
    """ê·œì œ ì´ë²¤íŠ¸"""
    event_type: str
    affected_assets: List[str]
    impact_level: str
    effective_date: datetime
    description: str
    compliance_requirements: List[str]

class RegulatoryNewsParser:
    """ë ˆê·¤ë ˆì´ì…˜ ë‰´ìŠ¤ íŒŒì„œ"""
    
    def __init__(self):
        self.regulatory_keywords = {
            'policy': ['ì •ì±…', 'policy', 'regulation', 'ê·œì œ', 'ë²•ì•ˆ', 'bill'],
            'enforcement': ['ì§‘í–‰', 'enforcement', 'ë‹¨ì†', 'crackdown', 'ê·œì œê°•í™”'],
            'compliance': ['ì¤€ìˆ˜', 'compliance', 'ì˜ë¬´', 'obligation', 'ìš”êµ¬ì‚¬í•­'],
            'penalty': ['ë²Œê¸ˆ', 'penalty', 'ì œì¬', 'sanction', 'ì§•ê³„'],
            'approval': ['ìŠ¹ì¸', 'approval', 'í—ˆê°€', 'permit', 'ì¸ê°€'],
            'ban': ['ê¸ˆì§€', 'ban', 'prohibition', 'ì œí•œ', 'restriction']
        }
        
        self.impact_levels = {
            'high': ['ê¸ˆì§€', 'ban', 'prohibition', 'ì¤‘ë‹¨', 'suspension'],
            'medium': ['ì œí•œ', 'restriction', 'ê·œì œ', 'regulation', 'ê°•í™”'],
            'low': ['ê°€ì´ë“œë¼ì¸', 'guideline', 'ê¶Œê³ ', 'recommendation']
        }
    
    def parse_regulatory_news(self, news_content: str, metadata: Dict) -> Optional[RegulatoryEvent]:
        """ê·œì œ ë‰´ìŠ¤ íŒŒì‹±"""
        try:
            # ê·œì œ í‚¤ì›Œë“œ ê°ì§€
            detected_keywords = self._detect_regulatory_keywords(news_content)
            
            if not detected_keywords:
                return None
            
            # ì´ë²¤íŠ¸ íƒ€ì… ê²°ì •
            event_type = self._determine_event_type(detected_keywords)
            
            # ì˜í–¥ë°›ëŠ” ìì‚° ì¶”ì¶œ
            affected_assets = self._extract_affected_assets(news_content)
            
            # ì˜í–¥ ìˆ˜ì¤€ í‰ê°€
            impact_level = self._assess_impact_level(news_content, detected_keywords)
            
            # ë°œíš¨ì¼ ì¶”ì¶œ
            effective_date = self._extract_effective_date(news_content, metadata)
            
            # ì„¤ëª… ìƒì„±
            description = self._generate_description(news_content, detected_keywords)
            
            # ì¤€ìˆ˜ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            compliance_requirements = self._extract_compliance_requirements(news_content)
            
            return RegulatoryEvent(
                event_type=event_type,
                affected_assets=affected_assets,
                impact_level=impact_level,
                effective_date=effective_date,
                description=description,
                compliance_requirements=compliance_requirements
            )
            
        except Exception as e:
            logger.error(f"ê·œì œ ë‰´ìŠ¤ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _detect_regulatory_keywords(self, content: str) -> Dict[str, List[str]]:
        """ê·œì œ í‚¤ì›Œë“œ ê°ì§€"""
        detected_keywords = {}
        
        for category, keywords in self.regulatory_keywords.items():
            found_keywords = []
            for keyword in keywords:
                if keyword.lower() in content.lower():
                    found_keywords.append(keyword)
            
            if found_keywords:
                detected_keywords[category] = found_keywords
        
        return detected_keywords
    
    def _determine_event_type(self, keywords: Dict[str, List[str]]) -> str:
        """ì´ë²¤íŠ¸ íƒ€ì… ê²°ì •"""
        if 'policy' in keywords:
            return 'policy_change'
        elif 'enforcement' in keywords:
            return 'enforcement_action'
        elif 'compliance' in keywords:
            return 'compliance_requirement'
        elif 'penalty' in keywords:
            return 'penalty_announcement'
        elif 'approval' in keywords:
            return 'approval_granted'
        elif 'ban' in keywords:
            return 'ban_announcement'
        else:
            return 'regulatory_update'
    
    def _extract_affected_assets(self, content: str) -> List[str]:
        """ì˜í–¥ë°›ëŠ” ìì‚° ì¶”ì¶œ"""
        # ìì‚° í‚¤ì›Œë“œ íŒ¨í„´
        asset_patterns = [
            r'([A-Z]{2,5})',  # ì£¼ì‹ ì‹¬ë³¼
            r'([ê°€-í£]+ì£¼)',   # í•œêµ­ ì£¼ì‹
            r'(crypto|bitcoin|ethereum)',  # ì•”í˜¸í™”í
            r'(ê¸ˆìœµ|ì€í–‰|ë³´í—˜|ì¦ê¶Œ)',  # ê¸ˆìœµ ì„¹í„°
        ]
        
        affected_assets = []
        
        for pattern in asset_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            affected_assets.extend(matches)
        
        return list(set(affected_assets))  # ì¤‘ë³µ ì œê±°
    
    def _assess_impact_level(self, content: str, keywords: Dict[str, List[str]]) -> str:
        """ì˜í–¥ ìˆ˜ì¤€ í‰ê°€"""
        impact_score = 0.0
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜
        for category, found_keywords in keywords.items():
            if category in ['ban', 'enforcement']:
                impact_score += 0.4
            elif category in ['policy', 'compliance']:
                impact_score += 0.3
            elif category in ['penalty']:
                impact_score += 0.2
            else:
                impact_score += 0.1
        
        # ì˜í–¥ ìˆ˜ì¤€ í‚¤ì›Œë“œ í™•ì¸
        for level, level_keywords in self.impact_levels.items():
            for keyword in level_keywords:
                if keyword.lower() in content.lower():
                    if level == 'high':
                        impact_score += 0.3
                    elif level == 'medium':
                        impact_score += 0.2
                    else:
                        impact_score += 0.1
        
        # ì˜í–¥ ìˆ˜ì¤€ ê²°ì •
        if impact_score >= 0.6:
            return 'high'
        elif impact_score >= 0.3:
            return 'medium'
        else:
            return 'low'
    
    def _extract_effective_date(self, content: str, metadata: Dict) -> datetime:
        """ë°œíš¨ì¼ ì¶”ì¶œ"""
        try:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            if metadata.get('published_date'):
                return metadata['published_date']
            
            # í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
            date_patterns = [
                r'(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)',
                r'(\d{1,2}/\d{1,2}/\d{4})',
                r'(\d{4}-\d{2}-\d{2})',
                r'(ë‚´ë…„|ë‚´ì›”|ë‹¤ìŒì£¼|ì˜¤ëŠ˜|ë‚´ì¼)'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, content)
                if match:
                    # ë‚ ì§œ íŒŒì‹± ë¡œì§ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
                    return datetime.now()  # ì‹¤ì œë¡œëŠ” íŒŒì‹±ëœ ë‚ ì§œ ë°˜í™˜
            
            # ê¸°ë³¸ê°’
            return datetime.now()
            
        except Exception as e:
            logger.error(f"ë°œíš¨ì¼ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return datetime.now()
    
    def _generate_description(self, content: str, keywords: Dict[str, List[str]]) -> str:
        """ì„¤ëª… ìƒì„±"""
        try:
            # í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
            sentences = content.split('.')
            
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
            relevant_sentences = []
            for sentence in sentences:
                for category, found_keywords in keywords.items():
                    if any(keyword.lower() in sentence.lower() for keyword in found_keywords):
                        relevant_sentences.append(sentence.strip())
                        break
            
            if relevant_sentences:
                return '. '.join(relevant_sentences[:3])  # ìƒìœ„ 3ê°œ ë¬¸ì¥
            else:
                return content[:200] + '...'  # ì²˜ìŒ 200ì
                
        except Exception as e:
            logger.error(f"ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {e}")
            return content[:200] + '...'
    
    def _extract_compliance_requirements(self, content: str) -> List[str]:
        """ì¤€ìˆ˜ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        requirements = []
        
        # ìš”êµ¬ì‚¬í•­ íŒ¨í„´
        requirement_patterns = [
            r'([ê°€-í£\s]+í•´ì•¼\s+í•œë‹¤)',
            r'([ê°€-í£\s]+í•„ìš”í•˜ë‹¤)',
            r'([ê°€-í£\s]+ì˜ë¬´ê°€\s+ìˆë‹¤)',
            r'(must\s+[a-zA-Z\s]+)',
            r'(required\s+to\s+[a-zA-Z\s]+)',
            r'(obligated\s+to\s+[a-zA-Z\s]+)'
        ]
        
        for pattern in requirement_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            requirements.extend(matches)
        
        return list(set(requirements))  # ì¤‘ë³µ ì œê±°
```

### ğŸ”— **API ì—°ë™ ì‹œìŠ¤í…œ**

```python
# news-event-analysis/api-integration/api_integration_manager.py
import asyncio
import aiohttp
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
import json
import os

logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬"""
    title: str
    content: str
    source: str
    url: str
    published_date: datetime
    sentiment_score: float
    importance_score: float

class APIIntegrationManager:
    """API ì—°ë™ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.api_keys = {
            'news_api': os.getenv('NEWS_API_KEY', ''),
            'twitter_api': os.getenv('TWITTER_API_KEY', ''),
            'reddit_api': os.getenv('REDDIT_API_KEY', ''),
            'alpha_vantage': os.getenv('ALPHA_VANTAGE_KEY', '')
        }
        
        self.rate_limits = {
            'news_api': {'requests_per_minute': 100, 'last_request': datetime.now()},
            'twitter_api': {'requests_per_minute': 300, 'last_request': datetime.now()},
            'reddit_api': {'requests_per_minute': 60, 'last_request': datetime.now()}
        }
    
    async def fetch_news_articles(self, keywords: List[str], 
                                sources: List[str] = None,
                                max_articles: int = 50) -> List[NewsArticle]:
        """ë‰´ìŠ¤ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°"""
        try:
            articles = []
            
            # News API ì‚¬ìš©
            if self.api_keys['news_api']:
                news_articles = await self._fetch_from_news_api(keywords, sources, max_articles)
                articles.extend(news_articles)
            
            # ë‹¤ë¥¸ ë‰´ìŠ¤ ì†ŒìŠ¤ë“¤
            other_articles = await self._fetch_from_other_sources(keywords, max_articles)
            articles.extend(other_articles)
            
            return articles[:max_articles]
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    async def fetch_social_media_posts(self, keywords: List[str], 
                                     platforms: List[str] = None,
                                     max_posts: int = 100) -> List[Dict]:
        """ì†Œì…œ ë¯¸ë””ì–´ í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            posts = []
            
            if not platforms:
                platforms = ['twitter', 'reddit']
            
            for platform in platforms:
                if platform == 'twitter' and self.api_keys['twitter_api']:
                    twitter_posts = await self._fetch_from_twitter(keywords, max_posts // len(platforms))
                    posts.extend(twitter_posts)
                
                elif platform == 'reddit' and self.api_keys['reddit_api']:
                    reddit_posts = await self._fetch_from_reddit(keywords, max_posts // len(platforms))
                    posts.extend(reddit_posts)
            
            return posts
            
        except Exception as e:
            logger.error(f"ì†Œì…œ ë¯¸ë””ì–´ í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    async def fetch_economic_calendar(self, start_date: datetime, 
                                    end_date: datetime) -> List[Dict]:
        """ê²½ì œ ìº˜ë¦°ë” ê°€ì ¸ì˜¤ê¸°"""
        try:
            calendar_events = []
            
            # Alpha Vantage API ì‚¬ìš©
            if self.api_keys['alpha_vantage']:
                events = await self._fetch_from_alpha_vantage(start_date, end_date)
                calendar_events.extend(events)
            
            # ë‹¤ë¥¸ ìº˜ë¦°ë” APIë“¤
            other_events = await self._fetch_from_other_calendars(start_date, end_date)
            calendar_events.extend(other_events)
            
            return calendar_events
            
        except Exception as e:
            logger.error(f"ê²½ì œ ìº˜ë¦°ë” ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    async def _fetch_from_news_api(self, keywords: List[str], 
                                  sources: List[str], 
                                  max_articles: int) -> List[NewsArticle]:
        """News APIì—ì„œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # Rate limiting ì²´í¬
            if not self._check_rate_limit('news_api'):
                return []
            
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': ' OR '.join(keywords),
                'language': 'en',
                'sortBy': 'publishedAt',
                'pageSize': min(max_articles, 100),
                'apiKey': self.api_keys['news_api']
            }
            
            if sources:
                params['domains'] = ','.join(sources)
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        articles = []
                        for article in data.get('articles', []):
                            news_article = NewsArticle(
                                title=article.get('title', ''),
                                content=article.get('description', ''),
                                source=article.get('source', {}).get('name', ''),
                                url=article.get('url', ''),
                                published_date=datetime.fromisoformat(article.get('publishedAt', '').replace('Z', '+00:00')),
                                sentiment_score=0.0,  # ë‚˜ì¤‘ì— ê³„ì‚°
                                importance_score=0.0   # ë‚˜ì¤‘ì— ê³„ì‚°
                            )
                            articles.append(news_article)
                        
                        return articles
                    else:
                        logger.error(f"News API ì˜¤ë¥˜: {response.status}")
                        return []
            
        except Exception as e:
            logger.error(f"News API ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    async def _fetch_from_twitter(self, keywords: List[str], max_posts: int) -> List[Dict]:
        """Twitterì—ì„œ í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # Rate limiting ì²´í¬
            if not self._check_rate_limit('twitter_api'):
                return []
            
            # Twitter API v2 ì‚¬ìš© (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì ì ˆí•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
            posts = []
            
            # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜
            for keyword in keywords:
                # ì‹¤ì œë¡œëŠ” Twitter API í˜¸ì¶œ
                posts.append({
                    'platform': 'twitter',
                    'content': f'Simulated tweet about {keyword}',
                    'author': 'user123',
                    'timestamp': datetime.now(),
                    'sentiment_score': 0.0
                })
            
            return posts[:max_posts]
            
        except Exception as e:
            logger.error(f"Twitter ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    async def _fetch_from_reddit(self, keywords: List[str], max_posts: int) -> List[Dict]:
        """Redditì—ì„œ í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # Rate limiting ì²´í¬
            if not self._check_rate_limit('reddit_api'):
                return []
            
            posts = []
            
            # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜
            for keyword in keywords:
                # ì‹¤ì œë¡œëŠ” Reddit API í˜¸ì¶œ
                posts.append({
                    'platform': 'reddit',
                    'content': f'Simulated Reddit post about {keyword}',
                    'author': 'redditor123',
                    'timestamp': datetime.now(),
                    'sentiment_score': 0.0
                })
            
            return posts[:max_posts]
            
        except Exception as e:
            logger.error(f"Reddit ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    def _check_rate_limit(self, api_name: str) -> bool:
        """Rate limiting ì²´í¬"""
        try:
            rate_limit = self.rate_limits[api_name]
            time_since_last = datetime.now() - rate_limit['last_request']
            
            if time_since_last.total_seconds() < 60 / rate_limit['requests_per_minute']:
                return False
            
            rate_limit['last_request'] = datetime.now()
            return True
            
        except Exception as e:
            logger.error(f"Rate limiting ì²´í¬ ì˜¤ë¥˜: {e}")
            return True
    
    async def _fetch_from_other_sources(self, keywords: List[str], max_articles: int) -> List[NewsArticle]:
        """ë‹¤ë¥¸ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì†ŒìŠ¤ API ì—°ë™
        return []
    
    async def _fetch_from_alpha_vantage(self, start_date: datetime, end_date: datetime) -> List[Dict]:
        """Alpha Vantageì—ì„œ ê²½ì œ ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Alpha Vantage API ì‚¬ìš©
        return []
    
    async def _fetch_from_other_calendars(self, start_date: datetime, end_date: datetime) -> List[Dict]:
        """ë‹¤ë¥¸ ìº˜ë¦°ë” APIì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‹¤ì–‘í•œ ìº˜ë¦°ë” API ì—°ë™
        return []
```

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-01-26  
**í”„ë¡œì íŠ¸ ìƒíƒœ**: ì„¤ê³„ ì™„ë£Œ, ê°œë°œ ì¤€ë¹„  
**ë‹¤ìŒ ë‹¨ê³„**: ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ êµ¬í˜„ 