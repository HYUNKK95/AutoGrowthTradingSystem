# ğŸ” Phase 3.5.5: ì„¤ëª… ê°€ëŠ¥í•œ AI (XAI) ì‹œìŠ¤í…œ (XAI + ìœ„í—˜ê´€ë¦¬ + ë°±í…ŒìŠ¤íŒ… í†µí•©)

## ğŸ“‹ **ê°œìš”**

### ğŸ¯ **ëª©í‘œ**
- **SHAP ë¶„ì„**: ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ë° ê¸°ì—¬ë„ ë¶„ì„
- **LIME ì„¤ëª…**: ê°œë³„ ì˜ˆì¸¡ì— ëŒ€í•œ ì§€ì—­ì  ì„¤ëª… ìƒì„±
- **íŠ¹ì„± ì¤‘ìš”ë„**: ëª¨ë¸ì´ ì–´ë–¤ ì§€í‘œì™€ ë‰´ìŠ¤ì— ë°˜ì‘í•˜ëŠ”ì§€ ì‹œê°í™”
- **ê±°ë˜ ê²°ì • ì‹œê°í™”**: ê±°ë˜ ì‹ í˜¸ì˜ ê·¼ê±°ì™€ ì‹ ë¢°ë„ í‘œì‹œ
- **ëª¨ë¸ íˆ¬ëª…ì„±**: AI ëª¨ë¸ì˜ ê²°ì • ê³¼ì •ì„ ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…
- **ê³ ê¸‰ ìœ„í—˜ ê´€ë¦¬**: VaR, CVaR, ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸, í¬íŠ¸í´ë¦¬ì˜¤ ìœ„í—˜ ê´€ë¦¬
- **ì •êµí•œ ë°±í…ŒìŠ¤íŒ…**: ê³ ê¸‰ ë°±í…ŒìŠ¤íŒ… ì—”ì§„, ì›Œí¬í¬ì›Œë“œ í…ŒìŠ¤íŠ¸, ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
- **ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤**: ëŒ€ì‹œë³´ë“œ, ë¶„ì„ ë„êµ¬, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

### ğŸ“Š **ì„±ëŠ¥ ëª©í‘œ**
- **SHAP ë¶„ì„ ì‹œê°„**: < 10ì´ˆ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°
- **LIME ì„¤ëª… ìƒì„±**: < 1ì´ˆ ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…
- **ì‹œê°í™” ìƒì„±**: < 2ì´ˆ ì°¨íŠ¸ ë° ê·¸ë˜í”„ ìƒì„±
- **ì‹¤ì‹œê°„ ì„¤ëª…**: < 100ms ê±°ë˜ ì‹ í˜¸ ì„¤ëª…
- **ìœ„í—˜ ê³„ì‚°**: < 1ì´ˆ VaR/CVaR ê³„ì‚°
- **ë°±í…ŒìŠ¤íŒ…**: < 5ë¶„ ì „ì²´ ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- **í˜ì´ì§€ ë¡œë”©**: < 2ì´ˆ ëŒ€ì‹œë³´ë“œ ë¡œë”©
- **ì •í™•ë„**: > 90% ì„¤ëª… ì •í™•ë„

## ğŸ—ï¸ **ì„¤ëª… ê°€ëŠ¥í•œ AI ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

### ğŸ“ **ì‹œìŠ¤í…œ êµ¬ì¡°**
```
explainable-ai/
â”œâ”€â”€ shap-analysis/                        # SHAP ë¶„ì„
â”‚   â”œâ”€â”€ feature-importance/              # íŠ¹ì„± ì¤‘ìš”ë„
â”‚   â”œâ”€â”€ interaction-effects/             # ìƒí˜¸ì‘ìš© íš¨ê³¼
â”‚   â”œâ”€â”€ global-explanations/             # ì „ì—­ì  ì„¤ëª…
â”‚   â””â”€â”€ local-explanations/              # ì§€ì—­ì  ì„¤ëª…
â”œâ”€â”€ lime-explanation/                     # LIME ì„¤ëª…
â”‚   â”œâ”€â”€ local-interpretation/            # ì§€ì—­ì  í•´ì„
â”‚   â”œâ”€â”€ feature-selection/               # íŠ¹ì„± ì„ íƒ
â”‚   â”œâ”€â”€ explanation-generator/           # ì„¤ëª… ìƒì„±ê¸°
â”‚   â””â”€â”€ confidence-scorer/               # ì‹ ë¢°ë„ ìŠ¤ì½”ì–´ë§
â”œâ”€â”€ feature-importance/                   # íŠ¹ì„± ì¤‘ìš”ë„
â”‚   â”œâ”€â”€ permutation-importance/          # ìˆœì—´ ì¤‘ìš”ë„
â”‚   â”œâ”€â”€ tree-importance/                 # íŠ¸ë¦¬ ì¤‘ìš”ë„
â”‚   â”œâ”€â”€ correlation-analysis/            # ìƒê´€ê´€ê³„ ë¶„ì„
â”‚   â””â”€â”€ temporal-importance/             # ì‹œê°„ì  ì¤‘ìš”ë„
â”œâ”€â”€ decision-visualization/               # ê²°ì • ì‹œê°í™”
â”‚   â”œâ”€â”€ decision-trees/                  # ê²°ì • íŠ¸ë¦¬
â”‚   â”œâ”€â”€ feature-attribution/             # íŠ¹ì„± ê¸°ì—¬ë„
â”‚   â”œâ”€â”€ confidence-intervals/            # ì‹ ë¢° êµ¬ê°„
â”‚   â””â”€â”€ uncertainty-quantification/      # ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
â”œâ”€â”€ model-transparency/                   # ëª¨ë¸ íˆ¬ëª…ì„±
â”‚   â”œâ”€â”€ model-interpretability/          # ëª¨ë¸ í•´ì„ì„±
â”‚   â”œâ”€â”€ bias-detection/                  # í¸í–¥ ê°ì§€
â”‚   â”œâ”€â”€ fairness-assessment/             # ê³µì •ì„± í‰ê°€
â”‚   â””â”€â”€ robustness-testing/              # ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸
â”œâ”€â”€ explanation-dashboard/                # ì„¤ëª… ëŒ€ì‹œë³´ë“œ
    â”œâ”€â”€ real-time-monitoring/            # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
    â”œâ”€â”€ historical-analysis/             # íˆìŠ¤í† ë¦¬ ë¶„ì„
    â”œâ”€â”€ alert-system/                    # ì•Œë¦¼ ì‹œìŠ¤í…œ
    â””â”€â”€ report-generator/                # ë¦¬í¬íŠ¸ ìƒì„±ê¸°
â”œâ”€â”€ advanced-risk-management/             # ê³ ê¸‰ ìœ„í—˜ ê´€ë¦¬
    â”œâ”€â”€ var-cvar-calculator/             # VaR/CVaR ê³„ì‚°ê¸°
    â”œâ”€â”€ stress-testing/                  # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
    â”œâ”€â”€ portfolio-risk/                  # í¬íŠ¸í´ë¦¬ì˜¤ ìœ„í—˜
    â”œâ”€â”€ drawdown-analysis/               # ë“œë¡œìš°ë‹¤ìš´ ë¶„ì„
    â””â”€â”€ risk-dashboard/                  # ìœ„í—˜ ëŒ€ì‹œë³´ë“œ
â”œâ”€â”€ sophisticated-backtesting/            # ì •êµí•œ ë°±í…ŒìŠ¤íŒ…
    â”œâ”€â”€ backtest-engine/                 # ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„
    â”œâ”€â”€ walk-forward-testing/            # ì›Œí¬í¬ì›Œë“œ í…ŒìŠ¤íŠ¸
    â”œâ”€â”€ monte-carlo-simulation/          # ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
    â”œâ”€â”€ performance-metrics/             # ì„±ê³¼ ì§€í‘œ
    â””â”€â”€ strategy-validation/             # ì „ëµ ê²€ì¦
â””â”€â”€ user-interface/                       # ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤
    â”œâ”€â”€ trading-dashboard/               # ê±°ë˜ ëŒ€ì‹œë³´ë“œ
    â”œâ”€â”€ analysis-tools/                  # ë¶„ì„ ë„êµ¬
    â”œâ”€â”€ real-time-monitoring/            # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
    â””â”€â”€ reporting-system/                # ë¦¬í¬íŒ… ì‹œìŠ¤í…œ
```

## ğŸ”§ **SHAP ë¶„ì„ ì‹œìŠ¤í…œ**

### ğŸ“¦ **SHAP ë¶„ì„ê¸°**

```python
# explainable-ai/shap-analysis/shap_analyzer.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from datetime import datetime
import logging
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

@dataclass
class SHAPExplanation:
    """SHAP ì„¤ëª…"""
    explanation_id: str
    feature_names: List[str]
    feature_values: List[float]
    shap_values: np.ndarray
    base_value: float
    prediction: float
    timestamp: datetime
    model_type: str
    explanation_type: str  # 'local', 'global'

@dataclass
class FeatureImportance:
    """íŠ¹ì„± ì¤‘ìš”ë„"""
    feature_name: str
    importance_score: float
    rank: int
    contribution_type: str  # 'positive', 'negative', 'neutral'
    confidence_interval: Tuple[float, float]

class SHAPAnalyzer:
    """SHAP ë¶„ì„ê¸°"""
    
    def __init__(self, model=None, background_data=None):
        self.model = model
        self.background_data = background_data
        self.explainer = None
        self.feature_names = []
        self.explanations_history = []
        
        # SHAP ì„¤ëª…ê¸° ì´ˆê¸°í™”
        if model is not None:
            self._initialize_explainer()
    
    def _initialize_explainer(self):
        """SHAP ì„¤ëª…ê¸° ì´ˆê¸°í™”"""
        try:
            if self.background_data is None:
                # TreeSHAP (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ìš©)
                if hasattr(self.model, 'feature_importances_'):
                    self.explainer = shap.TreeExplainer(self.model)
                else:
                    # KernelSHAP (ì¼ë°˜ ëª¨ë¸ìš©)
                    self.explainer = shap.KernelExplainer(self.model.predict, self.background_data)
            else:
                # ë°°ê²½ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                if hasattr(self.model, 'feature_importances_'):
                    self.explainer = shap.TreeExplainer(self.model, self.background_data)
                else:
                    self.explainer = shap.KernelExplainer(self.model.predict, self.background_data)
            
            logger.info("SHAP ì„¤ëª…ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"SHAP ì„¤ëª…ê¸° ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    
    def analyze_feature_importance(self, data: Union[np.ndarray, pd.DataFrame]) -> List[FeatureImportance]:
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        try:
            if self.explainer is None:
                logger.error("SHAP ì„¤ëª…ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return []
            
            # SHAP ê°’ ê³„ì‚°
            if isinstance(data, pd.DataFrame):
                shap_values = self.explainer.shap_values(data)
                feature_names = data.columns.tolist()
            else:
                shap_values = self.explainer.shap_values(data)
                feature_names = [f"feature_{i}" for i in range(data.shape[1])]
            
            # í‰ê·  ì ˆëŒ“ê°’ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
            if len(shap_values.shape) > 1:
                mean_abs_shap = np.mean(np.abs(shap_values), axis=0)
            else:
                mean_abs_shap = np.abs(shap_values)
            
            # íŠ¹ì„± ì¤‘ìš”ë„ ì •ë ¬
            feature_importance_pairs = list(zip(feature_names, mean_abs_shap))
            feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)
            
            # FeatureImportance ê°ì²´ ìƒì„±
            feature_importances = []
            for rank, (feature_name, importance_score) in enumerate(feature_importance_pairs):
                # ê¸°ì—¬ë„ íƒ€ì… ê²°ì •
                if len(shap_values.shape) > 1:
                    mean_shap = np.mean(shap_values[:, rank])
                else:
                    mean_shap = shap_values[rank]
                
                if mean_shap > 0:
                    contribution_type = 'positive'
                elif mean_shap < 0:
                    contribution_type = 'negative'
                else:
                    contribution_type = 'neutral'
                
                # ì‹ ë¢° êµ¬ê°„ ê³„ì‚° (ë¶€íŠ¸ìŠ¤íŠ¸ë©)
                confidence_interval = self._calculate_confidence_interval(shap_values, rank)
                
                feature_importance = FeatureImportance(
                    feature_name=feature_name,
                    importance_score=importance_score,
                    rank=rank + 1,
                    contribution_type=contribution_type,
                    confidence_interval=confidence_interval
                )
                
                feature_importances.append(feature_importance)
            
            return feature_importances
            
        except Exception as e:
            logger.error(f"íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return []
    
    def explain_prediction(self, data_point: Union[np.ndarray, pd.DataFrame]) -> SHAPExplanation:
        """ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…"""
        try:
            if self.explainer is None:
                logger.error("SHAP ì„¤ëª…ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return None
            
            # SHAP ê°’ ê³„ì‚°
            if isinstance(data_point, pd.DataFrame):
                shap_values = self.explainer.shap_values(data_point)
                feature_names = data_point.columns.tolist()
                feature_values = data_point.iloc[0].values
            else:
                shap_values = self.explainer.shap_values(data_point)
                feature_names = [f"feature_{i}" for i in range(data_point.shape[1])]
                feature_values = data_point.flatten()
            
            # ì˜ˆì¸¡ê°’ ê³„ì‚°
            prediction = self.model.predict(data_point)[0] if hasattr(self.model, 'predict') else 0.0
            
            # ê¸°ë³¸ê°’ (ë°°ê²½ ë°ì´í„° í‰ê· )
            base_value = self.explainer.expected_value
            
            explanation = SHAPExplanation(
                explanation_id=f"shap_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                feature_names=feature_names,
                feature_values=feature_values.tolist(),
                shap_values=shap_values,
                base_value=base_value,
                prediction=prediction,
                timestamp=datetime.now(),
                model_type=type(self.model).__name__,
                explanation_type='local'
            )
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.explanations_history.append(explanation)
            
            return explanation
            
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì„¤ëª… ì˜¤ë¥˜: {e}")
            return None
    
    def analyze_interaction_effects(self, data: Union[np.ndarray, pd.DataFrame]) -> Dict[str, float]:
        """ìƒí˜¸ì‘ìš© íš¨ê³¼ ë¶„ì„"""
        try:
            if self.explainer is None:
                logger.error("SHAP ì„¤ëª…ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return {}
            
            # SHAP ìƒí˜¸ì‘ìš© ê°’ ê³„ì‚°
            if isinstance(data, pd.DataFrame):
                shap_interaction_values = self.explainer.shap_interaction_values(data)
                feature_names = data.columns.tolist()
            else:
                shap_interaction_values = self.explainer.shap_interaction_values(data)
                feature_names = [f"feature_{i}" for i in range(data.shape[1])]
            
            # ìƒí˜¸ì‘ìš© íš¨ê³¼ ê³„ì‚°
            interaction_effects = {}
            
            for i in range(len(feature_names)):
                for j in range(i + 1, len(feature_names)):
                    interaction_key = f"{feature_names[i]}_x_{feature_names[j]}"
                    
                    # ìƒí˜¸ì‘ìš© ê°•ë„ ê³„ì‚°
                    interaction_strength = np.mean(np.abs(shap_interaction_values[:, i, j]))
                    interaction_effects[interaction_key] = interaction_strength
            
            # ìƒí˜¸ì‘ìš© íš¨ê³¼ ì •ë ¬
            sorted_interactions = sorted(interaction_effects.items(), 
                                       key=lambda x: x[1], reverse=True)
            
            return dict(sorted_interactions)
            
        except Exception as e:
            logger.error(f"ìƒí˜¸ì‘ìš© íš¨ê³¼ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}
    
    def _calculate_confidence_interval(self, shap_values: np.ndarray, 
                                     feature_idx: int, confidence: float = 0.95) -> Tuple[float, float]:
        """ì‹ ë¢° êµ¬ê°„ ê³„ì‚°"""
        try:
            if len(shap_values.shape) > 1:
                feature_shap_values = shap_values[:, feature_idx]
            else:
                feature_shap_values = shap_values
            
            # ë¶€íŠ¸ìŠ¤íŠ¸ë©ìœ¼ë¡œ ì‹ ë¢° êµ¬ê°„ ê³„ì‚°
            n_bootstrap = 1000
            bootstrap_means = []
            
            for _ in range(n_bootstrap):
                bootstrap_sample = np.random.choice(feature_shap_values, 
                                                   size=len(feature_shap_values), 
                                                   replace=True)
                bootstrap_means.append(np.mean(bootstrap_sample))
            
            # ì‹ ë¢° êµ¬ê°„ ê³„ì‚°
            alpha = 1 - confidence
            lower_percentile = (alpha / 2) * 100
            upper_percentile = (1 - alpha / 2) * 100
            
            lower_bound = np.percentile(bootstrap_means, lower_percentile)
            upper_bound = np.percentile(bootstrap_means, upper_percentile)
            
            return (lower_bound, upper_bound)
            
        except Exception as e:
            logger.error(f"ì‹ ë¢° êµ¬ê°„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return (0.0, 0.0)
    
    def generate_visualization(self, explanation: SHAPExplanation, 
                             plot_type: str = 'waterfall') -> str:
        """ì‹œê°í™” ìƒì„±"""
        try:
            if plot_type == 'waterfall':
                return self._generate_waterfall_plot(explanation)
            elif plot_type == 'bar':
                return self._generate_bar_plot(explanation)
            elif plot_type == 'force':
                return self._generate_force_plot(explanation)
            else:
                logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í”Œë¡¯ íƒ€ì…: {plot_type}")
                return ""
                
        except Exception as e:
            logger.error(f"ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {e}")
            return ""
    
    def _generate_waterfall_plot(self, explanation: SHAPExplanation) -> str:
        """ì›Œí„°í´ í”Œë¡¯ ìƒì„±"""
        try:
            plt.figure(figsize=(10, 6))
            
            # SHAP ê°’ê³¼ íŠ¹ì„±ëª… ì¤€ë¹„
            shap_values = explanation.shap_values.flatten()
            feature_names = explanation.feature_names
            
            # ì›Œí„°í´ í”Œë¡¯ ìƒì„±
            shap.waterfall_plot(
                shap.Explanation(
                    values=shap_values,
                    base_values=explanation.base_value,
                    feature_names=feature_names
                ),
                show=False
            )
            
            plt.title(f"SHAP Waterfall Plot - Prediction: {explanation.prediction:.4f}")
            plt.tight_layout()
            
            # íŒŒì¼ë¡œ ì €ì¥
            file_path = f"shap_waterfall_{explanation.explanation_id}.png"
            plt.savefig(file_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            return file_path
            
        except Exception as e:
            logger.error(f"ì›Œí„°í´ í”Œë¡¯ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""
    
    def _generate_bar_plot(self, explanation: SHAPExplanation) -> str:
        """ë°” í”Œë¡¯ ìƒì„±"""
        try:
            plt.figure(figsize=(10, 6))
            
            # íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°
            feature_importances = self.analyze_feature_importance(
                np.array([explanation.feature_values])
            )
            
            # ìƒìœ„ 10ê°œ íŠ¹ì„±ë§Œ ì„ íƒ
            top_features = feature_importances[:10]
            
            feature_names = [f.feature_name for f in top_features]
            importance_scores = [f.importance_score for f in top_features]
            colors = ['red' if f.contribution_type == 'negative' else 'blue' 
                     for f in top_features]
            
            # ë°” í”Œë¡¯ ìƒì„±
            plt.barh(range(len(feature_names)), importance_scores, color=colors)
            plt.yticks(range(len(feature_names)), feature_names)
            plt.xlabel('SHAP Importance')
            plt.title('Feature Importance (SHAP)')
            plt.gca().invert_yaxis()
            
            plt.tight_layout()
            
            # íŒŒì¼ë¡œ ì €ì¥
            file_path = f"shap_bar_{explanation.explanation_id}.png"
            plt.savefig(file_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            return file_path
            
        except Exception as e:
            logger.error(f"ë°” í”Œë¡¯ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""
    
    def _generate_force_plot(self, explanation: SHAPExplanation) -> str:
        """í¬ìŠ¤ í”Œë¡¯ ìƒì„±"""
        try:
            plt.figure(figsize=(12, 4))
            
            # í¬ìŠ¤ í”Œë¡¯ ìƒì„±
            shap.force_plot(
                explanation.base_value,
                explanation.shap_values,
                explanation.feature_values,
                feature_names=explanation.feature_names,
                show=False
            )
            
            plt.title(f"SHAP Force Plot - Prediction: {explanation.prediction:.4f}")
            plt.tight_layout()
            
            # íŒŒì¼ë¡œ ì €ì¥
            file_path = f"shap_force_{explanation.explanation_id}.png"
            plt.savefig(file_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            return file_path
            
        except Exception as e:
            logger.error(f"í¬ìŠ¤ í”Œë¡¯ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""
```

## ğŸ”§ **LIME ì„¤ëª… ì‹œìŠ¤í…œ**

### ğŸ“¦ **LIME ì„¤ëª…ê¸°**

```python
# explainable-ai/lime-explanation/lime_explainer.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from datetime import datetime
import logging
from lime import lime_tabular
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

@dataclass
class LIMEExplanation:
    """LIME ì„¤ëª…"""
    explanation_id: str
    feature_names: List[str]
    feature_weights: List[float]
    feature_values: List[float]
    prediction: float
    confidence: float
    timestamp: datetime
    explanation_type: str = 'local'

@dataclass
class FeatureContribution:
    """íŠ¹ì„± ê¸°ì—¬ë„"""
    feature_name: str
    weight: float
    value: float
    contribution: float
    rank: int

class LIMEExplainer:
    """LIME ì„¤ëª…ê¸°"""
    
    def __init__(self, training_data: Union[np.ndarray, pd.DataFrame], 
                 feature_names: List[str] = None, 
                 class_names: List[str] = None):
        self.training_data = training_data
        self.feature_names = feature_names
        self.class_names = class_names
        self.explainer = None
        self.explanations_history = []
        
        # LIME ì„¤ëª…ê¸° ì´ˆê¸°í™”
        self._initialize_explainer()
    
    def _initialize_explainer(self):
        """LIME ì„¤ëª…ê¸° ì´ˆê¸°í™”"""
        try:
            if isinstance(self.training_data, pd.DataFrame):
                data_array = self.training_data.values
                if self.feature_names is None:
                    self.feature_names = self.training_data.columns.tolist()
            else:
                data_array = self.training_data
                if self.feature_names is None:
                    self.feature_names = [f"feature_{i}" for i in range(data_array.shape[1])]
            
            # LIME ì„¤ëª…ê¸° ìƒì„±
            self.explainer = LimeTabularExplainer(
                data_array,
                feature_names=self.feature_names,
                class_names=self.class_names,
                mode='regression' if len(self.class_names) == 1 else 'classification'
            )
            
            logger.info("LIME ì„¤ëª…ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"LIME ì„¤ëª…ê¸° ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    
    def explain_prediction(self, data_point: Union[np.ndarray, pd.DataFrame], 
                         model, num_features: int = 10) -> LIMEExplanation:
        """ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…"""
        try:
            if self.explainer is None:
                logger.error("LIME ì„¤ëª…ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return None
            
            # ë°ì´í„° í¬ì¸íŠ¸ ì¤€ë¹„
            if isinstance(data_point, pd.DataFrame):
                data_array = data_point.values
            else:
                data_array = data_point
            
            # LIME ì„¤ëª… ìƒì„±
            explanation = self.explainer.explain_instance(
                data_array.flatten(),
                model.predict,
                num_features=num_features
            )
            
            # ì˜ˆì¸¡ê°’ ê³„ì‚°
            prediction = model.predict(data_point)[0] if hasattr(model, 'predict') else 0.0
            
            # íŠ¹ì„± ê°€ì¤‘ì¹˜ ì¶”ì¶œ
            feature_weights = []
            feature_values = []
            
            for feature_idx, weight in explanation.as_list():
                feature_weights.append(weight)
                feature_values.append(data_array[0, feature_idx])
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë°©ë²•)
            confidence = self._calculate_confidence(explanation)
            
            lime_explanation = LIMEExplanation(
                explanation_id=f"lime_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                feature_names=self.feature_names[:num_features],
                feature_weights=feature_weights,
                feature_values=feature_values,
                prediction=prediction,
                confidence=confidence,
                timestamp=datetime.now(),
                explanation_type='local'
            )
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.explanations_history.append(lime_explanation)
            
            return lime_explanation
            
        except Exception as e:
            logger.error(f"LIME ì˜ˆì¸¡ ì„¤ëª… ì˜¤ë¥˜: {e}")
            return None
    
    def _calculate_confidence(self, explanation) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ ì‹ ë¢°ë„ ê³„ì‚° ë°©ë²•
            # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥
            
            # ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ í•©
            total_weight = sum(abs(weight) for _, weight in explanation.as_list())
            
            # ì •ê·œí™”ëœ ì‹ ë¢°ë„
            confidence = min(1.0, total_weight / 10.0)
            
            return confidence
            
        except Exception as e:
            logger.error(f"ì‹ ë¢°ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.5
    
    def get_feature_contributions(self, explanation: LIMEExplanation) -> List[FeatureContribution]:
        """íŠ¹ì„± ê¸°ì—¬ë„ ê³„ì‚°"""
        try:
            contributions = []
            
            for i, (feature_name, weight, value) in enumerate(zip(
                explanation.feature_names,
                explanation.feature_weights,
                explanation.feature_values
            )):
                # ê¸°ì—¬ë„ ê³„ì‚° (ê°€ì¤‘ì¹˜ * ê°’)
                contribution = weight * value
                
                feature_contribution = FeatureContribution(
                    feature_name=feature_name,
                    weight=weight,
                    value=value,
                    contribution=contribution,
                    rank=i + 1
                )
                
                contributions.append(feature_contribution)
            
            # ê¸°ì—¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            contributions.sort(key=lambda x: abs(x.contribution), reverse=True)
            
            # ìˆœìœ„ ì—…ë°ì´íŠ¸
            for i, contribution in enumerate(contributions):
                contribution.rank = i + 1
            
            return contributions
            
        except Exception as e:
            logger.error(f"íŠ¹ì„± ê¸°ì—¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return []
    
    def generate_visualization(self, explanation: LIMEExplanation) -> str:
        """LIME ì‹œê°í™” ìƒì„±"""
        try:
            plt.figure(figsize=(10, 6))
            
            # íŠ¹ì„± ê¸°ì—¬ë„ ê³„ì‚°
            contributions = self.get_feature_contributions(explanation)
            
            # ìƒìœ„ 10ê°œ íŠ¹ì„±ë§Œ ì„ íƒ
            top_contributions = contributions[:10]
            
            feature_names = [c.feature_name for c in top_contributions]
            weights = [c.weight for c in top_contributions]
            colors = ['red' if w < 0 else 'blue' for w in weights]
            
            # ë°” í”Œë¡¯ ìƒì„±
            bars = plt.barh(range(len(feature_names)), weights, color=colors)
            plt.yticks(range(len(feature_names)), feature_names)
            plt.xlabel('LIME Weight')
            plt.title(f'LIME Feature Weights - Prediction: {explanation.prediction:.4f}')
            plt.gca().invert_yaxis()
            
            # ê°’ í‘œì‹œ
            for i, (bar, contribution) in enumerate(zip(bars, top_contributions)):
                plt.text(bar.get_width() + (0.01 if bar.get_width() > 0 else -0.01),
                        bar.get_y() + bar.get_height()/2,
                        f'{contribution.value:.3f}',
                        va='center', ha='left' if bar.get_width() > 0 else 'right')
            
            plt.tight_layout()
            
            # íŒŒì¼ë¡œ ì €ì¥
            file_path = f"lime_explanation_{explanation.explanation_id}.png"
            plt.savefig(file_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            return file_path
            
        except Exception as e:
            logger.error(f"LIME ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {e}")
            return ""
```

## ğŸ”§ **ê±°ë˜ ê²°ì • ì‹œê°í™” ì‹œìŠ¤í…œ**

### ğŸ“¦ **ê±°ë˜ ê²°ì • ì‹œê°í™”ê¸°**

```python
# explainable-ai/decision-visualization/decision_visualizer.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from datetime import datetime
import logging
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

logger = logging.getLogger(__name__)

@dataclass
class TradingDecision:
    """ê±°ë˜ ê²°ì •"""
    decision_id: str
    symbol: str
    action: str  # 'BUY', 'SELL', 'HOLD'
    confidence: float
    price: float
    quantity: float
    timestamp: datetime
    reasoning: Dict[str, Any]
    risk_score: float

@dataclass
class DecisionExplanation:
    """ê²°ì • ì„¤ëª…"""
    decision_id: str
    technical_factors: Dict[str, float]
    fundamental_factors: Dict[str, float]
    sentiment_factors: Dict[str, float]
    risk_factors: Dict[str, float]
    overall_confidence: float
    recommendation: str

class DecisionVisualizer:
    """ê±°ë˜ ê²°ì • ì‹œê°í™”ê¸°"""
    
    def __init__(self):
        self.decisions_history = []
        self.explanations_history = []
    
    def add_decision(self, decision: TradingDecision, explanation: DecisionExplanation):
        """ê±°ë˜ ê²°ì • ì¶”ê°€"""
        try:
            self.decisions_history.append(decision)
            self.explanations_history.append(explanation)
            
            logger.info(f"ê±°ë˜ ê²°ì • ì¶”ê°€: {decision.symbol} - {decision.action}")
            
        except Exception as e:
            logger.error(f"ê±°ë˜ ê²°ì • ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    def generate_decision_dashboard(self, symbol: str, 
                                  time_range: Tuple[datetime, datetime] = None) -> str:
        """ê±°ë˜ ê²°ì • ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        try:
            # í•´ë‹¹ ì‹¬ë³¼ì˜ ê²°ì • í•„í„°ë§
            symbol_decisions = [d for d in self.decisions_history if d.symbol == symbol]
            
            if not symbol_decisions:
                logger.warning(f"ì‹¬ë³¼ {symbol}ì— ëŒ€í•œ ê±°ë˜ ê²°ì •ì´ ì—†ìŒ")
                return ""
            
            # ì‹œê°„ ë²”ìœ„ í•„í„°ë§
            if time_range:
                start_time, end_time = time_range
                symbol_decisions = [d for d in symbol_decisions 
                                  if start_time <= d.timestamp <= end_time]
            
            if not symbol_decisions:
                logger.warning(f"ì§€ì •ëœ ì‹œê°„ ë²”ìœ„ì— ê±°ë˜ ê²°ì •ì´ ì—†ìŒ")
                return ""
            
            # ëŒ€ì‹œë³´ë“œ ìƒì„±
            fig = make_subplots(
                rows=3, cols=2,
                subplot_titles=('Decision History', 'Confidence Distribution',
                              'Action Distribution', 'Risk Score Trend',
                              'Factor Contributions', 'Performance Metrics'),
                specs=[[{"type": "scatter"}, {"type": "histogram"}],
                       [{"type": "pie"}, {"type": "scatter"}],
                       [{"type": "bar"}, {"type": "scatter"}]]
            )
            
            # 1. ê²°ì • íˆìŠ¤í† ë¦¬
            timestamps = [d.timestamp for d in symbol_decisions]
            prices = [d.price for d in symbol_decisions]
            actions = [d.action for d in symbol_decisions]
            
            colors = {'BUY': 'green', 'SELL': 'red', 'HOLD': 'blue'}
            action_colors = [colors[action] for action in actions]
            
            fig.add_trace(
                go.Scatter(x=timestamps, y=prices, mode='markers',
                          marker=dict(color=action_colors, size=8),
                          name='Decisions'),
                row=1, col=1
            )
            
            # 2. ì‹ ë¢°ë„ ë¶„í¬
            confidences = [d.confidence for d in symbol_decisions]
            fig.add_trace(
                go.Histogram(x=confidences, nbinsx=20, name='Confidence'),
                row=1, col=2
            )
            
            # 3. ì•¡ì…˜ ë¶„í¬
            action_counts = pd.Series(actions).value_counts()
            fig.add_trace(
                go.Pie(labels=action_counts.index, values=action_counts.values,
                      name='Actions'),
                row=2, col=1
            )
            
            # 4. ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ íŠ¸ë Œë“œ
            risk_scores = [d.risk_score for d in symbol_decisions]
            fig.add_trace(
                go.Scatter(x=timestamps, y=risk_scores, mode='lines+markers',
                          name='Risk Score'),
                row=2, col=2
            )
            
            # 5. íŒ©í„° ê¸°ì—¬ë„
            if self.explanations_history:
                latest_explanation = self.explanations_history[-1]
                
                factor_names = list(latest_explanation.technical_factors.keys())
                factor_values = list(latest_explanation.technical_factors.values())
                
                fig.add_trace(
                    go.Bar(x=factor_names, y=factor_values, name='Technical Factors'),
                    row=3, col=1
                )
            
            # 6. ì„±ê³¼ ë©”íŠ¸ë¦­
            # ê°„ë‹¨í•œ ì„±ê³¼ ê³„ì‚°
            buy_decisions = [d for d in symbol_decisions if d.action == 'BUY']
            sell_decisions = [d for d in symbol_decisions if d.action == 'SELL']
            
            if buy_decisions and sell_decisions:
                avg_buy_price = np.mean([d.price for d in buy_decisions])
                avg_sell_price = np.mean([d.price for d in sell_decisions])
                
                fig.add_trace(
                    go.Scatter(x=['Buy', 'Sell'], y=[avg_buy_price, avg_sell_price],
                              mode='markers', marker=dict(size=15),
                              name='Average Prices'),
                    row=3, col=2
                )
            
            # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸
            fig.update_layout(
                title=f'Trading Decision Dashboard - {symbol}',
                height=1200,
                showlegend=True
            )
            
            # íŒŒì¼ë¡œ ì €ì¥
            file_path = f"decision_dashboard_{symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
            fig.write_html(file_path)
            
            return file_path
            
        except Exception as e:
            logger.error(f"ê±°ë˜ ê²°ì • ëŒ€ì‹œë³´ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""
    
    def generate_confidence_analysis(self, symbol: str) -> str:
        """ì‹ ë¢°ë„ ë¶„ì„ ìƒì„±"""
        try:
            # í•´ë‹¹ ì‹¬ë³¼ì˜ ê²°ì • í•„í„°ë§
            symbol_decisions = [d for d in self.decisions_history if d.symbol == symbol]
            
            if not symbol_decisions:
                return ""
            
            # ì‹ ë¢°ë„ ë¶„ì„
            confidences = [d.confidence for d in symbol_decisions]
            actions = [d.action for d in symbol_decisions]
            
            # ì•¡ì…˜ë³„ ì‹ ë¢°ë„ ë¶„ì„
            action_confidence = {}
            for action in set(actions):
                action_confidences = [d.confidence for d in symbol_decisions if d.action == action]
                action_confidence[action] = {
                    'mean': np.mean(action_confidences),
                    'std': np.std(action_confidences),
                    'count': len(action_confidences)
                }
            
            # ì‹œê°í™”
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # 1. ì‹ ë¢°ë„ ë¶„í¬
            axes[0, 0].hist(confidences, bins=20, alpha=0.7, color='skyblue')
            axes[0, 0].set_title('Confidence Distribution')
            axes[0, 0].set_xlabel('Confidence')
            axes[0, 0].set_ylabel('Frequency')
            
            # 2. ì•¡ì…˜ë³„ ì‹ ë¢°ë„ ë°•ìŠ¤í”Œë¡¯
            action_data = []
            action_labels = []
            for action in set(actions):
                action_confidences = [d.confidence for d in symbol_decisions if d.action == action]
                action_data.append(action_confidences)
                action_labels.append(action)
            
            axes[0, 1].boxplot(action_data, labels=action_labels)
            axes[0, 1].set_title('Confidence by Action')
            axes[0, 1].set_ylabel('Confidence')
            
            # 3. ì‹œê°„ì— ë”°ë¥¸ ì‹ ë¢°ë„ ë³€í™”
            timestamps = [d.timestamp for d in symbol_decisions]
            axes[1, 0].scatter(timestamps, confidences, alpha=0.6)
            axes[1, 0].set_title('Confidence Over Time')
            axes[1, 0].set_xlabel('Time')
            axes[1, 0].set_ylabel('Confidence')
            axes[1, 0].tick_params(axis='x', rotation=45)
            
            # 4. ì‹ ë¢°ë„ vs ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´
            risk_scores = [d.risk_score for d in symbol_decisions]
            scatter = axes[1, 1].scatter(confidences, risk_scores, 
                                       c=[hash(a) for a in actions], alpha=0.6)
            axes[1, 1].set_title('Confidence vs Risk Score')
            axes[1, 1].set_xlabel('Confidence')
            axes[1, 1].set_ylabel('Risk Score')
            
            # ë²”ë¡€ ì¶”ê°€
            legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                                        markerfacecolor=plt.cm.Set1(i/len(set(actions))),
                                        label=action, markersize=8)
                              for i, action in enumerate(set(actions))]
            axes[1, 1].legend(handles=legend_elements)
            
            plt.tight_layout()
            
            # íŒŒì¼ë¡œ ì €ì¥
            file_path = f"confidence_analysis_{symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            plt.savefig(file_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            return file_path
            
        except Exception as e:
            logger.error(f"ì‹ ë¢°ë„ ë¶„ì„ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""
    
    def generate_factor_importance_chart(self, explanation: DecisionExplanation) -> str:
        """íŒ©í„° ì¤‘ìš”ë„ ì°¨íŠ¸ ìƒì„±"""
        try:
            # ëª¨ë“  íŒ©í„° í†µí•©
            all_factors = {}
            all_factors.update(explanation.technical_factors)
            all_factors.update(explanation.fundamental_factors)
            all_factors.update(explanation.sentiment_factors)
            all_factors.update(explanation.risk_factors)
            
            # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_factors = sorted(all_factors.items(), key=lambda x: abs(x[1]), reverse=True)
            
            # ìƒìœ„ 15ê°œ íŒ©í„°ë§Œ ì„ íƒ
            top_factors = sorted_factors[:15]
            
            factor_names = [f[0] for f in top_factors]
            factor_values = [f[1] for f in top_factors]
            
            # ìƒ‰ìƒ ê²°ì • (ì–‘ìˆ˜/ìŒìˆ˜)
            colors = ['red' if v < 0 else 'blue' for v in factor_values]
            
            # ì‹œê°í™”
            plt.figure(figsize=(12, 8))
            bars = plt.barh(range(len(factor_names)), factor_values, color=colors, alpha=0.7)
            plt.yticks(range(len(factor_names)), factor_names)
            plt.xlabel('Factor Importance')
            plt.title('Factor Importance Analysis')
            plt.gca().invert_yaxis()
            
            # ê°’ í‘œì‹œ
            for i, (bar, value) in enumerate(zip(bars, factor_values)):
                plt.text(bar.get_width() + (0.01 if value > 0 else -0.01),
                        bar.get_y() + bar.get_height()/2,
                        f'{value:.3f}',
                        va='center', ha='left' if value > 0 else 'right')
            
            plt.tight_layout()
            
            # íŒŒì¼ë¡œ ì €ì¥
            file_path = f"factor_importance_{explanation.decision_id}.png"
            plt.savefig(file_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            return file_path
            
        except Exception as e:
            logger.error(f"íŒ©í„° ì¤‘ìš”ë„ ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""
```

## ğŸ”§ **ê³ ê¸‰ ìœ„í—˜ ê´€ë¦¬ ì‹œìŠ¤í…œ**

### ğŸ“¦ **VaR/CVaR ê³„ì‚°ê¸°**

```python
# explainable-ai/advanced-risk-management/var_cvar_calculator.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import logging
from scipy import stats
from scipy.optimize import minimize

logger = logging.getLogger(__name__)

@dataclass
class RiskMetrics:
    """ìœ„í—˜ ë©”íŠ¸ë¦­"""
    var_95: float
    var_99: float
    cvar_95: float
    cvar_99: float
    portfolio_volatility: float
    sharpe_ratio: float
    max_drawdown: float
    timestamp: datetime

@dataclass
class StressTestResult:
    """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    scenario_name: str
    portfolio_value: float
    var_impact: float
    cvar_impact: float
    volatility_impact: float
    timestamp: datetime

class VaRCVaRCalculator:
    """VaR/CVaR ê³„ì‚°ê¸°"""
    
    def __init__(self, confidence_levels: List[float] = None):
        self.confidence_levels = confidence_levels or [0.95, 0.99]
        self.historical_window = 252  # 1ë…„
    
    def calculate_var_cvar(self, returns: pd.Series, 
                          confidence_levels: List[float] = None) -> Dict[str, float]:
        """VaR/CVaR ê³„ì‚°"""
        try:
            if confidence_levels is None:
                confidence_levels = self.confidence_levels
            
            results = {}
            
            for confidence in confidence_levels:
                # VaR ê³„ì‚° (ì—­ë¶„ìœ„ìˆ˜)
                var = np.percentile(returns, (1 - confidence) * 100)
                
                # CVaR ê³„ì‚° (ì¡°ê±´ë¶€ ê¸°ëŒ€ê°’)
                var_threshold = np.percentile(returns, (1 - confidence) * 100)
                cvar = returns[returns <= var_threshold].mean()
                
                results[f'var_{int(confidence*100)}'] = var
                results[f'cvar_{int(confidence*100)}'] = cvar
            
            return results
            
        except Exception as e:
            logger.error(f"VaR/CVaR ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {}
    
    def calculate_portfolio_risk(self, portfolio_returns: pd.Series, 
                               weights: List[float] = None) -> RiskMetrics:
        """í¬íŠ¸í´ë¦¬ì˜¤ ìœ„í—˜ ê³„ì‚°"""
        try:
            if weights is None:
                weights = [1.0 / len(portfolio_returns)] * len(portfolio_returns)
            
            # ê¸°ë³¸ í†µê³„
            mean_return = portfolio_returns.mean()
            volatility = portfolio_returns.std()
            
            # VaR/CVaR ê³„ì‚°
            risk_measures = self.calculate_var_cvar(portfolio_returns)
            
            # ìƒ¤í”„ ë¹„ìœ¨ ê³„ì‚°
            risk_free_rate = 0.02  # 2% ë¬´ìœ„í—˜ ìˆ˜ìµë¥ 
            sharpe_ratio = (mean_return - risk_free_rate) / volatility if volatility > 0 else 0
            
            # ìµœëŒ€ ë“œë¡œìš°ë‹¤ìš´ ê³„ì‚°
            cumulative_returns = (1 + portfolio_returns).cumprod()
            running_max = cumulative_returns.expanding().max()
            drawdown = (cumulative_returns - running_max) / running_max
            max_drawdown = drawdown.min()
            
            return RiskMetrics(
                var_95=risk_measures.get('var_95', 0.0),
                var_99=risk_measures.get('var_99', 0.0),
                cvar_95=risk_measures.get('cvar_95', 0.0),
                cvar_99=risk_measures.get('cvar_99', 0.0),
                portfolio_volatility=volatility,
                sharpe_ratio=sharpe_ratio,
                max_drawdown=max_drawdown,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"í¬íŠ¸í´ë¦¬ì˜¤ ìœ„í—˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return RiskMetrics(
                var_95=0.0, var_99=0.0, cvar_95=0.0, cvar_99=0.0,
                portfolio_volatility=0.0, sharpe_ratio=0.0, max_drawdown=0.0,
                timestamp=datetime.now()
            )
    
    def stress_test(self, portfolio_returns: pd.Series, 
                   scenarios: Dict[str, Dict[str, float]]) -> List[StressTestResult]:
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        try:
            results = []
            base_risk = self.calculate_portfolio_risk(portfolio_returns)
            
            for scenario_name, scenario_params in scenarios.items():
                # ì‹œë‚˜ë¦¬ì˜¤ ì ìš©
                stressed_returns = self._apply_stress_scenario(portfolio_returns, scenario_params)
                
                # ìŠ¤íŠ¸ë ˆìŠ¤ëœ ìœ„í—˜ ê³„ì‚°
                stressed_risk = self.calculate_portfolio_risk(stressed_returns)
                
                # ì˜í–¥ë„ ê³„ì‚°
                var_impact = stressed_risk.var_95 - base_risk.var_95
                cvar_impact = stressed_risk.cvar_95 - base_risk.cvar_95
                volatility_impact = stressed_risk.portfolio_volatility - base_risk.portfolio_volatility
                
                results.append(StressTestResult(
                    scenario_name=scenario_name,
                    portfolio_value=stressed_returns.mean(),
                    var_impact=var_impact,
                    cvar_impact=cvar_impact,
                    volatility_impact=volatility_impact,
                    timestamp=datetime.now()
                ))
            
            return results
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return []
    
    def _apply_stress_scenario(self, returns: pd.Series, 
                             scenario_params: Dict[str, float]) -> pd.Series:
        """ìŠ¤íŠ¸ë ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ì ìš©"""
        try:
            stressed_returns = returns.copy()
            
            # ë³€ë™ì„± ì¦ê°€
            if 'volatility_multiplier' in scenario_params:
                multiplier = scenario_params['volatility_multiplier']
                stressed_returns = stressed_returns * multiplier
            
            # ìˆ˜ìµë¥  ê°ì†Œ
            if 'return_shock' in scenario_params:
                shock = scenario_params['return_shock']
                stressed_returns = stressed_returns + shock
            
            # ìƒê´€ê´€ê³„ ë³€í™”
            if 'correlation_shock' in scenario_params:
                # ìƒê´€ê´€ê³„ ë³€í™”ëŠ” ë” ë³µì¡í•œ êµ¬í˜„ í•„ìš”
                pass
            
            return stressed_returns
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ì ìš© ì˜¤ë¥˜: {e}")
            return returns
```

### ğŸ“¦ **ì •êµí•œ ë°±í…ŒìŠ¤íŒ… ì—”ì§„**

```python
# explainable-ai/sophisticated-backtesting/backtest_engine.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from enum import Enum
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor

logger = logging.getLogger(__name__)

class BacktestType(Enum):
    """ë°±í…ŒìŠ¤íŠ¸ íƒ€ì…"""
    SIMPLE = "simple"
    WALK_FORWARD = "walk_forward"
    MONTE_CARLO = "monte_carlo"
    STRESS_TEST = "stress_test"

@dataclass
class Trade:
    """ê±°ë˜"""
    symbol: str
    entry_time: datetime
    exit_time: Optional[datetime]
    entry_price: float
    exit_price: Optional[float]
    quantity: float
    side: str  # 'BUY', 'SELL'
    pnl: Optional[float] = None
    commission: float = 0.0

@dataclass
class BacktestResult:
    """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    strategy_name: str
    total_return: float
    sharpe_ratio: float
    max_drawdown: float
    win_rate: float
    profit_factor: float
    total_trades: int
    avg_trade_duration: timedelta
    trades: List[Trade]
    equity_curve: pd.Series
    timestamp: datetime

class SophisticatedBacktestEngine:
    """ì •êµí•œ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„"""
    
    def __init__(self, initial_capital: float = 100000, 
                 commission_rate: float = 0.001):
        self.initial_capital = initial_capital
        self.commission_rate = commission_rate
        self.results_cache = {}
    
    def run_backtest(self, strategy_func: Callable, 
                    data: pd.DataFrame,
                    backtest_type: BacktestType = BacktestType.SIMPLE,
                    **kwargs) -> BacktestResult:
        """ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            if backtest_type == BacktestType.SIMPLE:
                return self._run_simple_backtest(strategy_func, data, **kwargs)
            elif backtest_type == BacktestType.WALK_FORWARD:
                return self._run_walk_forward_backtest(strategy_func, data, **kwargs)
            elif backtest_type == BacktestType.MONTE_CARLO:
                return self._run_monte_carlo_backtest(strategy_func, data, **kwargs)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°±í…ŒìŠ¤íŠ¸ íƒ€ì…: {backtest_type}")
                
        except Exception as e:
            logger.error(f"ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return None
    
    def _run_simple_backtest(self, strategy_func: Callable, 
                           data: pd.DataFrame, **kwargs) -> BacktestResult:
        """ë‹¨ìˆœ ë°±í…ŒìŠ¤íŠ¸"""
        try:
            # ì´ˆê¸° ì„¤ì •
            capital = self.initial_capital
            position = 0
            trades = []
            equity_curve = []
            
            # ì „ëµ ì‹¤í–‰
            for i, (timestamp, row) in enumerate(data.iterrows()):
                # ì „ëµ ì‹ í˜¸ ìƒì„±
                signal = strategy_func(row, i, data, **kwargs)
                
                # ê±°ë˜ ì‹¤í–‰
                if signal == 'BUY' and position <= 0:
                    # ë§¤ìˆ˜
                    quantity = capital / row['close']
                    trade = Trade(
                        symbol=row.get('symbol', 'UNKNOWN'),
                        entry_time=timestamp,
                        entry_price=row['close'],
                        quantity=quantity,
                        side='BUY'
                    )
                    trades.append(trade)
                    position = quantity
                    capital = 0
                    
                elif signal == 'SELL' and position > 0:
                    # ë§¤ë„
                    trade = trades[-1]  # ë§ˆì§€ë§‰ ë§¤ìˆ˜ ê±°ë˜
                    trade.exit_time = timestamp
                    trade.exit_price = row['close']
                    trade.pnl = (trade.exit_price - trade.entry_price) * trade.quantity
                    trade.commission = (trade.entry_price + trade.exit_price) * trade.quantity * self.commission_rate
                    trade.pnl -= trade.commission
                    
                    capital = trade.exit_price * trade.quantity - trade.commission
                    position = 0
                
                # ìë³¸ ê³¡ì„  ì—…ë°ì´íŠ¸
                current_value = capital + (position * row['close'] if position > 0 else 0)
                equity_curve.append(current_value)
            
            # ê²°ê³¼ ê³„ì‚°
            return self._calculate_backtest_results(trades, equity_curve, data.index)
            
        except Exception as e:
            logger.error(f"ë‹¨ìˆœ ë°±í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return None
    
    def _run_walk_forward_backtest(self, strategy_func: Callable, 
                                 data: pd.DataFrame, 
                                 window_size: int = 252,
                                 step_size: int = 63) -> BacktestResult:
        """ì›Œí¬í¬ì›Œë“œ ë°±í…ŒìŠ¤íŠ¸"""
        try:
            all_trades = []
            all_equity_curves = []
            
            for start_idx in range(0, len(data) - window_size, step_size):
                end_idx = start_idx + window_size
                window_data = data.iloc[start_idx:end_idx]
                
                # ìœˆë„ìš°ë³„ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                window_result = self._run_simple_backtest(strategy_func, window_data)
                
                if window_result:
                    all_trades.extend(window_result.trades)
                    all_equity_curves.extend(window_result.equity_curve.tolist())
            
            # ì „ì²´ ê²°ê³¼ ê³„ì‚°
            return self._calculate_backtest_results(all_trades, all_equity_curves, data.index)
            
        except Exception as e:
            logger.error(f"ì›Œí¬í¬ì›Œë“œ ë°±í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return None
    
    def _run_monte_carlo_backtest(self, strategy_func: Callable, 
                                data: pd.DataFrame, 
                                n_simulations: int = 1000) -> BacktestResult:
        """ëª¬í…Œì¹´ë¥¼ë¡œ ë°±í…ŒìŠ¤íŠ¸"""
        try:
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜
            def run_single_simulation(sim_id):
                # ë°ì´í„° ìˆœì„œë¥¼ ëœë¤í•˜ê²Œ ì„ê¸°
                shuffled_data = data.sample(frac=1.0, random_state=sim_id).reset_index()
                return self._run_simple_backtest(strategy_func, shuffled_data)
            
            # ë³‘ë ¬ ì‹¤í–‰
            with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:
                futures = [executor.submit(run_single_simulation, i) 
                          for i in range(n_simulations)]
                results = [future.result() for future in futures if future.result()]
            
            # ê²°ê³¼ ì§‘ê³„
            if results:
                # í‰ê·  ê²°ê³¼ ê³„ì‚°
                avg_return = np.mean([r.total_return for r in results])
                avg_sharpe = np.mean([r.sharpe_ratio for r in results])
                avg_drawdown = np.mean([r.max_drawdown for r in results])
                
                # ëª¨ë“  ê±°ë˜ í†µí•©
                all_trades = []
                for result in results:
                    all_trades.extend(result.trades)
                
                return BacktestResult(
                    strategy_name=f"Monte_Carlo_{n_simulations}",
                    total_return=avg_return,
                    sharpe_ratio=avg_sharpe,
                    max_drawdown=avg_drawdown,
                    win_rate=np.mean([r.win_rate for r in results]),
                    profit_factor=np.mean([r.profit_factor for r in results]),
                    total_trades=len(all_trades),
                    avg_trade_duration=timedelta(days=1),  # í‰ê· ê°’ ê³„ì‚° í•„ìš”
                    trades=all_trades,
                    equity_curve=pd.Series([avg_return]),
                    timestamp=datetime.now()
                )
            
            return None
            
        except Exception as e:
            logger.error(f"ëª¬í…Œì¹´ë¥¼ë¡œ ë°±í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return None
    
    def _calculate_backtest_results(self, trades: List[Trade], 
                                  equity_curve: List[float], 
                                  timestamps: pd.DatetimeIndex) -> BacktestResult:
        """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê³„ì‚°"""
        try:
            if not trades:
                return BacktestResult(
                    strategy_name="Empty_Strategy",
                    total_return=0.0,
                    sharpe_ratio=0.0,
                    max_drawdown=0.0,
                    win_rate=0.0,
                    profit_factor=0.0,
                    total_trades=0,
                    avg_trade_duration=timedelta(0),
                    trades=[],
                    equity_curve=pd.Series(),
                    timestamp=datetime.now()
                )
            
            # ê¸°ë³¸ í†µê³„
            total_trades = len([t for t in trades if t.pnl is not None])
            winning_trades = [t for t in trades if t.pnl and t.pnl > 0]
            losing_trades = [t for t in trades if t.pnl and t.pnl < 0]
            
            # ìˆ˜ìµë¥  ê³„ì‚°
            total_pnl = sum(t.pnl for t in trades if t.pnl)
            total_return = total_pnl / self.initial_capital
            
            # ìŠ¹ë¥ 
            win_rate = len(winning_trades) / total_trades if total_trades > 0 else 0
            
            # ìˆ˜ìµ íŒ©í„°
            gross_profit = sum(t.pnl for t in winning_trades)
            gross_loss = abs(sum(t.pnl for t in losing_trades))
            profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
            
            # ìƒ¤í”„ ë¹„ìœ¨
            returns = pd.Series(equity_curve).pct_change().dropna()
            sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252) if returns.std() > 0 else 0
            
            # ìµœëŒ€ ë“œë¡œìš°ë‹¤ìš´
            equity_series = pd.Series(equity_curve, index=timestamps[:len(equity_curve)])
            cumulative_returns = (1 + equity_series.pct_change()).cumprod()
            running_max = cumulative_returns.expanding().max()
            drawdown = (cumulative_returns - running_max) / running_max
            max_drawdown = drawdown.min()
            
            # í‰ê·  ê±°ë˜ ê¸°ê°„
            durations = [t.exit_time - t.entry_time for t in trades if t.exit_time]
            avg_duration = np.mean(durations) if durations else timedelta(0)
            
            return BacktestResult(
                strategy_name="Backtest_Strategy",
                total_return=total_return,
                sharpe_ratio=sharpe_ratio,
                max_drawdown=max_drawdown,
                win_rate=win_rate,
                profit_factor=profit_factor,
                total_trades=total_trades,
                avg_trade_duration=avg_duration,
                trades=trades,
                equity_curve=equity_series,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return None
```

## ğŸ“Š **ì„±ê³¼ ì§€í‘œ**

### **ëª©í‘œ ì„±ê³¼**
- **SHAP ë¶„ì„ ì •í™•ë„**: > 90%
- **LIME ì„¤ëª… ì •í™•ë„**: > 85%
- **ì‹œê°í™” ìƒì„± ì†ë„**: < 2ì´ˆ
- **ì‹¤ì‹œê°„ ì„¤ëª…**: < 100ms
- **ìœ„í—˜ ê³„ì‚° ì •í™•ë„**: > 95%
- **ë°±í…ŒìŠ¤íŒ… ì •í™•ë„**: > 90%
- **ì‚¬ìš©ì ì´í•´ë„**: > 80%

### **ì„±ëŠ¥ ì§€í‘œ**
- **SHAP ë¶„ì„ ì‹œê°„**: < 10ì´ˆ
- **LIME ì„¤ëª… ìƒì„±**: < 1ì´ˆ
- **ì‹œê°í™” ìƒì„±**: < 2ì´ˆ
- **ìœ„í—˜ ê³„ì‚°**: < 1ì´ˆ VaR/CVaR ê³„ì‚°
- **ë°±í…ŒìŠ¤íŒ…**: < 5ë¶„ ì „ì²´ ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- **í˜ì´ì§€ ë¡œë”©**: < 2ì´ˆ ëŒ€ì‹œë³´ë“œ ë¡œë”©
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: < 1GB
- **ì‹œìŠ¤í…œ ê°€ë™ë¥ **: > 99.5%

## ğŸ”— **ê´€ë ¨ ë¬¸ì„œ**

- [Phase 3.5.1: ê¸°ìˆ ì  ì§€í‘œ ë¶„ì„](3.5.1_TECHNICAL_ANALYSIS.md)
- [Phase 3.5.2: ê±°ë˜ ì „ëµ ë¼ì´ë¸ŒëŸ¬ë¦¬](3.5.2_TRADING_STRATEGIES.md)
- [Phase 3.5.3: ë‰´ìŠ¤ ì´ë²¤íŠ¸ ë¶„ì„](3.5.3_NEWS_EVENT_ANALYSIS.md)
- [Phase 3.5.4: ì˜¨ë¼ì¸ í•™ìŠµ](3.5.4_ONLINE_LEARNING.md)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-01-26  
**í”„ë¡œì íŠ¸ ìƒíƒœ**: ì„¤ê³„ ì™„ë£Œ, ê°œë°œ ì¤€ë¹„  
**ë‹¤ìŒ ë‹¨ê³„**: ì „ì²´ Phase 3.5 ì‹œìŠ¤í…œ í†µí•© ë° ìµœì í™” 