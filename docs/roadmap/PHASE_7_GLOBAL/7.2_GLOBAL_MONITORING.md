# ğŸŒ Phase 7.2: ê¸€ë¡œë²Œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

## ğŸ“‹ **ê°œìš”**

### ğŸ¯ **ëª©í‘œ**
- **ë¶„ì‚° ëª¨ë‹ˆí„°ë§**: ë‹¤ì¤‘ ë¦¬ì „ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼
- **ì„±ëŠ¥ ë¶„ì„**: ê¸€ë¡œë²Œ ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ ë° ë¶„ì„
- **ì¥ì•  ê°ì§€**: ì‹¤ì‹œê°„ ì¥ì•  ê°ì§€ ë° ìë™ ë³µêµ¬
- **ìš©ëŸ‰ ê³„íš**: ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ë° ê³„íš
- **ë³´ì•ˆ ëª¨ë‹ˆí„°ë§**: ê¸€ë¡œë²Œ ë³´ì•ˆ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§

### ğŸ“Š **ì„±ëŠ¥ ëª©í‘œ**
- **ëª¨ë‹ˆí„°ë§ ì§€ì—°**: < 5ì´ˆ ê¸€ë¡œë²Œ ì´ë²¤íŠ¸ ê°ì§€
- **ì•Œë¦¼ ì „ì†¡**: < 10ì´ˆ ì¤‘ìš” ì•Œë¦¼ ì „ì†¡
- **ë°ì´í„° ìˆ˜ì§‘**: < 1ì´ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- **ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸**: < 30ì´ˆ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
- **ì¥ì•  ë³µêµ¬**: < 5ë¶„ ìë™ ë³µêµ¬

## ğŸ—ï¸ **ê¸€ë¡œë²Œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

### ğŸ“ **ê¸€ë¡œë²Œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¡°**
```
global-monitoring/
â”œâ”€â”€ distributed-monitoring/           # ë¶„ì‚° ëª¨ë‹ˆí„°ë§
â”‚   â”œâ”€â”€ regional-agents/              # ì§€ì—­ë³„ ì—ì´ì „íŠ¸
â”‚   â”œâ”€â”€ central-collector/            # ì¤‘ì•™ ìˆ˜ì§‘ê¸°
â”‚   â”œâ”€â”€ data-aggregation/             # ë°ì´í„° ì§‘ê³„
â”‚   â””â”€â”€ cross-region-sync/            # ë¦¬ì „ ê°„ ë™ê¸°í™”
â”œâ”€â”€ performance-analysis/             # ì„±ëŠ¥ ë¶„ì„
â”‚   â”œâ”€â”€ metrics-collection/           # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
â”‚   â”œâ”€â”€ performance-dashboard/        # ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ
â”‚   â”œâ”€â”€ trend-analysis/               # íŠ¸ë Œë“œ ë¶„ì„
â”‚   â””â”€â”€ capacity-planning/            # ìš©ëŸ‰ ê³„íš
â”œâ”€â”€ incident-management/              # ì¥ì•  ê´€ë¦¬
â”‚   â”œâ”€â”€ incident-detection/           # ì¥ì•  ê°ì§€
â”‚   â”œâ”€â”€ alert-management/             # ì•Œë¦¼ ê´€ë¦¬
â”‚   â”œâ”€â”€ auto-recovery/                # ìë™ ë³µêµ¬
â”‚   â””â”€â”€ escalation-procedures/        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì ˆì°¨
â””â”€â”€ security-monitoring/              # ë³´ì•ˆ ëª¨ë‹ˆí„°ë§
    â”œâ”€â”€ threat-detection/             # ìœ„í˜‘ ê°ì§€
    â”œâ”€â”€ security-events/              # ë³´ì•ˆ ì´ë²¤íŠ¸
    â”œâ”€â”€ compliance-monitoring/        # ê·œì • ì¤€ìˆ˜ ëª¨ë‹ˆí„°ë§
    â””â”€â”€ audit-trails/                 # ê°ì‚¬ ì¶”ì 
```

## ğŸ”§ **ë¶„ì‚° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ**

### ğŸ“¦ **ì§€ì—­ë³„ ì—ì´ì „íŠ¸ ë° ì¤‘ì•™ ìˆ˜ì§‘ê¸°**

```python
# global-monitoring/distributed-monitoring/global_monitoring_manager.py
import asyncio
import time
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import threading
from collections import defaultdict, deque
import hashlib
import aiohttp
import websockets
import prometheus_client
from prometheus_client import Counter, Gauge, Histogram, Summary

logger = logging.getLogger(__name__)

@dataclass
class MonitoringRegion:
    """ëª¨ë‹ˆí„°ë§ ë¦¬ì „"""
    region_id: str
    name: str
    status: str  # 'active', 'maintenance', 'failed'
    agent_count: int
    service_count: int
    last_heartbeat: datetime
    performance_score: float
    created_at: datetime

@dataclass
class MonitoringAgent:
    """ëª¨ë‹ˆí„°ë§ ì—ì´ì „íŠ¸"""
    agent_id: str
    region_id: str
    hostname: str
    ip_address: str
    agent_type: str  # 'system', 'application', 'database', 'network'
    status: str  # 'online', 'offline', 'error'
    last_heartbeat: datetime
    metrics_count: int
    alerts_count: int

@dataclass
class MonitoringMetric:
    """ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­"""
    metric_id: str
    agent_id: str
    region_id: str
    metric_name: str
    metric_value: float
    metric_unit: str
    metric_type: str  # 'counter', 'gauge', 'histogram', 'summary'
    timestamp: datetime
    labels: Dict[str, str]

@dataclass
class MonitoringAlert:
    """ëª¨ë‹ˆí„°ë§ ì•Œë¦¼"""
    alert_id: str
    region_id: str
    agent_id: str
    alert_type: str  # 'critical', 'warning', 'info'
    alert_message: str
    metric_name: str
    metric_value: float
    threshold: float
    status: str  # 'active', 'acknowledged', 'resolved'
    created_at: datetime
    resolved_at: Optional[datetime]

class GlobalMonitoringManager:
    """ê¸€ë¡œë²Œ ëª¨ë‹ˆí„°ë§ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.monitoring_regions = self._initialize_monitoring_regions()
        self.monitoring_agents = {}
        self.monitoring_metrics = {}
        self.monitoring_alerts = {}
        self.performance_metrics = MonitoringMetrics()
        
        # ìŠ¤ë ˆë“œ ì•ˆì „
        self.lock = threading.Lock()
        
        # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        self.monitoring_thread = None
        self.monitoring_active = False
        
        # Prometheus ë©”íŠ¸ë¦­
        self.prometheus_metrics = self._initialize_prometheus_metrics()
        
        logger.info("Global monitoring manager initialized")
    
    def _initialize_monitoring_regions(self) -> Dict[str, MonitoringRegion]:
        """ëª¨ë‹ˆí„°ë§ ë¦¬ì „ ì´ˆê¸°í™”"""
        regions = {
            'us-east-1': MonitoringRegion(
                region_id='us-east-1',
                name='US East Monitoring Region',
                status='active',
                agent_count=0,
                service_count=0,
                last_heartbeat=datetime.now(),
                performance_score=1.0,
                created_at=datetime.now()
            ),
            'us-west-2': MonitoringRegion(
                region_id='us-west-2',
                name='US West Monitoring Region',
                status='active',
                agent_count=0,
                service_count=0,
                last_heartbeat=datetime.now(),
                performance_score=1.0,
                created_at=datetime.now()
            ),
            'eu-west-1': MonitoringRegion(
                region_id='eu-west-1',
                name='Europe Monitoring Region',
                status='active',
                agent_count=0,
                service_count=0,
                last_heartbeat=datetime.now(),
                performance_score=1.0,
                created_at=datetime.now()
            ),
            'ap-northeast-1': MonitoringRegion(
                region_id='ap-northeast-1',
                name='Asia Pacific Monitoring Region',
                status='active',
                agent_count=0,
                service_count=0,
                last_heartbeat=datetime.now(),
                performance_score=1.0,
                created_at=datetime.now()
            )
        }
        
        return regions
    
    def _initialize_prometheus_metrics(self) -> Dict[str, Any]:
        """Prometheus ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
        metrics = {
            'total_agents': Gauge('total_agents', 'Total number of monitoring agents'),
            'active_alerts': Gauge('active_alerts', 'Number of active alerts'),
            'metrics_collected': Counter('metrics_collected', 'Total metrics collected'),
            'alert_response_time': Histogram('alert_response_time', 'Alert response time in seconds'),
            'region_health_score': Gauge('region_health_score', 'Region health score', ['region_id']),
            'agent_uptime': Gauge('agent_uptime', 'Agent uptime percentage', ['agent_id', 'region_id'])
        }
        
        return metrics
    
    async def start_monitoring_manager(self):
        """ëª¨ë‹ˆí„°ë§ ê´€ë¦¬ì ì‹œì‘"""
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
        self.monitoring_thread.start()
        logger.info("Global monitoring manager started")
    
    async def stop_monitoring_manager(self):
        """ëª¨ë‹ˆí„°ë§ ê´€ë¦¬ì ì¤‘ì§€"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
        logger.info("Global monitoring manager stopped")
    
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring_active:
            try:
                # ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸
                self._check_agent_health()
                
                # ë¦¬ì „ ìƒíƒœ í™•ì¸
                self._check_region_health()
                
                # ì•Œë¦¼ ì²˜ë¦¬
                self._process_alerts()
                
                # ì„±ëŠ¥ ì¸¡ì •
                self.performance_metrics.record_monitoring_cycle()
                
                time.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì‹¤í–‰
                
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}")
                time.sleep(10)
    
    def _check_agent_health(self):
        """ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸"""
        current_time = datetime.now()
        
        for agent_id, agent in self.monitoring_agents.items():
            # í•˜íŠ¸ë¹„íŠ¸ í™•ì¸
            if (current_time - agent.last_heartbeat).total_seconds() > 60:
                agent.status = 'offline'
                self._create_alert(
                    agent.region_id, agent_id, 'warning',
                    f"Agent {agent_id} is offline",
                    'agent_uptime', 0.0, 0.5
                )
            else:
                agent.status = 'online'
                
                # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                self.prometheus_metrics['agent_uptime'].labels(
                    agent_id=agent_id, region_id=agent.region_id
                ).set(1.0)
    
    def _check_region_health(self):
        """ë¦¬ì „ ìƒíƒœ í™•ì¸"""
        for region_id, region in self.monitoring_regions.items():
            # ë¦¬ì „ ë‚´ ì—ì´ì „íŠ¸ ìˆ˜ ê³„ì‚°
            region_agents = [
                agent for agent in self.monitoring_agents.values()
                if agent.region_id == region_id
            ]
            
            region.agent_count = len(region_agents)
            region.service_count = len(set(agent.agent_type for agent in region_agents))
            
            # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
            online_agents = [agent for agent in region_agents if agent.status == 'online']
            if region_agents:
                region.performance_score = len(online_agents) / len(region_agents)
            else:
                region.performance_score = 0.0
            
            # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.prometheus_metrics['region_health_score'].labels(
                region_id=region_id
            ).set(region.performance_score)
            
            # ë¦¬ì „ ìƒíƒœ ì—…ë°ì´íŠ¸
            if region.performance_score < 0.5:
                region.status = 'failed'
            elif region.performance_score < 0.8:
                region.status = 'maintenance'
            else:
                region.status = 'active'
    
    def _process_alerts(self):
        """ì•Œë¦¼ ì²˜ë¦¬"""
        active_alerts = [
            alert for alert in self.monitoring_alerts.values()
            if alert.status == 'active'
        ]
        
        # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.prometheus_metrics['active_alerts'].set(len(active_alerts))
        
        for alert in active_alerts:
            # ì•Œë¦¼ ì—ìŠ¤ì»¬ë ˆì´ì…˜ í™•ì¸
            if (datetime.now() - alert.created_at).total_seconds() > 300:  # 5ë¶„
                self._escalate_alert(alert)
    
    def _escalate_alert(self, alert: MonitoringAlert):
        """ì•Œë¦¼ ì—ìŠ¤ì»¬ë ˆì´ì…˜"""
        logger.warning(f"Alert escalated: {alert.alert_id} - {alert.alert_message}")
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê´€ë¦¬ìì—ê²Œ ì•Œë¦¼ ë°œì†¡
    
    async def register_agent(self, region_id: str, hostname: str, 
                           ip_address: str, agent_type: str) -> str:
        """ì—ì´ì „íŠ¸ ë“±ë¡"""
        agent_id = f"agent_{region_id}_{hostname}_{int(time.time())}"
        
        agent = MonitoringAgent(
            agent_id=agent_id,
            region_id=region_id,
            hostname=hostname,
            ip_address=ip_address,
            agent_type=agent_type,
            status='online',
            last_heartbeat=datetime.now(),
            metrics_count=0,
            alerts_count=0
        )
        
        with self.lock:
            self.monitoring_agents[agent_id] = agent
        
        # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.prometheus_metrics['total_agents'].inc()
        
        logger.info(f"Agent registered: {agent_id}")
        return agent_id
    
    async def unregister_agent(self, agent_id: str):
        """ì—ì´ì „íŠ¸ ë“±ë¡ í•´ì œ"""
        with self.lock:
            if agent_id in self.monitoring_agents:
                del self.monitoring_agents[agent_id]
        
        # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.prometheus_metrics['total_agents'].dec()
        
        logger.info(f"Agent unregistered: {agent_id}")
    
    async def send_heartbeat(self, agent_id: str):
        """í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡"""
        with self.lock:
            if agent_id in self.monitoring_agents:
                agent = self.monitoring_agents[agent_id]
                agent.last_heartbeat = datetime.now()
                agent.status = 'online'
    
    async def collect_metric(self, agent_id: str, metric_name: str, 
                           metric_value: float, metric_unit: str = '', 
                           metric_type: str = 'gauge', labels: Dict[str, str] = None) -> str:
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        metric_id = f"metric_{agent_id}_{metric_name}_{int(time.time() * 1000)}"
        
        agent = self.monitoring_agents.get(agent_id)
        if not agent:
            raise Exception(f"Agent not found: {agent_id}")
        
        metric = MonitoringMetric(
            metric_id=metric_id,
            agent_id=agent_id,
            region_id=agent.region_id,
            metric_name=metric_name,
            metric_value=metric_value,
            metric_unit=metric_unit,
            metric_type=metric_type,
            timestamp=datetime.now(),
            labels=labels or {}
        )
        
        with self.lock:
            self.monitoring_metrics[metric_id] = metric
            agent.metrics_count += 1
        
        # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.prometheus_metrics['metrics_collected'].inc()
        
        # ì„ê³„ê°’ í™•ì¸ ë° ì•Œë¦¼ ìƒì„±
        await self._check_thresholds(metric)
        
        logger.debug(f"Metric collected: {metric_id} = {metric_value}")
        return metric_id
    
    async def _check_thresholds(self, metric: MonitoringMetric):
        """ì„ê³„ê°’ í™•ì¸"""
        # ì„ê³„ê°’ ì •ì˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì„¤ì •ì—ì„œ ë¡œë“œ)
        thresholds = {
            'cpu_usage': 80.0,
            'memory_usage': 85.0,
            'disk_usage': 90.0,
            'response_time': 1000.0,  # ms
            'error_rate': 5.0  # %
        }
        
        threshold = thresholds.get(metric.metric_name)
        if threshold and metric.metric_value > threshold:
            await self._create_alert(
                metric.region_id, metric.agent_id, 'warning',
                f"{metric.metric_name} exceeded threshold: {metric.metric_value} > {threshold}",
                metric.metric_name, metric.metric_value, threshold
            )
    
    async def _create_alert(self, region_id: str, agent_id: str, alert_type: str,
                          alert_message: str, metric_name: str, metric_value: float, 
                          threshold: float) -> str:
        """ì•Œë¦¼ ìƒì„±"""
        alert_id = f"alert_{region_id}_{agent_id}_{int(time.time() * 1000)}"
        
        alert = MonitoringAlert(
            alert_id=alert_id,
            region_id=region_id,
            agent_id=agent_id,
            alert_type=alert_type,
            alert_message=alert_message,
            metric_name=metric_name,
            metric_value=metric_value,
            threshold=threshold,
            status='active',
            created_at=datetime.now(),
            resolved_at=None
        )
        
        with self.lock:
            self.monitoring_alerts[alert_id] = alert
        
        # ì—ì´ì „íŠ¸ ì•Œë¦¼ ìˆ˜ ì¦ê°€
        if agent_id in self.monitoring_agents:
            self.monitoring_agents[agent_id].alerts_count += 1
        
        logger.warning(f"Alert created: {alert_id} - {alert_message}")
        return alert_id
    
    async def resolve_alert(self, alert_id: str):
        """ì•Œë¦¼ í•´ê²°"""
        with self.lock:
            if alert_id in self.monitoring_alerts:
                alert = self.monitoring_alerts[alert_id]
                alert.status = 'resolved'
                alert.resolved_at = datetime.now()
                
                logger.info(f"Alert resolved: {alert_id}")
    
    def get_monitoring_region(self, region_id: str) -> Optional[MonitoringRegion]:
        """ëª¨ë‹ˆí„°ë§ ë¦¬ì „ ì¡°íšŒ"""
        return self.monitoring_regions.get(region_id)
    
    def get_all_monitoring_regions(self) -> List[MonitoringRegion]:
        """ëª¨ë“  ëª¨ë‹ˆí„°ë§ ë¦¬ì „ ì¡°íšŒ"""
        return list(self.monitoring_regions.values())
    
    def get_monitoring_agent(self, agent_id: str) -> Optional[MonitoringAgent]:
        """ëª¨ë‹ˆí„°ë§ ì—ì´ì „íŠ¸ ì¡°íšŒ"""
        return self.monitoring_agents.get(agent_id)
    
    def get_agents_by_region(self, region_id: str) -> List[MonitoringAgent]:
        """ë¦¬ì „ë³„ ì—ì´ì „íŠ¸ ì¡°íšŒ"""
        return [
            agent for agent in self.monitoring_agents.values()
            if agent.region_id == region_id
        ]
    
    def get_monitoring_metric(self, metric_id: str) -> Optional[MonitoringMetric]:
        """ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return self.monitoring_metrics.get(metric_id)
    
    def get_metrics_by_agent(self, agent_id: str, 
                           time_range: Optional[Tuple[datetime, datetime]] = None) -> List[MonitoringMetric]:
        """ì—ì´ì „íŠ¸ë³„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        metrics = [
            metric for metric in self.monitoring_metrics.values()
            if metric.agent_id == agent_id
        ]
        
        if time_range:
            start_time, end_time = time_range
            metrics = [
                metric for metric in metrics
                if start_time <= metric.timestamp <= end_time
            ]
        
        return sorted(metrics, key=lambda x: x.timestamp)
    
    def get_monitoring_alert(self, alert_id: str) -> Optional[MonitoringAlert]:
        """ëª¨ë‹ˆí„°ë§ ì•Œë¦¼ ì¡°íšŒ"""
        return self.monitoring_alerts.get(alert_id)
    
    def get_active_alerts(self) -> List[MonitoringAlert]:
        """í™œì„± ì•Œë¦¼ ì¡°íšŒ"""
        return [
            alert for alert in self.monitoring_alerts.values()
            if alert.status == 'active'
        ]
    
    def get_alerts_by_region(self, region_id: str) -> List[MonitoringAlert]:
        """ë¦¬ì „ë³„ ì•Œë¦¼ ì¡°íšŒ"""
        return [
            alert for alert in self.monitoring_alerts.values()
            if alert.region_id == region_id
        ]

class MonitoringMetrics:
    """ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­"""
    
    def __init__(self):
        self.monitoring_cycles = 0
        self.metrics_processed = 0
        self.alerts_processed = 0
        self.start_time = time.time()
        self.lock = threading.Lock()
    
    def record_monitoring_cycle(self):
        """ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ê¸°ë¡"""
        with self.lock:
            self.monitoring_cycles += 1
    
    def record_metrics_processed(self, count: int = 1):
        """ì²˜ë¦¬ëœ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        with self.lock:
            self.metrics_processed += count
    
    def record_alerts_processed(self, count: int = 1):
        """ì²˜ë¦¬ëœ ì•Œë¦¼ ê¸°ë¡"""
        with self.lock:
            self.alerts_processed += count
    
    def get_metrics(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        with self.lock:
            uptime = time.time() - self.start_time
            return {
                'monitoring_cycles': self.monitoring_cycles,
                'metrics_processed': self.metrics_processed,
                'alerts_processed': self.alerts_processed,
                'cycles_per_second': self.monitoring_cycles / uptime if uptime > 0 else 0,
                'metrics_per_second': self.metrics_processed / uptime if uptime > 0 else 0,
                'uptime_seconds': uptime
            }
```

## ğŸ”§ **ì„±ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ**

### ğŸ“¦ **ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ**

```python
# global-monitoring/performance-analysis/performance_analyzer.py
import asyncio
import time
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import threading
from collections import defaultdict, deque
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetric:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    metric_id: str
    region_id: str
    service_name: str
    metric_type: str  # 'response_time', 'throughput', 'error_rate', 'availability'
    value: float
    unit: str
    timestamp: datetime
    percentile_95: Optional[float] = None
    percentile_99: Optional[float] = None

@dataclass
class PerformanceTrend:
    """ì„±ëŠ¥ íŠ¸ë Œë“œ"""
    trend_id: str
    region_id: str
    service_name: str
    metric_type: str
    trend_direction: str  # 'improving', 'stable', 'degrading'
    trend_value: float
    confidence: float
    start_time: datetime
    end_time: datetime

@dataclass
class CapacityForecast:
    """ìš©ëŸ‰ ì˜ˆì¸¡"""
    forecast_id: str
    region_id: str
    resource_type: str  # 'cpu', 'memory', 'storage', 'network'
    current_usage: float
    predicted_usage: float
    prediction_date: datetime
    confidence_interval: Tuple[float, float]

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.performance_metrics = {}
        self.performance_trends = {}
        self.capacity_forecasts = {}
        self.analysis_metrics = AnalysisMetrics()
        
        # ìŠ¤ë ˆë“œ ì•ˆì „
        self.lock = threading.Lock()
        
        # ë¶„ì„ ìŠ¤ë ˆë“œ
        self.analysis_thread = None
        self.analysis_active = False
        
        logger.info("Performance analyzer initialized")
    
    async def start_performance_analyzer(self):
        """ì„±ëŠ¥ ë¶„ì„ê¸° ì‹œì‘"""
        self.analysis_active = True
        self.analysis_thread = threading.Thread(target=self._analysis_loop)
        self.analysis_thread.start()
        logger.info("Performance analyzer started")
    
    async def stop_performance_analyzer(self):
        """ì„±ëŠ¥ ë¶„ì„ê¸° ì¤‘ì§€"""
        self.analysis_active = False
        if self.analysis_thread:
            self.analysis_thread.join()
        logger.info("Performance analyzer stopped")
    
    def _analysis_loop(self):
        """ë¶„ì„ ë£¨í”„"""
        while self.analysis_active:
            try:
                # ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
                self._analyze_performance_trends()
                
                # ìš©ëŸ‰ ì˜ˆì¸¡
                self._generate_capacity_forecasts()
                
                # ì„±ëŠ¥ ì¸¡ì •
                self.analysis_metrics.record_analysis_cycle()
                
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì‹¤í–‰
                
            except Exception as e:
                logger.error(f"Error in analysis loop: {e}")
                time.sleep(300)  # 5ë¶„ ëŒ€ê¸°
    
    async def add_performance_metric(self, region_id: str, service_name: str,
                                   metric_type: str, value: float, unit: str) -> str:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€"""
        metric_id = f"perf_{region_id}_{service_name}_{metric_type}_{int(time.time() * 1000)}"
        
        metric = PerformanceMetric(
            metric_id=metric_id,
            region_id=region_id,
            service_name=service_name,
            metric_type=metric_type,
            value=value,
            unit=unit,
            timestamp=datetime.now()
        )
        
        with self.lock:
            self.performance_metrics[metric_id] = metric
        
        logger.debug(f"Performance metric added: {metric_id}")
        return metric_id
    
    def _analyze_performance_trends(self):
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
        # ìµœê·¼ 24ì‹œê°„ ë°ì´í„° ë¶„ì„
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=24)
        
        # ë¦¬ì „ë³„, ì„œë¹„ìŠ¤ë³„, ë©”íŠ¸ë¦­ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        grouped_metrics = self._group_metrics_by_dimensions(start_time, end_time)
        
        for (region_id, service_name, metric_type), metrics in grouped_metrics.items():
            if len(metrics) < 10:  # ìµœì†Œ 10ê°œ ë°ì´í„° í¬ì¸íŠ¸ í•„ìš”
                continue
            
            trend = self._calculate_trend(metrics)
            if trend:
                trend_id = f"trend_{region_id}_{service_name}_{metric_type}_{int(time.time())}"
                
                performance_trend = PerformanceTrend(
                    trend_id=trend_id,
                    region_id=region_id,
                    service_name=service_name,
                    metric_type=metric_type,
                    trend_direction=trend['direction'],
                    trend_value=trend['value'],
                    confidence=trend['confidence'],
                    start_time=start_time,
                    end_time=end_time
                )
                
                with self.lock:
                    self.performance_trends[trend_id] = performance_trend
                
                logger.info(f"Performance trend calculated: {trend_id} - {trend['direction']}")
    
    def _group_metrics_by_dimensions(self, start_time: datetime, 
                                   end_time: datetime) -> Dict[Tuple[str, str, str], List[PerformanceMetric]]:
        """ë©”íŠ¸ë¦­ì„ ì°¨ì›ë³„ë¡œ ê·¸ë£¹í™”"""
        grouped = defaultdict(list)
        
        for metric in self.performance_metrics.values():
            if start_time <= metric.timestamp <= end_time:
                key = (metric.region_id, metric.service_name, metric.metric_type)
                grouped[key].append(metric)
        
        return dict(grouped)
    
    def _calculate_trend(self, metrics: List[PerformanceMetric]) -> Optional[Dict[str, Any]]:
        """íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(metrics) < 2:
            return None
        
        # ì‹œê°„ìˆœ ì •ë ¬
        sorted_metrics = sorted(metrics, key=lambda x: x.timestamp)
        
        # ê°’ ì¶”ì¶œ
        values = [metric.value for metric in sorted_metrics]
        timestamps = [metric.timestamp.timestamp() for metric in sorted_metrics]
        
        # ì„ í˜• íšŒê·€
        slope, intercept, r_value, p_value, std_err = stats.linregress(timestamps, values)
        
        # íŠ¸ë Œë“œ ë°©í–¥ ê²°ì •
        if slope > 0.01:  # ì„ê³„ê°’
            direction = 'degrading' if self._is_negative_metric(sorted_metrics[0].metric_type) else 'improving'
        elif slope < -0.01:
            direction = 'improving' if self._is_negative_metric(sorted_metrics[0].metric_type) else 'degrading'
        else:
            direction = 'stable'
        
        return {
            'direction': direction,
            'value': slope,
            'confidence': abs(r_value)
        }
    
    def _is_negative_metric(self, metric_type: str) -> bool:
        """ìŒìˆ˜ ë©”íŠ¸ë¦­ ì—¬ë¶€ (ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­)"""
        negative_metrics = ['response_time', 'error_rate', 'latency']
        return metric_type in negative_metrics
    
    def _generate_capacity_forecasts(self):
        """ìš©ëŸ‰ ì˜ˆì¸¡ ìƒì„±"""
        # ë¦¬ì „ë³„, ë¦¬ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        resource_types = ['cpu', 'memory', 'storage', 'network']
        
        for region_id in ['us-east-1', 'us-west-2', 'eu-west-1', 'ap-northeast-1']:
            for resource_type in resource_types:
                forecast = self._forecast_resource_usage(region_id, resource_type)
                if forecast:
                    forecast_id = f"forecast_{region_id}_{resource_type}_{int(time.time())}"
                    
                    with self.lock:
                        self.capacity_forecasts[forecast_id] = forecast
                    
                    logger.info(f"Capacity forecast generated: {forecast_id}")
    
    def _forecast_resource_usage(self, region_id: str, resource_type: str) -> Optional[CapacityForecast]:
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡"""
        # ìµœê·¼ 7ì¼ ë°ì´í„° ìˆ˜ì§‘
        end_time = datetime.now()
        start_time = end_time - timedelta(days=7)
        
        metrics = [
            metric for metric in self.performance_metrics.values()
            if (metric.region_id == region_id and 
                metric.service_name == 'system' and 
                metric.metric_type == f"{resource_type}_usage" and
                start_time <= metric.timestamp <= end_time)
        ]
        
        if len(metrics) < 24:  # ìµœì†Œ 24ê°œ ë°ì´í„° í¬ì¸íŠ¸ í•„ìš”
            return None
        
        # í˜„ì¬ ì‚¬ìš©ëŸ‰
        current_usage = metrics[-1].value if metrics else 0.0
        
        # ì˜ˆì¸¡ (ê°„ë‹¨í•œ ì„ í˜• ì˜ˆì¸¡)
        values = [metric.value for metric in metrics]
        timestamps = [metric.timestamp.timestamp() for metric in metrics]
        
        slope, intercept, r_value, p_value, std_err = stats.linregress(timestamps, values)
        
        # 7ì¼ í›„ ì˜ˆì¸¡
        future_timestamp = end_time.timestamp() + (7 * 24 * 3600)  # 7ì¼ í›„
        predicted_usage = slope * future_timestamp + intercept
        
        # ì‹ ë¢° êµ¬ê°„ ê³„ì‚°
        confidence_interval = (
            max(0, predicted_usage - 2 * std_err),
            min(100, predicted_usage + 2 * std_err)
        )
        
        return CapacityForecast(
            forecast_id=f"forecast_{region_id}_{resource_type}_{int(time.time())}",
            region_id=region_id,
            resource_type=resource_type,
            current_usage=current_usage,
            predicted_usage=predicted_usage,
            prediction_date=end_time + timedelta(days=7),
            confidence_interval=confidence_interval
        )
    
    def get_performance_metric(self, metric_id: str) -> Optional[PerformanceMetric]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return self.performance_metrics.get(metric_id)
    
    def get_metrics_by_region(self, region_id: str, 
                            time_range: Optional[Tuple[datetime, datetime]] = None) -> List[PerformanceMetric]:
        """ë¦¬ì „ë³„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        metrics = [
            metric for metric in self.performance_metrics.values()
            if metric.region_id == region_id
        ]
        
        if time_range:
            start_time, end_time = time_range
            metrics = [
                metric for metric in metrics
                if start_time <= metric.timestamp <= end_time
            ]
        
        return sorted(metrics, key=lambda x: x.timestamp)
    
    def get_performance_trend(self, trend_id: str) -> Optional[PerformanceTrend]:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ì¡°íšŒ"""
        return self.performance_trends.get(trend_id)
    
    def get_trends_by_region(self, region_id: str) -> List[PerformanceTrend]:
        """ë¦¬ì „ë³„ íŠ¸ë Œë“œ ì¡°íšŒ"""
        return [
            trend for trend in self.performance_trends.values()
            if trend.region_id == region_id
        ]
    
    def get_capacity_forecast(self, forecast_id: str) -> Optional[CapacityForecast]:
        """ìš©ëŸ‰ ì˜ˆì¸¡ ì¡°íšŒ"""
        return self.capacity_forecasts.get(forecast_id)
    
    def get_forecasts_by_region(self, region_id: str) -> List[CapacityForecast]:
        """ë¦¬ì „ë³„ ì˜ˆì¸¡ ì¡°íšŒ"""
        return [
            forecast for forecast in self.capacity_forecasts.values()
            if forecast.region_id == region_id
        ]

class AnalysisMetrics:
    """ë¶„ì„ ë©”íŠ¸ë¦­"""
    
    def __init__(self):
        self.analysis_cycles = 0
        self.trends_calculated = 0
        self.forecasts_generated = 0
        self.start_time = time.time()
        self.lock = threading.Lock()
    
    def record_analysis_cycle(self):
        """ë¶„ì„ ì‚¬ì´í´ ê¸°ë¡"""
        with self.lock:
            self.analysis_cycles += 1
    
    def record_trends_calculated(self, count: int = 1):
        """ê³„ì‚°ëœ íŠ¸ë Œë“œ ê¸°ë¡"""
        with self.lock:
            self.trends_calculated += count
    
    def record_forecasts_generated(self, count: int = 1):
        """ìƒì„±ëœ ì˜ˆì¸¡ ê¸°ë¡"""
        with self.lock:
            self.forecasts_generated += count
    
    def get_metrics(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        with self.lock:
            uptime = time.time() - self.start_time
            return {
                'analysis_cycles': self.analysis_cycles,
                'trends_calculated': self.trends_calculated,
                'forecasts_generated': self.forecasts_generated,
                'cycles_per_minute': self.analysis_cycles / (uptime / 60) if uptime > 0 else 0,
                'uptime_seconds': uptime
            }
```

## ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„**

### ğŸ“‹ **ì™„ë£Œëœ ì‘ì—…**
- âœ… ì‹¤ì‹œê°„ ìœ„í—˜ í‰ê°€ ì‹œìŠ¤í…œ ì„¤ê³„ (í¬íŠ¸í´ë¦¬ì˜¤ ìœ„í—˜, ì‹œì¥ ìœ„í—˜)
- âœ… ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì„¤ê³„ (ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜, ë³‘ë ¬ ì‹¤í–‰)
- âœ… ìœ„í—˜ í•œë„ ê´€ë¦¬ ì‹œìŠ¤í…œ ì„¤ê³„
- âœ… ëŒ€ì¹­í‚¤ ì•”í˜¸í™” ì‹œìŠ¤í…œ ì„¤ê³„ (AES-256-GCM)
- âœ… ë°ì´í„° ìµëª…í™” ì‹œìŠ¤í…œ ì„¤ê³„ (ë§ˆìŠ¤í‚¹, í•´ì‹±, ì¼ë°˜í™”)
- âœ… ì‹¤ì‹œê°„ ë¡œê·¸ ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì„¤ê³„ (êµ¬ì¡°í™”ëœ ë¡œê¹…, ì‹¤ì‹œê°„ ì²˜ë¦¬)
- âœ… ê·œì • ì¤€ìˆ˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ê³„ (GDPR, SOX, PCI-DSS)
- âœ… ë‹¤ì¤‘ ë¦¬ì „ ë°°í¬ ì‹œìŠ¤í…œ ì„¤ê³„ (ë¦¬ì „ ê´€ë¦¬, ë°°í¬ ìë™í™”)
- âœ… ê¸€ë¡œë²Œ ë¡œë“œ ë°¸ëŸ°ì‹± ì‹œìŠ¤í…œ ì„¤ê³„ (ì§€ë¦¬ì  ë¼ìš°íŒ…, íŠ¸ë˜í”½ ê´€ë¦¬)
- âœ… ì§€ì—­ë³„ ì €ì¥ì†Œ ì‹œìŠ¤í…œ ì„¤ê³„ (ì£¼ ì €ì¥ì†Œ, ë³µì œ ì €ì¥ì†Œ, ìºì‹œ ì €ì¥ì†Œ)
- âœ… ë°ì´í„° ë™ê¸°í™” ì‹œìŠ¤í…œ ì„¤ê³„ (ì‹¤ì‹œê°„ ë™ê¸°í™”, ì¶©ëŒ í•´ê²°)
- âœ… ë¶„ì‚° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ê³„ (ì§€ì—­ë³„ ì—ì´ì „íŠ¸, ì¤‘ì•™ ìˆ˜ì§‘ê¸°)
- âœ… ì„±ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ ì„¤ê³„ (ë©”íŠ¸ë¦­ ìˆ˜ì§‘, íŠ¸ë Œë“œ ë¶„ì„, ìš©ëŸ‰ ì˜ˆì¸¡)

### ğŸ”„ **ì§„í–‰ ì¤‘ì¸ ì‘ì—…**
- ğŸ”„ ì¥ì•  ê´€ë¦¬ ì‹œìŠ¤í…œ (ì¥ì•  ê°ì§€, ì•Œë¦¼ ê´€ë¦¬, ìë™ ë³µêµ¬)
- ğŸ”„ ë³´ì•ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (ìœ„í˜‘ ê°ì§€, ë³´ì•ˆ ì´ë²¤íŠ¸, ê·œì • ì¤€ìˆ˜)

### â³ **ë‹¤ìŒ ë‹¨ê³„**
1. **ì¥ì•  ê´€ë¦¬ ì‹œìŠ¤í…œ** ë¬¸ì„œ ìƒì„±
2. **ë³´ì•ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ** ë¬¸ì„œ ìƒì„±
3. **Phase 7.3 ì¬í•´ ë³µêµ¬** ë¬¸ì„œ ìƒì„±

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-01-31
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: 2024-02-01 (ì¥ì•  ê´€ë¦¬ ì‹œìŠ¤í…œ)
**ê¸€ë¡œë²Œ ëª¨ë‹ˆí„°ë§ ëª©í‘œ**: < 5ì´ˆ ì´ë²¤íŠ¸ ê°ì§€, < 10ì´ˆ ì•Œë¦¼ ì „ì†¡, < 1ì´ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
**ê¸€ë¡œë²Œ ëª¨ë‹ˆí„°ë§ ì„±ê³¼**: ë¶„ì‚° ëª¨ë‹ˆí„°ë§, ì„±ëŠ¥ ë¶„ì„, ì¥ì•  ê´€ë¦¬ 