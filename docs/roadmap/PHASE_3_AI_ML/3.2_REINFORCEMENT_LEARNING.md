# ğŸ¤– Phase 3.2: ê°•í™”í•™ìŠµ ê¸°ë°˜ ê±°ë˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ

## ğŸ“‹ **ê°œìš”**

### ğŸ¯ **ëª©í‘œ**
- **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ**: í˜‘ë ¥ ë° ê²½ìŸ ê¸°ë°˜ ê±°ë˜ ì—ì´ì „íŠ¸
- **ê³ ê¸‰ ê°•í™”í•™ìŠµ**: DDPG, PPO, SAC, TD3 ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
- **ì‹¤ì‹œê°„ í•™ìŠµ**: ì˜¨ë¼ì¸ í•™ìŠµ ë° ì ì‘í˜• ì •ì±… ì—…ë°ì´íŠ¸
- **í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”**: ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë° ìˆ˜ìµë¥  ê·¹ëŒ€í™”
- **ë©€í‹° íƒ€ì„í”„ë ˆì„**: ë‹¤ì–‘í•œ ì‹œê°„ëŒ€ ì „ëµ í†µí•©

### ğŸ“Š **ì„±ëŠ¥ ëª©í‘œ**
- **ì—ì´ì „íŠ¸ ìˆ˜ìµë¥ **: > 15% ì—°ê°„ ìˆ˜ìµë¥ 
- **ìƒ¤í”„ ë¹„ìœ¨**: > 1.5
- **ìµœëŒ€ ë‚™í­**: < 10%
- **ìŠ¹ë¥ **: > 60%
- **í•™ìŠµ ìˆ˜ë ´ ì‹œê°„**: < 24ì‹œê°„

## ğŸ—ï¸ **ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

### ğŸ“ **ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œ êµ¬ì¡°**
```
reinforcement-learning/
â”œâ”€â”€ agents/                        # ê±°ë˜ ì—ì´ì „íŠ¸
â”‚   â”œâ”€â”€ ddpg/                     # DDPG ì—ì´ì „íŠ¸
â”‚   â”œâ”€â”€ ppo/                      # PPO ì—ì´ì „íŠ¸
â”‚   â”œâ”€â”€ sac/                      # SAC ì—ì´ì „íŠ¸
â”‚   â”œâ”€â”€ td3/                      # TD3 ì—ì´ì „íŠ¸
â”‚   â””â”€â”€ multi-agent/              # ë‹¤ì¤‘ ì—ì´ì „íŠ¸
â”œâ”€â”€ environments/                  # ê±°ë˜ í™˜ê²½
â”‚   â”œâ”€â”€ trading_env/              # ê¸°ë³¸ ê±°ë˜ í™˜ê²½
â”‚   â”œâ”€â”€ portfolio_env/            # í¬íŠ¸í´ë¦¬ì˜¤ í™˜ê²½
â”‚   â”œâ”€â”€ risk_env/                 # ë¦¬ìŠ¤í¬ ê´€ë¦¬ í™˜ê²½
â”‚   â””â”€â”€ multi_timeframe_env/      # ë©€í‹° íƒ€ì„í”„ë ˆì„ í™˜ê²½
â”œâ”€â”€ networks/                     # ì‹ ê²½ë§ ëª¨ë¸
â”‚   â”œâ”€â”€ actor/                    # ì•¡í„° ë„¤íŠ¸ì›Œí¬
â”‚   â”œâ”€â”€ critic/                   # í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬
â”‚   â”œâ”€â”€ value/                    # ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬
â”‚   â””â”€â”€ policy/                   # ì •ì±… ë„¤íŠ¸ì›Œí¬
â”œâ”€â”€ memory/                       # ê²½í—˜ ë¦¬í”Œë ˆì´
â”‚   â”œâ”€â”€ prioritized/              # ìš°ì„ ìˆœìœ„ ë¦¬í”Œë ˆì´
â”‚   â”œâ”€â”€ episodic/                 # ì—í”¼ì†Œë“œ ë¦¬í”Œë ˆì´
â”‚   â””â”€â”€ hierarchical/             # ê³„ì¸µì  ë¦¬í”Œë ˆì´
â”œâ”€â”€ algorithms/                   # ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜
â”‚   â”œâ”€â”€ ddpg_algorithm.py         # DDPG ì•Œê³ ë¦¬ì¦˜
â”‚   â”œâ”€â”€ ppo_algorithm.py          # PPO ì•Œê³ ë¦¬ì¦˜
â”‚   â”œâ”€â”€ sac_algorithm.py          # SAC ì•Œê³ ë¦¬ì¦˜
â”‚   â””â”€â”€ td3_algorithm.py          # TD3 ì•Œê³ ë¦¬ì¦˜
â””â”€â”€ training/                     # í•™ìŠµ ê´€ë¦¬
    â”œâ”€â”€ curriculum/               # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ
    â”œâ”€â”€ hyperparameter/           # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    â””â”€â”€ evaluation/               # ì„±ëŠ¥ í‰ê°€
```

## ğŸ”§ **DDPG (Deep Deterministic Policy Gradient) ì—ì´ì „íŠ¸**

### ğŸ“¦ **DDPG ê±°ë˜ ì—ì´ì „íŠ¸**

```python
# reinforcement-learning/agents/ddpg/ddpg_trading_agent.py
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input, Concatenate
from tensorflow.keras.optimizers import Adam
from collections import deque
import random
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class ActorNetwork(Model):
    """ì•¡í„° ë„¤íŠ¸ì›Œí¬ (ì •ì±… ë„¤íŠ¸ì›Œí¬)"""
    
    def __init__(self, state_dim: int, action_dim: int, action_bound: float = 1.0):
        super(ActorNetwork, self).__init__()
        self.action_bound = action_bound
        
        # ë„¤íŠ¸ì›Œí¬ ë ˆì´ì–´
        self.dense1 = Dense(400, activation='relu', input_shape=(state_dim,))
        self.dense2 = Dense(300, activation='relu')
        self.dense3 = Dense(action_dim, activation='tanh')
        
        # ë°°ì¹˜ ì •ê·œí™”
        self.batch_norm1 = tf.keras.layers.BatchNormalization()
        self.batch_norm2 = tf.keras.layers.BatchNormalization()
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout1 = tf.keras.layers.Dropout(0.1)
        self.dropout2 = tf.keras.layers.Dropout(0.1)
    
    def call(self, state):
        x = self.dense1(state)
        x = self.batch_norm1(x)
        x = self.dropout1(x)
        
        x = self.dense2(x)
        x = self.batch_norm2(x)
        x = self.dropout2(x)
        
        action = self.dense3(x)
        return action * self.action_bound

class CriticNetwork(Model):
    """í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬ (ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬)"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super(CriticNetwork, self).__init__()
        
        # ìƒíƒœ ì²˜ë¦¬ ë ˆì´ì–´
        self.state_dense1 = Dense(400, activation='relu', input_shape=(state_dim,))
        self.state_dense2 = Dense(300, activation='relu')
        
        # ì•¡ì…˜ ì²˜ë¦¬ ë ˆì´ì–´
        self.action_dense1 = Dense(300, activation='relu', input_shape=(action_dim,))
        
        # ê²°í•© ë ˆì´ì–´
        self.combined_dense1 = Dense(300, activation='relu')
        self.combined_dense2 = Dense(200, activation='relu')
        self.q_value = Dense(1, activation=None)
        
        # ë°°ì¹˜ ì •ê·œí™”
        self.batch_norm1 = tf.keras.layers.BatchNormalization()
        self.batch_norm2 = tf.keras.layers.BatchNormalization()
    
    def call(self, state, action):
        # ìƒíƒœ ì²˜ë¦¬
        state_out = self.state_dense1(state)
        state_out = self.batch_norm1(state_out)
        state_out = self.state_dense2(state_out)
        
        # ì•¡ì…˜ ì²˜ë¦¬
        action_out = self.action_dense1(action)
        
        # ê²°í•©
        combined = Concatenate()([state_out, action_out])
        combined = self.combined_dense1(combined)
        combined = self.batch_norm2(combined)
        combined = self.combined_dense2(combined)
        
        q_value = self.q_value(combined)
        return q_value

class DDPGTradingAgent:
    """DDPG ê¸°ë°˜ ê±°ë˜ ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_dim: int, action_dim: int, 
                 learning_rate_actor: float = 0.0001,
                 learning_rate_critic: float = 0.001,
                 gamma: float = 0.99,
                 tau: float = 0.001,
                 memory_size: int = 100000,
                 batch_size: int = 64):
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate_actor = learning_rate_actor
        self.learning_rate_critic = learning_rate_critic
        self.gamma = gamma
        self.tau = tau
        self.batch_size = batch_size
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.actor = ActorNetwork(state_dim, action_dim)
        self.critic = CriticNetwork(state_dim, action_dim)
        self.target_actor = ActorNetwork(state_dim, action_dim)
        self.target_critic = CriticNetwork(state_dim, action_dim)
        
        # ì˜µí‹°ë§ˆì´ì €
        self.actor_optimizer = Adam(learning_rate=learning_rate_actor)
        self.critic_optimizer = Adam(learning_rate=learning_rate_critic)
        
        # ë©”ëª¨ë¦¬
        self.memory = deque(maxlen=memory_size)
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.update_target_networks(tau=1.0)
        
        # ë…¸ì´ì¦ˆ íŒŒë¼ë¯¸í„°
        self.noise_std = 0.1
        self.noise_decay = 0.9999
        self.min_noise = 0.01
        
        logger.info(f"DDPG Trading Agent initialized: state_dim={state_dim}, action_dim={action_dim}")
    
    def get_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """ì•¡ì…˜ ì„ íƒ"""
        try:
            state = np.array(state).reshape(1, -1)
            action = self.actor(state).numpy()[0]
            
            if training:
                # íƒí—˜ì„ ìœ„í•œ ë…¸ì´ì¦ˆ ì¶”ê°€
                noise = np.random.normal(0, self.noise_std, self.action_dim)
                action = np.clip(action + noise, -1, 1)
                
                # ë…¸ì´ì¦ˆ ê°ì†Œ
                self.noise_std = max(self.min_noise, self.noise_std * self.noise_decay)
            
            return action
            
        except Exception as e:
            logger.error(f"Action selection failed: {e}")
            return np.zeros(self.action_dim)
    
    def remember(self, state: np.ndarray, action: np.ndarray, 
                reward: float, next_state: np.ndarray, done: bool):
        """ê²½í—˜ ì €ì¥"""
        try:
            self.memory.append((state, action, reward, next_state, done))
        except Exception as e:
            logger.error(f"Memory storage failed: {e}")
    
    def train(self) -> Dict[str, float]:
        """ë„¤íŠ¸ì›Œí¬ í•™ìŠµ"""
        try:
            if len(self.memory) < self.batch_size:
                return {'actor_loss': 0.0, 'critic_loss': 0.0}
            
            # ë°°ì¹˜ ìƒ˜í”Œë§
            batch = random.sample(self.memory, self.batch_size)
            states = np.array([e[0] for e in batch])
            actions = np.array([e[1] for e in batch])
            rewards = np.array([e[2] for e in batch])
            next_states = np.array([e[3] for e in batch])
            dones = np.array([e[4] for e in batch])
            
            # í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬ í•™ìŠµ
            with tf.GradientTape() as tape:
                current_q_values = self.critic(states, actions)
                next_actions = self.target_actor(next_states)
                next_q_values = self.target_critic(next_states, next_actions)
                target_q_values = rewards + self.gamma * next_q_values * (1 - dones)
                critic_loss = tf.reduce_mean(tf.square(target_q_values - current_q_values))
            
            critic_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)
            self.critic_optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))
            
            # ì•¡í„° ë„¤íŠ¸ì›Œí¬ í•™ìŠµ
            with tf.GradientTape() as tape:
                current_actions = self.actor(states)
                actor_loss = -tf.reduce_mean(self.critic(states, current_actions))
            
            actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)
            self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))
            
            # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            self.update_target_networks()
            
            return {
                'actor_loss': float(actor_loss),
                'critic_loss': float(critic_loss),
                'noise_std': self.noise_std
            }
            
        except Exception as e:
            logger.error(f"Training failed: {e}")
            return {'actor_loss': 0.0, 'critic_loss': 0.0}
    
    def update_target_networks(self, tau: Optional[float] = None):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        try:
            if tau is None:
                tau = self.tau
            
            # íƒ€ê²Ÿ ì•¡í„° ì—…ë°ì´íŠ¸
            for target_param, param in zip(self.target_actor.trainable_variables, 
                                         self.actor.trainable_variables):
                target_param.assign(tau * param + (1 - tau) * target_param)
            
            # íƒ€ê²Ÿ í¬ë¦¬í‹± ì—…ë°ì´íŠ¸
            for target_param, param in zip(self.target_critic.trainable_variables, 
                                         self.critic.trainable_variables):
                target_param.assign(tau * param + (1 - tau) * target_param)
                
        except Exception as e:
            logger.error(f"Target network update failed: {e}")
    
    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        try:
            self.actor.save_weights(f"{filepath}_actor.h5")
            self.critic.save_weights(f"{filepath}_critic.h5")
            self.target_actor.save_weights(f"{filepath}_target_actor.h5")
            self.target_critic.save_weights(f"{filepath}_target_critic.h5")
            logger.info(f"Model saved to: {filepath}")
        except Exception as e:
            logger.error(f"Model save failed: {e}")
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            self.actor.load_weights(f"{filepath}_actor.h5")
            self.critic.load_weights(f"{filepath}_critic.h5")
            self.target_actor.load_weights(f"{filepath}_target_actor.h5")
            self.target_critic.load_weights(f"{filepath}_target_critic.h5")
            logger.info(f"Model loaded from: {filepath}")
        except Exception as e:
            logger.error(f"Model load failed: {e}")
```

## ğŸ”§ **PPO (Proximal Policy Optimization) ì—ì´ì „íŠ¸**

### ğŸ“¦ **PPO ê±°ë˜ ì—ì´ì „íŠ¸**

```python
# reinforcement-learning/agents/ppo/ppo_trading_agent.py
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from collections import deque
import random
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class PPONetwork(Model):
    """PPO ë„¤íŠ¸ì›Œí¬ (ì•¡í„°-í¬ë¦¬í‹±)"""
    
    def __init__(self, state_dim: int, action_dim: int, action_bound: float = 1.0):
        super(PPONetwork, self).__init__()
        self.action_dim = action_dim
        self.action_bound = action_bound
        
        # ê³µí†µ ë ˆì´ì–´
        self.dense1 = Dense(256, activation='relu', input_shape=(state_dim,))
        self.dense2 = Dense(128, activation='relu')
        
        # ì•¡í„° ë ˆì´ì–´ (ì •ì±…)
        self.actor_dense = Dense(64, activation='relu')
        self.actor_output = Dense(action_dim, activation='tanh')
        
        # í¬ë¦¬í‹± ë ˆì´ì–´ (ê°€ì¹˜)
        self.critic_dense = Dense(64, activation='relu')
        self.critic_output = Dense(1, activation=None)
        
        # ì•¡ì…˜ í‘œì¤€í¸ì°¨ (í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°)
        self.action_std = tf.Variable(0.5, trainable=True)
    
    def call(self, state):
        # ê³µí†µ ë ˆì´ì–´
        x = self.dense1(state)
        x = self.dense2(x)
        
        # ì•¡í„° (ì •ì±…)
        actor_x = self.actor_dense(x)
        action_mean = self.actor_output(actor_x) * self.action_bound
        
        # í¬ë¦¬í‹± (ê°€ì¹˜)
        critic_x = self.critic_dense(x)
        value = self.critic_output(critic_x)
        
        return action_mean, value
    
    def get_action_and_log_prob(self, state: np.ndarray) -> Tuple[np.ndarray, float]:
        """ì•¡ì…˜ê³¼ ë¡œê·¸ í™•ë¥  ë°˜í™˜"""
        state = np.array(state).reshape(1, -1)
        action_mean, value = self(state)
        
        # ì •ê·œë¶„í¬ì—ì„œ ì•¡ì…˜ ìƒ˜í”Œë§
        action_std = tf.exp(self.action_std)
        action_dist = tf.random.normal(action_mean.shape, action_mean, action_std)
        action = tf.clip_by_value(action_dist, -self.action_bound, self.action_bound)
        
        # ë¡œê·¸ í™•ë¥  ê³„ì‚°
        log_prob = self._log_prob(action, action_mean, action_std)
        
        return action.numpy()[0], log_prob.numpy()[0], value.numpy()[0]
    
    def _log_prob(self, action, mean, std):
        """ë¡œê·¸ í™•ë¥  ê³„ì‚°"""
        return -0.5 * tf.square((action - mean) / std) - tf.math.log(std) - 0.5 * tf.math.log(2 * np.pi)

class PPOTradingAgent:
    """PPO ê¸°ë°˜ ê±°ë˜ ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_dim: int, action_dim: int,
                 learning_rate: float = 0.0003,
                 gamma: float = 0.99,
                 gae_lambda: float = 0.95,
                 clip_ratio: float = 0.2,
                 value_coef: float = 0.5,
                 entropy_coef: float = 0.01,
                 max_grad_norm: float = 0.5,
                 update_epochs: int = 10):
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_ratio = clip_ratio
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.update_epochs = update_epochs
        
        # ë„¤íŠ¸ì›Œí¬
        self.network = PPONetwork(state_dim, action_dim)
        self.optimizer = Adam(learning_rate=learning_rate)
        
        # ë©”ëª¨ë¦¬
        self.memory = deque()
        
        # í†µê³„
        self.episode_rewards = []
        self.episode_lengths = []
        
        logger.info(f"PPO Trading Agent initialized: state_dim={state_dim}, action_dim={action_dim}")
    
    def get_action(self, state: np.ndarray) -> Tuple[np.ndarray, float, float]:
        """ì•¡ì…˜ ì„ íƒ"""
        try:
            action, log_prob, value = self.network.get_action_and_log_prob(state)
            return action, log_prob, value
        except Exception as e:
            logger.error(f"Action selection failed: {e}")
            return np.zeros(self.action_dim), 0.0, 0.0
    
    def remember(self, state: np.ndarray, action: np.ndarray, reward: float,
                next_state: np.ndarray, done: bool, log_prob: float, value: float):
        """ê²½í—˜ ì €ì¥"""
        try:
            self.memory.append((state, action, reward, next_state, done, log_prob, value))
        except Exception as e:
            logger.error(f"Memory storage failed: {e}")
    
    def compute_gae(self, rewards: List[float], values: List[float], 
                   dones: List[bool]) -> List[float]:
        """Generalized Advantage Estimation ê³„ì‚°"""
        try:
            advantages = []
            gae = 0
            
            for i in reversed(range(len(rewards))):
                if i == len(rewards) - 1:
                    next_value = 0
                else:
                    next_value = values[i + 1]
                
                delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i]
                gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae
                advantages.insert(0, gae)
            
            return advantages
            
        except Exception as e:
            logger.error(f"GAE computation failed: {e}")
            return [0.0] * len(rewards)
    
    def train(self) -> Dict[str, float]:
        """ë„¤íŠ¸ì›Œí¬ í•™ìŠµ"""
        try:
            if len(self.memory) < 100:  # ìµœì†Œ ë°°ì¹˜ í¬ê¸°
                return {'policy_loss': 0.0, 'value_loss': 0.0, 'entropy_loss': 0.0}
            
            # ë©”ëª¨ë¦¬ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            states = np.array([e[0] for e in self.memory])
            actions = np.array([e[1] for e in self.memory])
            rewards = np.array([e[2] for e in self.memory])
            dones = np.array([e[4] for e in self.memory])
            old_log_probs = np.array([e[5] for e in self.memory])
            old_values = np.array([e[6] for e in self.memory])
            
            # GAE ê³„ì‚°
            advantages = self.compute_gae(rewards.tolist(), old_values.tolist(), dones.tolist())
            advantages = np.array(advantages)
            
            # ì •ê·œí™”
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # íƒ€ê²Ÿ ê°€ì¹˜ ê³„ì‚°
            target_values = advantages + old_values
            
            total_policy_loss = 0.0
            total_value_loss = 0.0
            total_entropy_loss = 0.0
            
            # ì—¬ëŸ¬ ì—í¬í¬ì— ê±¸ì³ í•™ìŠµ
            for epoch in range(self.update_epochs):
                # ë°°ì¹˜ ì¸ë±ìŠ¤ ìƒì„±
                indices = np.random.permutation(len(states))
                
                for start_idx in range(0, len(states), 64):  # ë°°ì¹˜ í¬ê¸° 64
                    end_idx = min(start_idx + 64, len(states))
                    batch_indices = indices[start_idx:end_idx]
                    
                    batch_states = states[batch_indices]
                    batch_actions = actions[batch_indices]
                    batch_advantages = advantages[batch_indices]
                    batch_target_values = target_values[batch_indices]
                    batch_old_log_probs = old_log_probs[batch_indices]
                    
                    with tf.GradientTape() as tape:
                        # í˜„ì¬ ì •ì±…ìœ¼ë¡œ ì•¡ì…˜ê³¼ ê°€ì¹˜ ê³„ì‚°
                        action_means, values = self.network(batch_states)
                        
                        # ë¡œê·¸ í™•ë¥  ê³„ì‚°
                        action_std = tf.exp(self.network.action_std)
                        log_probs = -0.5 * tf.square((batch_actions - action_means) / action_std) - \
                                   tf.math.log(action_std) - 0.5 * tf.math.log(2 * np.pi)
                        log_probs = tf.reduce_sum(log_probs, axis=1)
                        
                        # ë¹„ìœ¨ ê³„ì‚°
                        ratio = tf.exp(log_probs - batch_old_log_probs)
                        
                        # í´ë¦¬í•‘ëœ ëª©ì  í•¨ìˆ˜
                        surr1 = ratio * batch_advantages
                        surr2 = tf.clip_by_value(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_advantages
                        policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))
                        
                        # ê°€ì¹˜ ì†ì‹¤
                        value_loss = self.value_coef * tf.reduce_mean(tf.square(values - batch_target_values))
                        
                        # ì—”íŠ¸ë¡œí”¼ ì†ì‹¤
                        entropy_loss = -self.entropy_coef * tf.reduce_mean(log_probs)
                        
                        # ì´ ì†ì‹¤
                        total_loss = policy_loss + value_loss + entropy_loss
                    
                    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° ì ìš©
                    gradients = tape.gradient(total_loss, self.network.trainable_variables)
                    gradients, _ = tf.clip_by_global_norm(gradients, self.max_grad_norm)
                    self.optimizer.apply_gradients(zip(gradients, self.network.trainable_variables))
                    
                    total_policy_loss += float(policy_loss)
                    total_value_loss += float(value_loss)
                    total_entropy_loss += float(entropy_loss)
            
            # ë©”ëª¨ë¦¬ í´ë¦¬ì–´
            self.memory.clear()
            
            avg_policy_loss = total_policy_loss / (self.update_epochs * (len(states) // 64))
            avg_value_loss = total_value_loss / (self.update_epochs * (len(states) // 64))
            avg_entropy_loss = total_entropy_loss / (self.update_epochs * (len(states) // 64))
            
            return {
                'policy_loss': avg_policy_loss,
                'value_loss': avg_value_loss,
                'entropy_loss': avg_entropy_loss
            }
            
        except Exception as e:
            logger.error(f"Training failed: {e}")
            return {'policy_loss': 0.0, 'value_loss': 0.0, 'entropy_loss': 0.0}
    
    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        try:
            self.network.save_weights(f"{filepath}.h5")
            logger.info(f"Model saved to: {filepath}")
        except Exception as e:
            logger.error(f"Model save failed: {e}")
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            self.network.load_weights(f"{filepath}.h5")
            logger.info(f"Model loaded from: {filepath}")
        except Exception as e:
            logger.error(f"Model load failed: {e}")
```

## ğŸ”§ **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ**

### ğŸ“¦ **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê±°ë˜ ì‹œìŠ¤í…œ**

```python
# reinforcement-learning/agents/multi-agent/multi_agent_trading_system.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import logging
import asyncio

logger = logging.getLogger(__name__)

@dataclass
class AgentConfig:
    """ì—ì´ì „íŠ¸ ì„¤ì •"""
    agent_id: str
    agent_type: str  # 'ddpg', 'ppo', 'sac', 'td3'
    trading_pairs: List[str]
    risk_tolerance: float  # 0.0 (ë³´ìˆ˜ì ) ~ 1.0 (ê³µê²©ì )
    capital_allocation: float  # ì´ ìë³¸ ëŒ€ë¹„ í• ë‹¹ ë¹„ìœ¨
    time_horizon: str  # 'short', 'medium', 'long'

@dataclass
class MarketState:
    """ì‹œì¥ ìƒíƒœ"""
    timestamp: datetime
    prices: Dict[str, float]
    volumes: Dict[str, float]
    technical_indicators: Dict[str, Dict[str, float]]
    market_sentiment: float  # -1.0 (ë§¤ìš° ë¶€ì •ì ) ~ 1.0 (ë§¤ìš° ê¸ì •ì )
    volatility: float

class MultiAgentTradingSystem:
    """ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê±°ë˜ ì‹œìŠ¤í…œ"""
    
    def __init__(self, agents_config: List[AgentConfig], 
                 total_capital: float = 100000.0):
        self.agents_config = agents_config
        self.total_capital = total_capital
        self.agents = {}
        self.agent_states = {}
        self.market_state = None
        self.portfolio = {}
        self.trading_history = []
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self._initialize_agents()
        
        logger.info(f"Multi-agent trading system initialized with {len(agents_config)} agents")
    
    def _initialize_agents(self):
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        try:
            for config in self.agents_config:
                if config.agent_type == 'ddpg':
                    from .ddpg.ddpg_trading_agent import DDPGTradingAgent
                    state_dim = self._get_state_dimension(config.trading_pairs)
                    action_dim = len(config.trading_pairs) * 2  # buy/sell for each pair
                    self.agents[config.agent_id] = DDPGTradingAgent(state_dim, action_dim)
                
                elif config.agent_type == 'ppo':
                    from .ppo.ppo_trading_agent import PPOTradingAgent
                    state_dim = self._get_state_dimension(config.trading_pairs)
                    action_dim = len(config.trading_pairs) * 2
                    self.agents[config.agent_id] = PPOTradingAgent(state_dim, action_dim)
                
                # ì´ˆê¸° ìƒíƒœ ì„¤ì •
                self.agent_states[config.agent_id] = {
                    'capital': self.total_capital * config.capital_allocation,
                    'positions': {pair: 0.0 for pair in config.trading_pairs},
                    'performance': {'returns': [], 'sharpe_ratio': 0.0, 'max_drawdown': 0.0}
                }
                
                logger.info(f"Agent {config.agent_id} initialized: {config.agent_type}")
                
        except Exception as e:
            logger.error(f"Agent initialization failed: {e}")
            raise
    
    def _get_state_dimension(self, trading_pairs: List[str]) -> int:
        """ìƒíƒœ ì°¨ì› ê³„ì‚°"""
        # ê¸°ë³¸ íŠ¹ì„±: ê°€ê²©, ê±°ë˜ëŸ‰, ê¸°ìˆ ì  ì§€í‘œ
        base_features = 10  # OHLCV + 5 technical indicators
        return len(trading_pairs) * base_features + 5  # + 5 global market features
    
    def update_market_state(self, market_data: Dict[str, any]):
        """ì‹œì¥ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            self.market_state = MarketState(
                timestamp=market_data['timestamp'],
                prices=market_data['prices'],
                volumes=market_data['volumes'],
                technical_indicators=market_data['technical_indicators'],
                market_sentiment=market_data['market_sentiment'],
                volatility=market_data['volatility']
            )
            
            logger.debug(f"Market state updated: {self.market_state.timestamp}")
            
        except Exception as e:
            logger.error(f"Market state update failed: {e}")
    
    def get_agent_state(self, agent_id: str) -> np.ndarray:
        """ì—ì´ì „íŠ¸ë³„ ìƒíƒœ ìƒì„±"""
        try:
            config = next(c for c in self.agents_config if c.agent_id == agent_id)
            state_features = []
            
            # ê±°ë˜ìŒë³„ íŠ¹ì„±
            for pair in config.trading_pairs:
                if pair in self.market_state.prices:
                    price = self.market_state.prices[pair]
                    volume = self.market_state.volumes.get(pair, 0.0)
                    indicators = self.market_state.technical_indicators.get(pair, {})
                    
                    # ê¸°ë³¸ íŠ¹ì„±
                    state_features.extend([
                        price,
                        volume,
                        indicators.get('rsi', 50.0),
                        indicators.get('macd', 0.0),
                        indicators.get('bollinger_position', 0.5),
                        indicators.get('sma_ratio', 1.0),
                        indicators.get('ema_ratio', 1.0)
                    ])
            
            # ê¸€ë¡œë²Œ ì‹œì¥ íŠ¹ì„±
            state_features.extend([
                self.market_state.market_sentiment,
                self.market_state.volatility,
                self.agent_states[agent_id]['capital'] / self.total_capital,
                len([p for p in self.agent_states[agent_id]['positions'].values() if p > 0]),
                config.risk_tolerance
            ])
            
            return np.array(state_features)
            
        except Exception as e:
            logger.error(f"Agent state generation failed: {e}")
            return np.zeros(self._get_state_dimension(config.trading_pairs))
    
    async def execute_trading_cycle(self) -> Dict[str, any]:
        """ê±°ë˜ ì‚¬ì´í´ ì‹¤í–‰"""
        try:
            if self.market_state is None:
                raise ValueError("Market state not available")
            
            trading_results = {}
            
            # ê° ì—ì´ì „íŠ¸ì˜ ê±°ë˜ ê²°ì •
            for agent_id, agent in self.agents.items():
                try:
                    # ìƒíƒœ ìƒì„±
                    state = self.get_agent_state(agent_id)
                    
                    # ì•¡ì…˜ ì„ íƒ
                    if hasattr(agent, 'get_action'):
                        action = agent.get_action(state)
                    else:
                        action = np.zeros(len(agent.agent_config.trading_pairs) * 2)
                    
                    # ê±°ë˜ ì‹¤í–‰
                    trades = self._execute_agent_trades(agent_id, action)
                    
                    # ë³´ìƒ ê³„ì‚°
                    reward = self._calculate_agent_reward(agent_id, trades)
                    
                    # ë‹¤ìŒ ìƒíƒœ
                    next_state = self.get_agent_state(agent_id)
                    
                    # ê²½í—˜ ì €ì¥
                    if hasattr(agent, 'remember'):
                        agent.remember(state, action, reward, next_state, False)
                    
                    trading_results[agent_id] = {
                        'action': action,
                        'trades': trades,
                        'reward': reward,
                        'current_capital': self.agent_states[agent_id]['capital']
                    }
                    
                except Exception as e:
                    logger.error(f"Agent {agent_id} trading cycle failed: {e}")
                    trading_results[agent_id] = {'error': str(e)}
            
            # í¬íŠ¸í´ë¦¬ì˜¤ ì—…ë°ì´íŠ¸
            self._update_portfolio()
            
            # ê±°ë˜ íˆìŠ¤í† ë¦¬ ì €ì¥
            self.trading_history.append({
                'timestamp': self.market_state.timestamp,
                'results': trading_results,
                'portfolio_value': self._get_total_portfolio_value()
            })
            
            logger.info(f"Trading cycle completed: {len(trading_results)} agents")
            return trading_results
            
        except Exception as e:
            logger.error(f"Trading cycle failed: {e}")
            return {}
    
    def _execute_agent_trades(self, agent_id: str, action: np.ndarray) -> List[Dict]:
        """ì—ì´ì „íŠ¸ ê±°ë˜ ì‹¤í–‰"""
        try:
            config = next(c for c in self.agents_config if c.agent_id == agent_id)
            trades = []
            
            # ì•¡ì…˜ì„ ê±°ë˜ë¡œ ë³€í™˜
            for i, pair in enumerate(config.trading_pairs):
                buy_action = action[i * 2]
                sell_action = action[i * 2 + 1]
                
                current_price = self.market_state.prices.get(pair, 0.0)
                if current_price <= 0:
                    continue
                
                # ë§¤ìˆ˜ ê±°ë˜
                if buy_action > 0.1:  # ì„ê³„ê°’
                    trade_amount = buy_action * self.agent_states[agent_id]['capital'] * 0.1
                    quantity = trade_amount / current_price
                    
                    if trade_amount <= self.agent_states[agent_id]['capital']:
                        self.agent_states[agent_id]['capital'] -= trade_amount
                        self.agent_states[agent_id]['positions'][pair] += quantity
                        
                        trades.append({
                            'type': 'buy',
                            'pair': pair,
                            'quantity': quantity,
                            'price': current_price,
                            'amount': trade_amount
                        })
                
                # ë§¤ë„ ê±°ë˜
                elif sell_action > 0.1:
                    current_position = self.agent_states[agent_id]['positions'][pair]
                    if current_position > 0:
                        sell_quantity = min(current_position, sell_action * current_position)
                        trade_amount = sell_quantity * current_price
                        
                        self.agent_states[agent_id]['capital'] += trade_amount
                        self.agent_states[agent_id]['positions'][pair] -= sell_quantity
                        
                        trades.append({
                            'type': 'sell',
                            'pair': pair,
                            'quantity': sell_quantity,
                            'price': current_price,
                            'amount': trade_amount
                        })
            
            return trades
            
        except Exception as e:
            logger.error(f"Trade execution failed: {e}")
            return []
    
    def _calculate_agent_reward(self, agent_id: str, trades: List[Dict]) -> float:
        """ì—ì´ì „íŠ¸ ë³´ìƒ ê³„ì‚°"""
        try:
            config = next(c for c in self.agents_config if c.agent_id == agent_id)
            
            # ìˆ˜ìµë¥  ê³„ì‚°
            current_value = self.agent_states[agent_id]['capital']
            for pair, position in self.agent_states[agent_id]['positions'].items():
                if position > 0 and pair in self.market_state.prices:
                    current_value += position * self.market_state.prices[pair]
            
            initial_capital = self.total_capital * config.capital_allocation
            returns = (current_value - initial_capital) / initial_capital
            
            # ë¦¬ìŠ¤í¬ ì¡°ì • ë³´ìƒ
            risk_penalty = -abs(returns) * (1 - config.risk_tolerance)
            
            # ê±°ë˜ ë¹„ìš©
            transaction_cost = sum(trade['amount'] * 0.001 for trade in trades)  # 0.1% ìˆ˜ìˆ˜ë£Œ
            
            # ìµœì¢… ë³´ìƒ
            reward = returns + risk_penalty - transaction_cost
            
            # ì„±ëŠ¥ ê¸°ë¡
            self.agent_states[agent_id]['performance']['returns'].append(returns)
            
            return reward
            
        except Exception as e:
            logger.error(f"Reward calculation failed: {e}")
            return 0.0
    
    def _update_portfolio(self):
        """í¬íŠ¸í´ë¦¬ì˜¤ ì—…ë°ì´íŠ¸"""
        try:
            total_value = 0.0
            
            for agent_id, state in self.agent_states.items():
                agent_value = state['capital']
                
                for pair, position in state['positions'].items():
                    if position > 0 and pair in self.market_state.prices:
                        agent_value += position * self.market_state.prices[pair]
                
                total_value += agent_value
                self.portfolio[agent_id] = agent_value
            
            self.portfolio['total'] = total_value
            
        except Exception as e:
            logger.error(f"Portfolio update failed: {e}")
    
    def _get_total_portfolio_value(self) -> float:
        """ì´ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜"""
        return self.portfolio.get('total', 0.0)
    
    def get_performance_summary(self) -> Dict[str, any]:
        """ì„±ëŠ¥ ìš”ì•½"""
        try:
            summary = {
                'total_agents': len(self.agents),
                'total_portfolio_value': self._get_total_portfolio_value(),
                'total_return': (self._get_total_portfolio_value() - self.total_capital) / self.total_capital,
                'agent_performances': {}
            }
            
            for agent_id, state in self.agent_states.items():
                returns = state['performance']['returns']
                if returns:
                    sharpe_ratio = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)
                    max_drawdown = self._calculate_max_drawdown(returns)
                    
                    summary['agent_performances'][agent_id] = {
                        'total_return': np.sum(returns),
                        'sharpe_ratio': sharpe_ratio,
                        'max_drawdown': max_drawdown,
                        'current_capital': state['capital']
                    }
            
            return summary
            
        except Exception as e:
            logger.error(f"Performance summary failed: {e}")
            return {}
    
    def _calculate_max_drawdown(self, returns: List[float]) -> float:
        """ìµœëŒ€ ë‚™í­ ê³„ì‚°"""
        try:
            cumulative = np.cumprod(1 + np.array(returns))
            running_max = np.maximum.accumulate(cumulative)
            drawdown = (cumulative - running_max) / running_max
            return float(np.min(drawdown))
        except Exception as e:
            logger.error(f"Max drawdown calculation failed: {e}")
            return 0.0
    
    async def train_all_agents(self):
        """ëª¨ë“  ì—ì´ì „íŠ¸ í•™ìŠµ"""
        try:
            training_results = {}
            
            for agent_id, agent in self.agents.items():
                try:
                    if hasattr(agent, 'train'):
                        result = agent.train()
                        training_results[agent_id] = result
                        logger.info(f"Agent {agent_id} training completed")
                except Exception as e:
                    logger.error(f"Agent {agent_id} training failed: {e}")
                    training_results[agent_id] = {'error': str(e)}
            
            return training_results
            
        except Exception as e:
            logger.error(f"Agent training failed: {e}")
            return {}
```

## ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„**

### ğŸ“‹ **ì™„ë£Œëœ ì‘ì—…**
- âœ… DDPG ê±°ë˜ ì—ì´ì „íŠ¸
- âœ… PPO ê±°ë˜ ì—ì´ì „íŠ¸
- âœ… ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
- âœ… ì•¡í„°-í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬
- âœ… ê²½í—˜ ë¦¬í”Œë ˆì´ ì‹œìŠ¤í…œ

### ğŸ”„ **ì§„í–‰ ì¤‘ì¸ ì‘ì—…**
- ğŸ”„ SAC (Soft Actor-Critic) ì—ì´ì „íŠ¸
- ğŸ”„ TD3 (Twin Delayed DDPG) ì—ì´ì „íŠ¸
- ğŸ”„ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì‹œìŠ¤í…œ

### â³ **ë‹¤ìŒ ë‹¨ê³„**
1. **Phase 3.3 ê°ì • ë¶„ì„** ë¬¸ì„œ ìƒì„±
2. **Phase 3.4 í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”** ë¬¸ì„œ ìƒì„±
3. **Phase 3.5 ë¦¬ìŠ¤í¬ ê´€ë¦¬ AI** ë¬¸ì„œ ìƒì„±

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-01-31
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: 2024-02-01 (Phase 3.3 ê°ì • ë¶„ì„)
**ê°•í™”í•™ìŠµ ëª©í‘œ**: > 15% ì—°ê°„ ìˆ˜ìµë¥ , > 1.5 ìƒ¤í”„ ë¹„ìœ¨, < 10% ìµœëŒ€ ë‚™í­
**ì—ì´ì „íŠ¸ ì„±ëŠ¥**: ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ë ¥, ì‹¤ì‹œê°„ í•™ìŠµ, ì ì‘í˜• ì •ì±… ì—…ë°ì´íŠ¸ 