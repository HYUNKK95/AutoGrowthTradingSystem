# 🤖 Phase 3.2: 강화학습 기반 거래 에이전트 시스템

## 📋 **개요**

### 🎯 **목표**
- **다중 에이전트 시스템**: 협력 및 경쟁 기반 거래 에이전트
- **고급 강화학습**: DDPG, PPO, SAC, TD3 알고리즘 구현
- **실시간 학습**: 온라인 학습 및 적응형 정책 업데이트
- **포트폴리오 최적화**: 리스크 관리 및 수익률 극대화
- **멀티 타임프레임**: 다양한 시간대 전략 통합

### 📊 **성능 목표**
- **에이전트 수익률**: > 15% 연간 수익률
- **샤프 비율**: > 1.5
- **최대 낙폭**: < 10%
- **승률**: > 60%
- **학습 수렴 시간**: < 24시간

## 🏗️ **강화학습 시스템 아키텍처**

### 📁 **강화학습 시스템 구조**
```
reinforcement-learning/
├── agents/                        # 거래 에이전트
│   ├── ddpg/                     # DDPG 에이전트
│   ├── ppo/                      # PPO 에이전트
│   ├── sac/                      # SAC 에이전트
│   ├── td3/                      # TD3 에이전트
│   └── multi-agent/              # 다중 에이전트
├── environments/                  # 거래 환경
│   ├── trading_env/              # 기본 거래 환경
│   ├── portfolio_env/            # 포트폴리오 환경
│   ├── risk_env/                 # 리스크 관리 환경
│   └── multi_timeframe_env/      # 멀티 타임프레임 환경
├── networks/                     # 신경망 모델
│   ├── actor/                    # 액터 네트워크
│   ├── critic/                   # 크리틱 네트워크
│   ├── value/                    # 가치 네트워크
│   └── policy/                   # 정책 네트워크
├── memory/                       # 경험 리플레이
│   ├── prioritized/              # 우선순위 리플레이
│   ├── episodic/                 # 에피소드 리플레이
│   └── hierarchical/             # 계층적 리플레이
├── algorithms/                   # 강화학습 알고리즘
│   ├── ddpg_algorithm.py         # DDPG 알고리즘
│   ├── ppo_algorithm.py          # PPO 알고리즘
│   ├── sac_algorithm.py          # SAC 알고리즘
│   └── td3_algorithm.py          # TD3 알고리즘
└── training/                     # 학습 관리
    ├── curriculum/               # 커리큘럼 학습
    ├── hyperparameter/           # 하이퍼파라미터 최적화
    └── evaluation/               # 성능 평가
```

## 🔧 **DDPG (Deep Deterministic Policy Gradient) 에이전트**

### 📦 **DDPG 거래 에이전트**

```python
# reinforcement-learning/agents/ddpg/ddpg_trading_agent.py
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input, Concatenate
from tensorflow.keras.optimizers import Adam
from collections import deque
import random
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class ActorNetwork(Model):
    """액터 네트워크 (정책 네트워크)"""
    
    def __init__(self, state_dim: int, action_dim: int, action_bound: float = 1.0):
        super(ActorNetwork, self).__init__()
        self.action_bound = action_bound
        
        # 네트워크 레이어
        self.dense1 = Dense(400, activation='relu', input_shape=(state_dim,))
        self.dense2 = Dense(300, activation='relu')
        self.dense3 = Dense(action_dim, activation='tanh')
        
        # 배치 정규화
        self.batch_norm1 = tf.keras.layers.BatchNormalization()
        self.batch_norm2 = tf.keras.layers.BatchNormalization()
        
        # 드롭아웃
        self.dropout1 = tf.keras.layers.Dropout(0.1)
        self.dropout2 = tf.keras.layers.Dropout(0.1)
    
    def call(self, state):
        x = self.dense1(state)
        x = self.batch_norm1(x)
        x = self.dropout1(x)
        
        x = self.dense2(x)
        x = self.batch_norm2(x)
        x = self.dropout2(x)
        
        action = self.dense3(x)
        return action * self.action_bound

class CriticNetwork(Model):
    """크리틱 네트워크 (가치 네트워크)"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super(CriticNetwork, self).__init__()
        
        # 상태 처리 레이어
        self.state_dense1 = Dense(400, activation='relu', input_shape=(state_dim,))
        self.state_dense2 = Dense(300, activation='relu')
        
        # 액션 처리 레이어
        self.action_dense1 = Dense(300, activation='relu', input_shape=(action_dim,))
        
        # 결합 레이어
        self.combined_dense1 = Dense(300, activation='relu')
        self.combined_dense2 = Dense(200, activation='relu')
        self.q_value = Dense(1, activation=None)
        
        # 배치 정규화
        self.batch_norm1 = tf.keras.layers.BatchNormalization()
        self.batch_norm2 = tf.keras.layers.BatchNormalization()
    
    def call(self, state, action):
        # 상태 처리
        state_out = self.state_dense1(state)
        state_out = self.batch_norm1(state_out)
        state_out = self.state_dense2(state_out)
        
        # 액션 처리
        action_out = self.action_dense1(action)
        
        # 결합
        combined = Concatenate()([state_out, action_out])
        combined = self.combined_dense1(combined)
        combined = self.batch_norm2(combined)
        combined = self.combined_dense2(combined)
        
        q_value = self.q_value(combined)
        return q_value

class DDPGTradingAgent:
    """DDPG 기반 거래 에이전트"""
    
    def __init__(self, state_dim: int, action_dim: int, 
                 learning_rate_actor: float = 0.0001,
                 learning_rate_critic: float = 0.001,
                 gamma: float = 0.99,
                 tau: float = 0.001,
                 memory_size: int = 100000,
                 batch_size: int = 64):
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate_actor = learning_rate_actor
        self.learning_rate_critic = learning_rate_critic
        self.gamma = gamma
        self.tau = tau
        self.batch_size = batch_size
        
        # 네트워크 초기화
        self.actor = ActorNetwork(state_dim, action_dim)
        self.critic = CriticNetwork(state_dim, action_dim)
        self.target_actor = ActorNetwork(state_dim, action_dim)
        self.target_critic = CriticNetwork(state_dim, action_dim)
        
        # 옵티마이저
        self.actor_optimizer = Adam(learning_rate=learning_rate_actor)
        self.critic_optimizer = Adam(learning_rate=learning_rate_critic)
        
        # 메모리
        self.memory = deque(maxlen=memory_size)
        
        # 타겟 네트워크 초기화
        self.update_target_networks(tau=1.0)
        
        # 노이즈 파라미터
        self.noise_std = 0.1
        self.noise_decay = 0.9999
        self.min_noise = 0.01
        
        logger.info(f"DDPG Trading Agent initialized: state_dim={state_dim}, action_dim={action_dim}")
    
    def get_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """액션 선택"""
        try:
            state = np.array(state).reshape(1, -1)
            action = self.actor(state).numpy()[0]
            
            if training:
                # 탐험을 위한 노이즈 추가
                noise = np.random.normal(0, self.noise_std, self.action_dim)
                action = np.clip(action + noise, -1, 1)
                
                # 노이즈 감소
                self.noise_std = max(self.min_noise, self.noise_std * self.noise_decay)
            
            return action
            
        except Exception as e:
            logger.error(f"Action selection failed: {e}")
            return np.zeros(self.action_dim)
    
    def remember(self, state: np.ndarray, action: np.ndarray, 
                reward: float, next_state: np.ndarray, done: bool):
        """경험 저장"""
        try:
            self.memory.append((state, action, reward, next_state, done))
        except Exception as e:
            logger.error(f"Memory storage failed: {e}")
    
    def train(self) -> Dict[str, float]:
        """네트워크 학습"""
        try:
            if len(self.memory) < self.batch_size:
                return {'actor_loss': 0.0, 'critic_loss': 0.0}
            
            # 배치 샘플링
            batch = random.sample(self.memory, self.batch_size)
            states = np.array([e[0] for e in batch])
            actions = np.array([e[1] for e in batch])
            rewards = np.array([e[2] for e in batch])
            next_states = np.array([e[3] for e in batch])
            dones = np.array([e[4] for e in batch])
            
            # 크리틱 네트워크 학습
            with tf.GradientTape() as tape:
                current_q_values = self.critic(states, actions)
                next_actions = self.target_actor(next_states)
                next_q_values = self.target_critic(next_states, next_actions)
                target_q_values = rewards + self.gamma * next_q_values * (1 - dones)
                critic_loss = tf.reduce_mean(tf.square(target_q_values - current_q_values))
            
            critic_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)
            self.critic_optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))
            
            # 액터 네트워크 학습
            with tf.GradientTape() as tape:
                current_actions = self.actor(states)
                actor_loss = -tf.reduce_mean(self.critic(states, current_actions))
            
            actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)
            self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))
            
            # 타겟 네트워크 업데이트
            self.update_target_networks()
            
            return {
                'actor_loss': float(actor_loss),
                'critic_loss': float(critic_loss),
                'noise_std': self.noise_std
            }
            
        except Exception as e:
            logger.error(f"Training failed: {e}")
            return {'actor_loss': 0.0, 'critic_loss': 0.0}
    
    def update_target_networks(self, tau: Optional[float] = None):
        """타겟 네트워크 업데이트"""
        try:
            if tau is None:
                tau = self.tau
            
            # 타겟 액터 업데이트
            for target_param, param in zip(self.target_actor.trainable_variables, 
                                         self.actor.trainable_variables):
                target_param.assign(tau * param + (1 - tau) * target_param)
            
            # 타겟 크리틱 업데이트
            for target_param, param in zip(self.target_critic.trainable_variables, 
                                         self.critic.trainable_variables):
                target_param.assign(tau * param + (1 - tau) * target_param)
                
        except Exception as e:
            logger.error(f"Target network update failed: {e}")
    
    def save_model(self, filepath: str):
        """모델 저장"""
        try:
            self.actor.save_weights(f"{filepath}_actor.h5")
            self.critic.save_weights(f"{filepath}_critic.h5")
            self.target_actor.save_weights(f"{filepath}_target_actor.h5")
            self.target_critic.save_weights(f"{filepath}_target_critic.h5")
            logger.info(f"Model saved to: {filepath}")
        except Exception as e:
            logger.error(f"Model save failed: {e}")
    
    def load_model(self, filepath: str):
        """모델 로드"""
        try:
            self.actor.load_weights(f"{filepath}_actor.h5")
            self.critic.load_weights(f"{filepath}_critic.h5")
            self.target_actor.load_weights(f"{filepath}_target_actor.h5")
            self.target_critic.load_weights(f"{filepath}_target_critic.h5")
            logger.info(f"Model loaded from: {filepath}")
        except Exception as e:
            logger.error(f"Model load failed: {e}")
```

## 🔧 **PPO (Proximal Policy Optimization) 에이전트**

### 📦 **PPO 거래 에이전트**

```python
# reinforcement-learning/agents/ppo/ppo_trading_agent.py
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from collections import deque
import random
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class PPONetwork(Model):
    """PPO 네트워크 (액터-크리틱)"""
    
    def __init__(self, state_dim: int, action_dim: int, action_bound: float = 1.0):
        super(PPONetwork, self).__init__()
        self.action_dim = action_dim
        self.action_bound = action_bound
        
        # 공통 레이어
        self.dense1 = Dense(256, activation='relu', input_shape=(state_dim,))
        self.dense2 = Dense(128, activation='relu')
        
        # 액터 레이어 (정책)
        self.actor_dense = Dense(64, activation='relu')
        self.actor_output = Dense(action_dim, activation='tanh')
        
        # 크리틱 레이어 (가치)
        self.critic_dense = Dense(64, activation='relu')
        self.critic_output = Dense(1, activation=None)
        
        # 액션 표준편차 (학습 가능한 파라미터)
        self.action_std = tf.Variable(0.5, trainable=True)
    
    def call(self, state):
        # 공통 레이어
        x = self.dense1(state)
        x = self.dense2(x)
        
        # 액터 (정책)
        actor_x = self.actor_dense(x)
        action_mean = self.actor_output(actor_x) * self.action_bound
        
        # 크리틱 (가치)
        critic_x = self.critic_dense(x)
        value = self.critic_output(critic_x)
        
        return action_mean, value
    
    def get_action_and_log_prob(self, state: np.ndarray) -> Tuple[np.ndarray, float]:
        """액션과 로그 확률 반환"""
        state = np.array(state).reshape(1, -1)
        action_mean, value = self(state)
        
        # 정규분포에서 액션 샘플링
        action_std = tf.exp(self.action_std)
        action_dist = tf.random.normal(action_mean.shape, action_mean, action_std)
        action = tf.clip_by_value(action_dist, -self.action_bound, self.action_bound)
        
        # 로그 확률 계산
        log_prob = self._log_prob(action, action_mean, action_std)
        
        return action.numpy()[0], log_prob.numpy()[0], value.numpy()[0]
    
    def _log_prob(self, action, mean, std):
        """로그 확률 계산"""
        return -0.5 * tf.square((action - mean) / std) - tf.math.log(std) - 0.5 * tf.math.log(2 * np.pi)

class PPOTradingAgent:
    """PPO 기반 거래 에이전트"""
    
    def __init__(self, state_dim: int, action_dim: int,
                 learning_rate: float = 0.0003,
                 gamma: float = 0.99,
                 gae_lambda: float = 0.95,
                 clip_ratio: float = 0.2,
                 value_coef: float = 0.5,
                 entropy_coef: float = 0.01,
                 max_grad_norm: float = 0.5,
                 update_epochs: int = 10):
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_ratio = clip_ratio
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.update_epochs = update_epochs
        
        # 네트워크
        self.network = PPONetwork(state_dim, action_dim)
        self.optimizer = Adam(learning_rate=learning_rate)
        
        # 메모리
        self.memory = deque()
        
        # 통계
        self.episode_rewards = []
        self.episode_lengths = []
        
        logger.info(f"PPO Trading Agent initialized: state_dim={state_dim}, action_dim={action_dim}")
    
    def get_action(self, state: np.ndarray) -> Tuple[np.ndarray, float, float]:
        """액션 선택"""
        try:
            action, log_prob, value = self.network.get_action_and_log_prob(state)
            return action, log_prob, value
        except Exception as e:
            logger.error(f"Action selection failed: {e}")
            return np.zeros(self.action_dim), 0.0, 0.0
    
    def remember(self, state: np.ndarray, action: np.ndarray, reward: float,
                next_state: np.ndarray, done: bool, log_prob: float, value: float):
        """경험 저장"""
        try:
            self.memory.append((state, action, reward, next_state, done, log_prob, value))
        except Exception as e:
            logger.error(f"Memory storage failed: {e}")
    
    def compute_gae(self, rewards: List[float], values: List[float], 
                   dones: List[bool]) -> List[float]:
        """Generalized Advantage Estimation 계산"""
        try:
            advantages = []
            gae = 0
            
            for i in reversed(range(len(rewards))):
                if i == len(rewards) - 1:
                    next_value = 0
                else:
                    next_value = values[i + 1]
                
                delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i]
                gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae
                advantages.insert(0, gae)
            
            return advantages
            
        except Exception as e:
            logger.error(f"GAE computation failed: {e}")
            return [0.0] * len(rewards)
    
    def train(self) -> Dict[str, float]:
        """네트워크 학습"""
        try:
            if len(self.memory) < 100:  # 최소 배치 크기
                return {'policy_loss': 0.0, 'value_loss': 0.0, 'entropy_loss': 0.0}
            
            # 메모리에서 데이터 추출
            states = np.array([e[0] for e in self.memory])
            actions = np.array([e[1] for e in self.memory])
            rewards = np.array([e[2] for e in self.memory])
            dones = np.array([e[4] for e in self.memory])
            old_log_probs = np.array([e[5] for e in self.memory])
            old_values = np.array([e[6] for e in self.memory])
            
            # GAE 계산
            advantages = self.compute_gae(rewards.tolist(), old_values.tolist(), dones.tolist())
            advantages = np.array(advantages)
            
            # 정규화
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # 타겟 가치 계산
            target_values = advantages + old_values
            
            total_policy_loss = 0.0
            total_value_loss = 0.0
            total_entropy_loss = 0.0
            
            # 여러 에포크에 걸쳐 학습
            for epoch in range(self.update_epochs):
                # 배치 인덱스 생성
                indices = np.random.permutation(len(states))
                
                for start_idx in range(0, len(states), 64):  # 배치 크기 64
                    end_idx = min(start_idx + 64, len(states))
                    batch_indices = indices[start_idx:end_idx]
                    
                    batch_states = states[batch_indices]
                    batch_actions = actions[batch_indices]
                    batch_advantages = advantages[batch_indices]
                    batch_target_values = target_values[batch_indices]
                    batch_old_log_probs = old_log_probs[batch_indices]
                    
                    with tf.GradientTape() as tape:
                        # 현재 정책으로 액션과 가치 계산
                        action_means, values = self.network(batch_states)
                        
                        # 로그 확률 계산
                        action_std = tf.exp(self.network.action_std)
                        log_probs = -0.5 * tf.square((batch_actions - action_means) / action_std) - \
                                   tf.math.log(action_std) - 0.5 * tf.math.log(2 * np.pi)
                        log_probs = tf.reduce_sum(log_probs, axis=1)
                        
                        # 비율 계산
                        ratio = tf.exp(log_probs - batch_old_log_probs)
                        
                        # 클리핑된 목적 함수
                        surr1 = ratio * batch_advantages
                        surr2 = tf.clip_by_value(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_advantages
                        policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))
                        
                        # 가치 손실
                        value_loss = self.value_coef * tf.reduce_mean(tf.square(values - batch_target_values))
                        
                        # 엔트로피 손실
                        entropy_loss = -self.entropy_coef * tf.reduce_mean(log_probs)
                        
                        # 총 손실
                        total_loss = policy_loss + value_loss + entropy_loss
                    
                    # 그래디언트 계산 및 적용
                    gradients = tape.gradient(total_loss, self.network.trainable_variables)
                    gradients, _ = tf.clip_by_global_norm(gradients, self.max_grad_norm)
                    self.optimizer.apply_gradients(zip(gradients, self.network.trainable_variables))
                    
                    total_policy_loss += float(policy_loss)
                    total_value_loss += float(value_loss)
                    total_entropy_loss += float(entropy_loss)
            
            # 메모리 클리어
            self.memory.clear()
            
            avg_policy_loss = total_policy_loss / (self.update_epochs * (len(states) // 64))
            avg_value_loss = total_value_loss / (self.update_epochs * (len(states) // 64))
            avg_entropy_loss = total_entropy_loss / (self.update_epochs * (len(states) // 64))
            
            return {
                'policy_loss': avg_policy_loss,
                'value_loss': avg_value_loss,
                'entropy_loss': avg_entropy_loss
            }
            
        except Exception as e:
            logger.error(f"Training failed: {e}")
            return {'policy_loss': 0.0, 'value_loss': 0.0, 'entropy_loss': 0.0}
    
    def save_model(self, filepath: str):
        """모델 저장"""
        try:
            self.network.save_weights(f"{filepath}.h5")
            logger.info(f"Model saved to: {filepath}")
        except Exception as e:
            logger.error(f"Model save failed: {e}")
    
    def load_model(self, filepath: str):
        """모델 로드"""
        try:
            self.network.load_weights(f"{filepath}.h5")
            logger.info(f"Model loaded from: {filepath}")
        except Exception as e:
            logger.error(f"Model load failed: {e}")
```

## 🔧 **다중 에이전트 시스템**

### 📦 **다중 에이전트 거래 시스템**

```python
# reinforcement-learning/agents/multi-agent/multi_agent_trading_system.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import logging
import asyncio

logger = logging.getLogger(__name__)

@dataclass
class AgentConfig:
    """에이전트 설정"""
    agent_id: str
    agent_type: str  # 'ddpg', 'ppo', 'sac', 'td3'
    trading_pairs: List[str]
    risk_tolerance: float  # 0.0 (보수적) ~ 1.0 (공격적)
    capital_allocation: float  # 총 자본 대비 할당 비율
    time_horizon: str  # 'short', 'medium', 'long'

@dataclass
class MarketState:
    """시장 상태"""
    timestamp: datetime
    prices: Dict[str, float]
    volumes: Dict[str, float]
    technical_indicators: Dict[str, Dict[str, float]]
    market_sentiment: float  # -1.0 (매우 부정적) ~ 1.0 (매우 긍정적)
    volatility: float

class MultiAgentTradingSystem:
    """다중 에이전트 거래 시스템"""
    
    def __init__(self, agents_config: List[AgentConfig], 
                 total_capital: float = 100000.0):
        self.agents_config = agents_config
        self.total_capital = total_capital
        self.agents = {}
        self.agent_states = {}
        self.market_state = None
        self.portfolio = {}
        self.trading_history = []
        
        # 에이전트 초기화
        self._initialize_agents()
        
        logger.info(f"Multi-agent trading system initialized with {len(agents_config)} agents")
    
    def _initialize_agents(self):
        """에이전트 초기화"""
        try:
            for config in self.agents_config:
                if config.agent_type == 'ddpg':
                    from .ddpg.ddpg_trading_agent import DDPGTradingAgent
                    state_dim = self._get_state_dimension(config.trading_pairs)
                    action_dim = len(config.trading_pairs) * 2  # buy/sell for each pair
                    self.agents[config.agent_id] = DDPGTradingAgent(state_dim, action_dim)
                
                elif config.agent_type == 'ppo':
                    from .ppo.ppo_trading_agent import PPOTradingAgent
                    state_dim = self._get_state_dimension(config.trading_pairs)
                    action_dim = len(config.trading_pairs) * 2
                    self.agents[config.agent_id] = PPOTradingAgent(state_dim, action_dim)
                
                # 초기 상태 설정
                self.agent_states[config.agent_id] = {
                    'capital': self.total_capital * config.capital_allocation,
                    'positions': {pair: 0.0 for pair in config.trading_pairs},
                    'performance': {'returns': [], 'sharpe_ratio': 0.0, 'max_drawdown': 0.0}
                }
                
                logger.info(f"Agent {config.agent_id} initialized: {config.agent_type}")
                
        except Exception as e:
            logger.error(f"Agent initialization failed: {e}")
            raise
    
    def _get_state_dimension(self, trading_pairs: List[str]) -> int:
        """상태 차원 계산"""
        # 기본 특성: 가격, 거래량, 기술적 지표
        base_features = 10  # OHLCV + 5 technical indicators
        return len(trading_pairs) * base_features + 5  # + 5 global market features
    
    def update_market_state(self, market_data: Dict[str, any]):
        """시장 상태 업데이트"""
        try:
            self.market_state = MarketState(
                timestamp=market_data['timestamp'],
                prices=market_data['prices'],
                volumes=market_data['volumes'],
                technical_indicators=market_data['technical_indicators'],
                market_sentiment=market_data['market_sentiment'],
                volatility=market_data['volatility']
            )
            
            logger.debug(f"Market state updated: {self.market_state.timestamp}")
            
        except Exception as e:
            logger.error(f"Market state update failed: {e}")
    
    def get_agent_state(self, agent_id: str) -> np.ndarray:
        """에이전트별 상태 생성"""
        try:
            config = next(c for c in self.agents_config if c.agent_id == agent_id)
            state_features = []
            
            # 거래쌍별 특성
            for pair in config.trading_pairs:
                if pair in self.market_state.prices:
                    price = self.market_state.prices[pair]
                    volume = self.market_state.volumes.get(pair, 0.0)
                    indicators = self.market_state.technical_indicators.get(pair, {})
                    
                    # 기본 특성
                    state_features.extend([
                        price,
                        volume,
                        indicators.get('rsi', 50.0),
                        indicators.get('macd', 0.0),
                        indicators.get('bollinger_position', 0.5),
                        indicators.get('sma_ratio', 1.0),
                        indicators.get('ema_ratio', 1.0)
                    ])
            
            # 글로벌 시장 특성
            state_features.extend([
                self.market_state.market_sentiment,
                self.market_state.volatility,
                self.agent_states[agent_id]['capital'] / self.total_capital,
                len([p for p in self.agent_states[agent_id]['positions'].values() if p > 0]),
                config.risk_tolerance
            ])
            
            return np.array(state_features)
            
        except Exception as e:
            logger.error(f"Agent state generation failed: {e}")
            return np.zeros(self._get_state_dimension(config.trading_pairs))
    
    async def execute_trading_cycle(self) -> Dict[str, any]:
        """거래 사이클 실행"""
        try:
            if self.market_state is None:
                raise ValueError("Market state not available")
            
            trading_results = {}
            
            # 각 에이전트의 거래 결정
            for agent_id, agent in self.agents.items():
                try:
                    # 상태 생성
                    state = self.get_agent_state(agent_id)
                    
                    # 액션 선택
                    if hasattr(agent, 'get_action'):
                        action = agent.get_action(state)
                    else:
                        action = np.zeros(len(agent.agent_config.trading_pairs) * 2)
                    
                    # 거래 실행
                    trades = self._execute_agent_trades(agent_id, action)
                    
                    # 보상 계산
                    reward = self._calculate_agent_reward(agent_id, trades)
                    
                    # 다음 상태
                    next_state = self.get_agent_state(agent_id)
                    
                    # 경험 저장
                    if hasattr(agent, 'remember'):
                        agent.remember(state, action, reward, next_state, False)
                    
                    trading_results[agent_id] = {
                        'action': action,
                        'trades': trades,
                        'reward': reward,
                        'current_capital': self.agent_states[agent_id]['capital']
                    }
                    
                except Exception as e:
                    logger.error(f"Agent {agent_id} trading cycle failed: {e}")
                    trading_results[agent_id] = {'error': str(e)}
            
            # 포트폴리오 업데이트
            self._update_portfolio()
            
            # 거래 히스토리 저장
            self.trading_history.append({
                'timestamp': self.market_state.timestamp,
                'results': trading_results,
                'portfolio_value': self._get_total_portfolio_value()
            })
            
            logger.info(f"Trading cycle completed: {len(trading_results)} agents")
            return trading_results
            
        except Exception as e:
            logger.error(f"Trading cycle failed: {e}")
            return {}
    
    def _execute_agent_trades(self, agent_id: str, action: np.ndarray) -> List[Dict]:
        """에이전트 거래 실행"""
        try:
            config = next(c for c in self.agents_config if c.agent_id == agent_id)
            trades = []
            
            # 액션을 거래로 변환
            for i, pair in enumerate(config.trading_pairs):
                buy_action = action[i * 2]
                sell_action = action[i * 2 + 1]
                
                current_price = self.market_state.prices.get(pair, 0.0)
                if current_price <= 0:
                    continue
                
                # 매수 거래
                if buy_action > 0.1:  # 임계값
                    trade_amount = buy_action * self.agent_states[agent_id]['capital'] * 0.1
                    quantity = trade_amount / current_price
                    
                    if trade_amount <= self.agent_states[agent_id]['capital']:
                        self.agent_states[agent_id]['capital'] -= trade_amount
                        self.agent_states[agent_id]['positions'][pair] += quantity
                        
                        trades.append({
                            'type': 'buy',
                            'pair': pair,
                            'quantity': quantity,
                            'price': current_price,
                            'amount': trade_amount
                        })
                
                # 매도 거래
                elif sell_action > 0.1:
                    current_position = self.agent_states[agent_id]['positions'][pair]
                    if current_position > 0:
                        sell_quantity = min(current_position, sell_action * current_position)
                        trade_amount = sell_quantity * current_price
                        
                        self.agent_states[agent_id]['capital'] += trade_amount
                        self.agent_states[agent_id]['positions'][pair] -= sell_quantity
                        
                        trades.append({
                            'type': 'sell',
                            'pair': pair,
                            'quantity': sell_quantity,
                            'price': current_price,
                            'amount': trade_amount
                        })
            
            return trades
            
        except Exception as e:
            logger.error(f"Trade execution failed: {e}")
            return []
    
    def _calculate_agent_reward(self, agent_id: str, trades: List[Dict]) -> float:
        """에이전트 보상 계산"""
        try:
            config = next(c for c in self.agents_config if c.agent_id == agent_id)
            
            # 수익률 계산
            current_value = self.agent_states[agent_id]['capital']
            for pair, position in self.agent_states[agent_id]['positions'].items():
                if position > 0 and pair in self.market_state.prices:
                    current_value += position * self.market_state.prices[pair]
            
            initial_capital = self.total_capital * config.capital_allocation
            returns = (current_value - initial_capital) / initial_capital
            
            # 리스크 조정 보상
            risk_penalty = -abs(returns) * (1 - config.risk_tolerance)
            
            # 거래 비용
            transaction_cost = sum(trade['amount'] * 0.001 for trade in trades)  # 0.1% 수수료
            
            # 최종 보상
            reward = returns + risk_penalty - transaction_cost
            
            # 성능 기록
            self.agent_states[agent_id]['performance']['returns'].append(returns)
            
            return reward
            
        except Exception as e:
            logger.error(f"Reward calculation failed: {e}")
            return 0.0
    
    def _update_portfolio(self):
        """포트폴리오 업데이트"""
        try:
            total_value = 0.0
            
            for agent_id, state in self.agent_states.items():
                agent_value = state['capital']
                
                for pair, position in state['positions'].items():
                    if position > 0 and pair in self.market_state.prices:
                        agent_value += position * self.market_state.prices[pair]
                
                total_value += agent_value
                self.portfolio[agent_id] = agent_value
            
            self.portfolio['total'] = total_value
            
        except Exception as e:
            logger.error(f"Portfolio update failed: {e}")
    
    def _get_total_portfolio_value(self) -> float:
        """총 포트폴리오 가치"""
        return self.portfolio.get('total', 0.0)
    
    def get_performance_summary(self) -> Dict[str, any]:
        """성능 요약"""
        try:
            summary = {
                'total_agents': len(self.agents),
                'total_portfolio_value': self._get_total_portfolio_value(),
                'total_return': (self._get_total_portfolio_value() - self.total_capital) / self.total_capital,
                'agent_performances': {}
            }
            
            for agent_id, state in self.agent_states.items():
                returns = state['performance']['returns']
                if returns:
                    sharpe_ratio = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)
                    max_drawdown = self._calculate_max_drawdown(returns)
                    
                    summary['agent_performances'][agent_id] = {
                        'total_return': np.sum(returns),
                        'sharpe_ratio': sharpe_ratio,
                        'max_drawdown': max_drawdown,
                        'current_capital': state['capital']
                    }
            
            return summary
            
        except Exception as e:
            logger.error(f"Performance summary failed: {e}")
            return {}
    
    def _calculate_max_drawdown(self, returns: List[float]) -> float:
        """최대 낙폭 계산"""
        try:
            cumulative = np.cumprod(1 + np.array(returns))
            running_max = np.maximum.accumulate(cumulative)
            drawdown = (cumulative - running_max) / running_max
            return float(np.min(drawdown))
        except Exception as e:
            logger.error(f"Max drawdown calculation failed: {e}")
            return 0.0
    
    async def train_all_agents(self):
        """모든 에이전트 학습"""
        try:
            training_results = {}
            
            for agent_id, agent in self.agents.items():
                try:
                    if hasattr(agent, 'train'):
                        result = agent.train()
                        training_results[agent_id] = result
                        logger.info(f"Agent {agent_id} training completed")
                except Exception as e:
                    logger.error(f"Agent {agent_id} training failed: {e}")
                    training_results[agent_id] = {'error': str(e)}
            
            return training_results
            
        except Exception as e:
            logger.error(f"Agent training failed: {e}")
            return {}
```

## 🎯 **다음 단계**

### 📋 **완료된 작업**
- ✅ DDPG 거래 에이전트
- ✅ PPO 거래 에이전트
- ✅ 다중 에이전트 시스템
- ✅ 액터-크리틱 네트워크
- ✅ 경험 리플레이 시스템

### 🔄 **진행 중인 작업**
- 🔄 SAC (Soft Actor-Critic) 에이전트
- 🔄 TD3 (Twin Delayed DDPG) 에이전트
- 🔄 커리큘럼 학습 시스템

### ⏳ **다음 단계**
1. **Phase 3.3 감정 분석** 문서 생성
2. **Phase 3.4 포트폴리오 최적화** 문서 생성
3. **Phase 3.5 리스크 관리 AI** 문서 생성

---

**마지막 업데이트**: 2024-01-31
**다음 업데이트**: 2024-02-01 (Phase 3.3 감정 분석)
**강화학습 목표**: > 15% 연간 수익률, > 1.5 샤프 비율, < 10% 최대 낙폭
**에이전트 성능**: 다중 에이전트 협력, 실시간 학습, 적응형 정책 업데이트 