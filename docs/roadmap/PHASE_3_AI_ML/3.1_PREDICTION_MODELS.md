# 🔮 Phase 3.1: 고급 예측 모델 시스템

## 📋 **개요**

### 🎯 **목표**
- **다중 모델 앙상블**: LSTM, Transformer, GRU, CNN 앙상블
- **시계열 분석**: ARIMA, Prophet, VAR 모델 통합
- **특성 엔지니어링**: 기술적 지표, 시장 데이터, 온체인 데이터
- **실시간 예측**: < 1초 응답 시간, 실시간 모델 업데이트
- **예측 정확도**: > 70% (앙상블 모델 기준)

### 📊 **성능 목표**
- **단일 모델 정확도**: > 65%
- **앙상블 모델 정확도**: > 70%
- **예측 지연 시간**: < 100ms
- **모델 학습 시간**: < 30분 (일일 재학습)
- **메모리 사용량**: < 4GB per model

## 🏗️ **예측 모델 아키텍처**

### 📁 **예측 모델 구조**
```
prediction-models/
├── ensemble/                      # 앙상블 모델
│   ├── voting/                   # 투표 기반 앙상블
│   ├── stacking/                 # 스태킹 앙상블
│   ├── blending/                 # 블렌딩 앙상블
│   └── dynamic/                  # 동적 앙상블
├── deep-learning/                # 딥러닝 모델
│   ├── lstm/                     # LSTM 모델
│   ├── gru/                      # GRU 모델
│   ├── transformer/              # Transformer 모델
│   ├── cnn/                      # CNN 모델
│   └── hybrid/                   # 하이브리드 모델
├── statistical/                  # 통계 모델
│   ├── arima/                    # ARIMA 모델
│   ├── var/                      # VAR 모델
│   ├── prophet/                  # Prophet 모델
│   └── exponential-smoothing/    # 지수 평활
├── feature-engineering/          # 특성 엔지니어링
│   ├── technical-indicators/     # 기술적 지표
│   ├── market-features/          # 시장 특성
│   ├── onchain-features/         # 온체인 특성
│   └── sentiment-features/       # 감정 특성
└── model-management/             # 모델 관리
    ├── versioning/               # 모델 버전 관리
    ├── deployment/               # 모델 배포
    ├── monitoring/               # 모델 모니터링
    └── optimization/             # 모델 최적화
```

## 🔧 **앙상블 모델 시스템**

### 📦 **동적 앙상블 모델**

```python
# prediction-models/ensemble/dynamic_ensemble.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from sklearn.metrics import mean_squared_error, mean_absolute_error
import joblib

logger = logging.getLogger(__name__)

@dataclass
class ModelPrediction:
    """모델 예측 결과"""
    model_name: str
    prediction: float
    confidence: float
    timestamp: datetime
    features: Dict[str, float]

@dataclass
class EnsembleWeights:
    """앙상블 가중치"""
    model_name: str
    weight: float
    performance_score: float
    last_updated: datetime

class DynamicEnsemble:
    """동적 앙상블 모델"""
    
    def __init__(self, models: Dict[str, Callable], 
                 weight_update_frequency: int = 24,  # 24시간마다 가중치 업데이트
                 performance_window: int = 168):     # 7일 성능 윈도우
        self.models = models
        self.weight_update_frequency = weight_update_frequency
        self.performance_window = performance_window
        
        # 앙상블 가중치
        self.ensemble_weights = {}
        self.model_performances = {}
        self.prediction_history = []
        
        # 초기 가중치 설정 (균등 분배)
        initial_weight = 1.0 / len(models)
        for model_name in models.keys():
            self.ensemble_weights[model_name] = EnsembleWeights(
                model_name=model_name,
                weight=initial_weight,
                performance_score=0.0,
                last_updated=datetime.now()
            )
        
        logger.info(f"Initialized dynamic ensemble with {len(models)} models")
    
    def predict(self, features: Dict[str, float]) -> Dict[str, any]:
        """앙상블 예측"""
        try:
            predictions = []
            weighted_sum = 0.0
            total_weight = 0.0
            
            # 각 모델의 예측 수집
            for model_name, model_func in self.models.items():
                try:
                    # 모델 예측
                    prediction = model_func(features)
                    
                    # 가중치 적용
                    weight = self.ensemble_weights[model_name].weight
                    weighted_prediction = prediction * weight
                    
                    predictions.append(ModelPrediction(
                        model_name=model_name,
                        prediction=prediction,
                        confidence=self._calculate_confidence(model_name),
                        timestamp=datetime.now(),
                        features=features
                    ))
                    
                    weighted_sum += weighted_prediction
                    total_weight += weight
                    
                except Exception as e:
                    logger.error(f"Model {model_name} prediction failed: {e}")
                    continue
            
            if not predictions:
                raise ValueError("No valid predictions from any model")
            
            # 가중 평균 계산
            ensemble_prediction = weighted_sum / total_weight if total_weight > 0 else 0.0
            
            # 앙상블 신뢰도 계산
            ensemble_confidence = self._calculate_ensemble_confidence(predictions)
            
            # 예측 히스토리에 저장
            self.prediction_history.append({
                'timestamp': datetime.now(),
                'ensemble_prediction': ensemble_prediction,
                'ensemble_confidence': ensemble_confidence,
                'individual_predictions': predictions,
                'features': features
            })
            
            result = {
                'prediction': ensemble_prediction,
                'confidence': ensemble_confidence,
                'individual_predictions': [
                    {
                        'model': p.model_name,
                        'prediction': p.prediction,
                        'confidence': p.confidence,
                        'weight': self.ensemble_weights[p.model_name].weight
                    }
                    for p in predictions
                ],
                'timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"Ensemble prediction: {ensemble_prediction:.4f}, confidence: {ensemble_confidence:.3f}")
            return result
            
        except Exception as e:
            logger.error(f"Ensemble prediction failed: {e}")
            raise
    
    def update_weights(self, actual_values: List[float], predicted_values: List[float]):
        """앙상블 가중치 업데이트"""
        try:
            if len(actual_values) != len(predicted_values):
                raise ValueError("Actual and predicted values must have same length")
            
            # 각 모델의 성능 계산
            model_performances = {}
            
            for model_name in self.models.keys():
                # 해당 모델의 예측값 추출
                model_predictions = []
                for pred in self.prediction_history[-len(actual_values):]:
                    for individual_pred in pred['individual_predictions']:
                        if individual_pred['model'] == model_name:
                            model_predictions.append(individual_pred['prediction'])
                            break
                
                if len(model_predictions) == len(actual_values):
                    # 성능 메트릭 계산
                    mse = mean_squared_error(actual_values, model_predictions)
                    mae = mean_absolute_error(actual_values, model_predictions)
                    
                    # 방향 정확도
                    direction_accuracy = np.mean(
                        np.sign(np.diff(actual_values)) == 
                        np.sign(np.diff(model_predictions))
                    )
                    
                    # 종합 성능 점수 (낮을수록 좋음)
                    performance_score = mse * 0.5 + mae * 0.3 + (1 - direction_accuracy) * 0.2
                    
                    model_performances[model_name] = performance_score
            
            # 가중치 업데이트 (성능이 좋을수록 높은 가중치)
            if model_performances:
                total_performance = sum(model_performances.values())
                
                for model_name, performance in model_performances.items():
                    # 성능 기반 가중치 계산
                    inverse_performance = 1.0 / (performance + 1e-8)  # 0으로 나누기 방지
                    new_weight = inverse_performance / sum(1.0 / (p + 1e-8) for p in model_performances.values())
                    
                    # 가중치 업데이트
                    self.ensemble_weights[model_name].weight = new_weight
                    self.ensemble_weights[model_name].performance_score = performance
                    self.ensemble_weights[model_name].last_updated = datetime.now()
                
                logger.info(f"Ensemble weights updated: {model_performances}")
            
        except Exception as e:
            logger.error(f"Weight update failed: {e}")
            raise
    
    def _calculate_confidence(self, model_name: str) -> float:
        """모델별 신뢰도 계산"""
        try:
            # 최근 성능 기반 신뢰도
            if model_name in self.ensemble_weights:
                performance_score = self.ensemble_weights[model_name].performance_score
                # 성능 점수를 0-1 범위로 정규화
                confidence = max(0.0, min(1.0, 1.0 - performance_score))
                return confidence
            
            return 0.5  # 기본 신뢰도
            
        except Exception as e:
            logger.error(f"Confidence calculation failed: {e}")
            return 0.5
    
    def _calculate_ensemble_confidence(self, predictions: List[ModelPrediction]) -> float:
        """앙상블 신뢰도 계산"""
        try:
            if not predictions:
                return 0.0
            
            # 가중 평균 신뢰도
            weighted_confidence = 0.0
            total_weight = 0.0
            
            for pred in predictions:
                weight = self.ensemble_weights[pred.model_name].weight
                weighted_confidence += pred.confidence * weight
                total_weight += weight
            
            ensemble_confidence = weighted_confidence / total_weight if total_weight > 0 else 0.0
            
            # 예측 일관성 고려
            if len(predictions) > 1:
                predictions_values = [p.prediction for p in predictions]
                prediction_std = np.std(predictions_values)
                prediction_mean = np.mean(predictions_values)
                
                # 표준편차가 작을수록 높은 신뢰도
                consistency_factor = max(0.0, 1.0 - prediction_std / (prediction_mean + 1e-8))
                ensemble_confidence *= consistency_factor
            
            return max(0.0, min(1.0, ensemble_confidence))
            
        except Exception as e:
            logger.error(f"Ensemble confidence calculation failed: {e}")
            return 0.5
    
    def get_model_performance_summary(self) -> Dict[str, any]:
        """모델 성능 요약"""
        try:
            summary = {
                'total_models': len(self.models),
                'active_models': len([w for w in self.ensemble_weights.values() if w.weight > 0.01]),
                'last_weight_update': max([w.last_updated for w in self.ensemble_weights.values()]).isoformat(),
                'model_performances': {}
            }
            
            for model_name, weights in self.ensemble_weights.items():
                summary['model_performances'][model_name] = {
                    'weight': weights.weight,
                    'performance_score': weights.performance_score,
                    'last_updated': weights.last_updated.isoformat()
                }
            
            return summary
            
        except Exception as e:
            logger.error(f"Performance summary failed: {e}")
            return {}
    
    def save_ensemble(self, filepath: str):
        """앙상블 모델 저장"""
        try:
            ensemble_data = {
                'ensemble_weights': self.ensemble_weights,
                'model_performances': self.model_performances,
                'prediction_history': self.prediction_history[-1000:]  # 최근 1000개만 저장
            }
            
            joblib.dump(ensemble_data, filepath)
            logger.info(f"Ensemble saved to: {filepath}")
            
        except Exception as e:
            logger.error(f"Ensemble save failed: {e}")
            raise
    
    def load_ensemble(self, filepath: str):
        """앙상블 모델 로드"""
        try:
            ensemble_data = joblib.load(filepath)
            
            self.ensemble_weights = ensemble_data['ensemble_weights']
            self.model_performances = ensemble_data.get('model_performances', {})
            self.prediction_history = ensemble_data.get('prediction_history', [])
            
            logger.info(f"Ensemble loaded from: {filepath}")
            
        except Exception as e:
            logger.error(f"Ensemble load failed: {e}")
            raise
```

### 📦 **스태킹 앙상블**

```python
# prediction-models/ensemble/stacking_ensemble.py
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.model_selection import TimeSeriesSplit
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class StackingEnsemble:
    """스태킹 앙상블 모델"""
    
    def __init__(self, base_models: Dict[str, any], meta_model: any = None):
        self.base_models = base_models
        self.meta_model = meta_model or Ridge(alpha=1.0)
        self.trained_base_models = {}
        self.trained_meta_model = None
        
        logger.info(f"Initialized stacking ensemble with {len(base_models)} base models")
    
    def fit(self, X: np.ndarray, y: np.ndarray, cv_folds: int = 5):
        """스태킹 앙상블 학습"""
        try:
            # 시계열 교차 검증
            tscv = TimeSeriesSplit(n_splits=cv_folds)
            
            # 베이스 모델 예측을 저장할 배열
            base_predictions = np.zeros((len(X), len(self.base_models)))
            
            # 각 베이스 모델 학습 및 예측
            for i, (model_name, model) in enumerate(self.base_models.items()):
                logger.info(f"Training base model: {model_name}")
                
                # 교차 검증을 통한 예측
                cv_predictions = []
                
                for train_idx, val_idx in tscv.split(X):
                    X_train, X_val = X[train_idx], X[val_idx]
                    y_train, y_val = y[train_idx], y[val_idx]
                    
                    # 모델 학습
                    model.fit(X_train, y_train)
                    
                    # 검증 세트 예측
                    val_pred = model.predict(X_val)
                    cv_predictions.extend(val_pred)
                
                # 전체 데이터로 최종 모델 학습
                final_model = type(model)()
                final_model.fit(X, y)
                self.trained_base_models[model_name] = final_model
                
                # 교차 검증 예측을 base_predictions에 저장
                base_predictions[:, i] = cv_predictions
            
            # 메타 모델 학습
            logger.info("Training meta model")
            self.trained_meta_model = self.meta_model.fit(base_predictions, y)
            
            logger.info("Stacking ensemble training completed")
            
        except Exception as e:
            logger.error(f"Stacking ensemble training failed: {e}")
            raise
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """스태킹 앙상블 예측"""
        try:
            if not self.trained_base_models or self.trained_meta_model is None:
                raise ValueError("Model not trained. Call fit() first.")
            
            # 베이스 모델 예측
            base_predictions = np.zeros((len(X), len(self.base_models)))
            
            for i, (model_name, model) in enumerate(self.trained_base_models.items()):
                base_predictions[:, i] = model.predict(X)
            
            # 메타 모델 예측
            ensemble_prediction = self.trained_meta_model.predict(base_predictions)
            
            logger.info(f"Stacking ensemble prediction completed")
            return ensemble_prediction
            
        except Exception as e:
            logger.error(f"Stacking ensemble prediction failed: {e}")
            raise
    
    def get_feature_importance(self) -> Dict[str, float]:
        """메타 모델의 특성 중요도 (베이스 모델 가중치)"""
        try:
            if self.trained_meta_model is None:
                return {}
            
            if hasattr(self.trained_meta_model, 'coef_'):
                # 선형 모델의 경우 계수 사용
                importance = {}
                for i, model_name in enumerate(self.base_models.keys()):
                    importance[model_name] = abs(self.trained_meta_model.coef_[i])
                
                # 정규화
                total_importance = sum(importance.values())
                if total_importance > 0:
                    importance = {k: v / total_importance for k, v in importance.items()}
                
                return importance
            
            elif hasattr(self.trained_meta_model, 'feature_importances_'):
                # 트리 기반 모델의 경우 특성 중요도 사용
                importance = {}
                for i, model_name in enumerate(self.base_models.keys()):
                    importance[model_name] = self.trained_meta_model.feature_importances_[i]
                
                return importance
            
            else:
                return {}
                
        except Exception as e:
            logger.error(f"Feature importance calculation failed: {e}")
            return {}
```

## 🔧 **고급 딥러닝 모델**

### 📦 **GRU 기반 예측 모델**

```python
# prediction-models/deep-learning/gru/gru_predictor.py
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class GRUPredictor:
    """GRU 기반 가격 예측 모델"""
    
    def __init__(self, sequence_length: int = 60, prediction_horizon: int = 1,
                 gru_units: List[int] = [128, 64, 32], dropout_rate: float = 0.2):
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.gru_units = gru_units
        self.dropout_rate = dropout_rate
        self.model = None
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.feature_columns = [
            'open', 'high', 'low', 'close', 'volume',
            'rsi', 'macd', 'bollinger_upper', 'bollinger_lower',
            'sma_20', 'sma_50', 'ema_12', 'ema_26',
            'atr', 'stoch_k', 'stoch_d', 'williams_r'
        ]
        
        logger.info(f"Initialized GRU predictor: units={gru_units}, dropout={dropout_rate}")
    
    def build_model(self, input_shape: Tuple[int, int]) -> Sequential:
        """GRU 모델 구축"""
        model = Sequential()
        
        # 첫 번째 GRU 레이어
        model.add(GRU(
            units=self.gru_units[0],
            return_sequences=True,
            input_shape=input_shape,
            recurrent_dropout=self.dropout_rate
        ))
        model.add(BatchNormalization())
        model.add(Dropout(self.dropout_rate))
        
        # 중간 GRU 레이어들
        for units in self.gru_units[1:-1]:
            model.add(GRU(
                units=units,
                return_sequences=True,
                recurrent_dropout=self.dropout_rate
            ))
            model.add(BatchNormalization())
            model.add(Dropout(self.dropout_rate))
        
        # 마지막 GRU 레이어
        model.add(GRU(
            units=self.gru_units[-1],
            return_sequences=False,
            recurrent_dropout=self.dropout_rate
        ))
        model.add(BatchNormalization())
        model.add(Dropout(self.dropout_rate))
        
        # 출력 레이어
        model.add(Dense(units=64, activation='relu'))
        model.add(Dropout(self.dropout_rate))
        model.add(Dense(units=32, activation='relu'))
        model.add(Dense(units=self.prediction_horizon))
        
        # 모델 컴파일
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='huber',  # Huber loss for robustness
            metrics=['mae', 'mse']
        )
        
        logger.info("GRU model built successfully")
        return model
    
    def prepare_data(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """데이터 준비"""
        try:
            # 특성 선택
            features = data[self.feature_columns].values
            
            # 정규화
            scaled_features = self.scaler.fit_transform(features)
            
            # 시퀀스 데이터 생성
            X, y = [], []
            
            for i in range(self.sequence_length, len(scaled_features) - self.prediction_horizon + 1):
                X.append(scaled_features[i-self.sequence_length:i])
                y.append(scaled_features[i:i+self.prediction_horizon, 3])  # close price
            
            X = np.array(X)
            y = np.array(y)
            
            logger.info(f"Prepared data: X shape {X.shape}, y shape {y.shape}")
            return X, y
            
        except Exception as e:
            logger.error(f"Data preparation failed: {e}")
            raise
    
    def train(self, data: pd.DataFrame, epochs: int = 200, batch_size: int = 32,
              validation_split: float = 0.2) -> Dict[str, List[float]]:
        """모델 학습"""
        try:
            # 데이터 준비
            X, y = self.prepare_data(data)
            
            # 모델 구축
            self.model = self.build_model((X.shape[1], X.shape[2]))
            
            # 콜백 설정
            callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=20,
                    restore_best_weights=True,
                    verbose=1
                ),
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=10,
                    min_lr=1e-7,
                    verbose=1
                )
            ]
            
            # 학습
            history = self.model.fit(
                X, y,
                epochs=epochs,
                batch_size=batch_size,
                validation_split=validation_split,
                callbacks=callbacks,
                verbose=1
            )
            
            logger.info("GRU model training completed")
            return history.history
            
        except Exception as e:
            logger.error(f"Model training failed: {e}")
            raise
    
    def predict(self, data: pd.DataFrame) -> np.ndarray:
        """가격 예측"""
        try:
            if self.model is None:
                raise ValueError("Model not trained. Call train() first.")
            
            # 데이터 준비
            features = data[self.feature_columns].values
            scaled_features = self.scaler.transform(features)
            
            # 예측을 위한 시퀀스 생성
            last_sequence = scaled_features[-self.sequence_length:]
            X_pred = last_sequence.reshape(1, self.sequence_length, len(self.feature_columns))
            
            # 예측
            scaled_prediction = self.model.predict(X_pred)
            
            # 역정규화
            prediction = self.scaler.inverse_transform(
                np.zeros((1, len(self.feature_columns)))
            )
            prediction[0, 3] = scaled_prediction[0, 0]  # close price
            
            logger.info(f"GRU prediction completed: {prediction[0, 3]}")
            return prediction[0, 3]
            
        except Exception as e:
            logger.error(f"Prediction failed: {e}")
            raise
    
    def predict_sequence(self, data: pd.DataFrame, steps: int = 5) -> List[float]:
        """다단계 예측"""
        try:
            if self.model is None:
                raise ValueError("Model not trained. Call train() first.")
            
            predictions = []
            current_data = data.copy()
            
            for step in range(steps):
                # 단일 예측
                prediction = self.predict(current_data)
                predictions.append(prediction)
                
                # 데이터 업데이트 (예측값을 다음 입력에 추가)
                new_row = current_data.iloc[-1].copy()
                new_row['close'] = prediction
                new_row['timestamp'] = new_row['timestamp'] + pd.Timedelta(hours=1)
                
                current_data = current_data.append(new_row, ignore_index=True)
                current_data = current_data.iloc[1:]  # 첫 번째 행 제거
            
            logger.info(f"Sequence prediction completed: {predictions}")
            return predictions
            
        except Exception as e:
            logger.error(f"Sequence prediction failed: {e}")
            raise
```

### 📦 **CNN 기반 예측 모델**

```python
# prediction-models/deep-learning/cnn/cnn_predictor.py
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class CNNPredictor:
    """CNN 기반 가격 예측 모델"""
    
    def __init__(self, sequence_length: int = 60, prediction_horizon: int = 1,
                 filters: List[int] = [64, 128, 256], kernel_size: int = 3):
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.filters = filters
        self.kernel_size = kernel_size
        self.model = None
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.feature_columns = [
            'open', 'high', 'low', 'close', 'volume',
            'rsi', 'macd', 'bollinger_upper', 'bollinger_lower',
            'sma_20', 'sma_50', 'ema_12', 'ema_26'
        ]
        
        logger.info(f"Initialized CNN predictor: filters={filters}, kernel_size={kernel_size}")
    
    def build_model(self, input_shape: Tuple[int, int]) -> Sequential:
        """CNN 모델 구축"""
        model = Sequential()
        
        # 첫 번째 Conv1D 레이어
        model.add(Conv1D(
            filters=self.filters[0],
            kernel_size=self.kernel_size,
            activation='relu',
            input_shape=input_shape,
            padding='same'
        ))
        model.add(BatchNormalization())
        model.add(MaxPooling1D(pool_size=2))
        model.add(Dropout(0.2))
        
        # 중간 Conv1D 레이어들
        for filters in self.filters[1:]:
            model.add(Conv1D(
                filters=filters,
                kernel_size=self.kernel_size,
                activation='relu',
                padding='same'
            ))
            model.add(BatchNormalization())
            model.add(MaxPooling1D(pool_size=2))
            model.add(Dropout(0.2))
        
        # Flatten 레이어
        model.add(Flatten())
        
        # Dense 레이어들
        model.add(Dense(128, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(64, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(32, activation='relu'))
        model.add(Dense(self.prediction_horizon))
        
        # 모델 컴파일
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mean_squared_error',
            metrics=['mae']
        )
        
        logger.info("CNN model built successfully")
        return model
    
    def prepare_data(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """데이터 준비"""
        try:
            # 특성 선택
            features = data[self.feature_columns].values
            
            # 정규화
            scaled_features = self.scaler.fit_transform(features)
            
            # 시퀀스 데이터 생성
            X, y = [], []
            
            for i in range(self.sequence_length, len(scaled_features) - self.prediction_horizon + 1):
                X.append(scaled_features[i-self.sequence_length:i])
                y.append(scaled_features[i:i+self.prediction_horizon, 3])  # close price
            
            X = np.array(X)
            y = np.array(y)
            
            logger.info(f"Prepared data: X shape {X.shape}, y shape {y.shape}")
            return X, y
            
        except Exception as e:
            logger.error(f"Data preparation failed: {e}")
            raise
    
    def train(self, data: pd.DataFrame, epochs: int = 150, batch_size: int = 32,
              validation_split: float = 0.2) -> Dict[str, List[float]]:
        """모델 학습"""
        try:
            # 데이터 준비
            X, y = self.prepare_data(data)
            
            # 모델 구축
            self.model = self.build_model((X.shape[1], X.shape[2]))
            
            # 콜백 설정
            callbacks = [
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=15,
                    restore_best_weights=True
                ),
                tf.keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=8,
                    min_lr=1e-7
                )
            ]
            
            # 학습
            history = self.model.fit(
                X, y,
                epochs=epochs,
                batch_size=batch_size,
                validation_split=validation_split,
                callbacks=callbacks,
                verbose=1
            )
            
            logger.info("CNN model training completed")
            return history.history
            
        except Exception as e:
            logger.error(f"Model training failed: {e}")
            raise
    
    def predict(self, data: pd.DataFrame) -> np.ndarray:
        """가격 예측"""
        try:
            if self.model is None:
                raise ValueError("Model not trained. Call train() first.")
            
            # 데이터 준비
            features = data[self.feature_columns].values
            scaled_features = self.scaler.transform(features)
            
            # 예측을 위한 시퀀스 생성
            last_sequence = scaled_features[-self.sequence_length:]
            X_pred = last_sequence.reshape(1, self.sequence_length, len(self.feature_columns))
            
            # 예측
            scaled_prediction = self.model.predict(X_pred)
            
            # 역정규화
            prediction = self.scaler.inverse_transform(
                np.zeros((1, len(self.feature_columns)))
            )
            prediction[0, 3] = scaled_prediction[0, 0]  # close price
            
            logger.info(f"CNN prediction completed: {prediction[0, 3]}")
            return prediction[0, 3]
            
        except Exception as e:
            logger.error(f"Prediction failed: {e}")
            raise
```

## 🎯 **다음 단계**

### 📋 **완료된 작업**
- ✅ 동적 앙상블 모델
- ✅ 스태킹 앙상블
- ✅ GRU 기반 예측 모델
- ✅ CNN 기반 예측 모델

### 🔄 **진행 중인 작업**
- 🔄 하이브리드 모델 (LSTM + CNN)
- 🔄 통계 모델 (ARIMA, VAR, Prophet)
- 🔄 특성 엔지니어링 시스템

### ⏳ **다음 단계**
1. **Phase 3.2 강화학습** 문서 생성
2. **Phase 3.3 감정 분석** 문서 생성
3. **Phase 3.4 포트폴리오 최적화** 문서 생성

---

**마지막 업데이트**: 2024-01-31
**다음 업데이트**: 2024-02-01 (Phase 3.2 강화학습)
**예측 모델 목표**: > 70% 앙상블 정확도, < 100ms 예측 지연, < 30분 학습 시간
**모델 성능**: 동적 가중치 조정, 실시간 모델 업데이트, 다중 모델 앙상블 