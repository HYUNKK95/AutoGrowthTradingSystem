# ğŸ“Š Phase 3.3: ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ

## ğŸ“‹ **ê°œìš”**

### ğŸ¯ **ëª©í‘œ**
- **ë‹¤ì¤‘ ì†ŒìŠ¤ ê°ì • ë¶„ì„**: ë‰´ìŠ¤, ì†Œì…œë¯¸ë””ì–´, ë¸”ë¡œê·¸, í¬ëŸ¼ í†µí•© ë¶„ì„
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: < 1ì´ˆ ì‘ë‹µ ì‹œê°„, ì‹¤ì‹œê°„ ê°ì • ìŠ¤ì½”ì–´ ì—…ë°ì´íŠ¸
- **ë‹¤êµ­ì–´ ì§€ì›**: ì˜ì–´, í•œêµ­ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ê°ì • ë¶„ì„
- **ì»¨í…ìŠ¤íŠ¸ ì¸ì‹**: ì‹œì¥ ìƒí™©, ë‰´ìŠ¤ ì¤‘ìš”ë„, ì˜í–¥ë ¥ ë¶„ì„
- **ê°ì • ê¸°ë°˜ ê±°ë˜**: ê°ì • ì‹ í˜¸ë¥¼ ê±°ë˜ ê²°ì •ì— í†µí•©

### ğŸ“Š **ì„±ëŠ¥ ëª©í‘œ**
- **ê°ì • ë¶„ì„ ì •í™•ë„**: > 85%
- **ì‹¤ì‹œê°„ ì²˜ë¦¬ ì†ë„**: < 500ms
- **ë‹¤êµ­ì–´ ì •í™•ë„**: > 80%
- **ê°ì • ì‹ í˜¸ ì˜í–¥ë ¥**: > 60% ê±°ë˜ ê²°ì • ê°œì„ 
- **ì‹œì¥ ì˜ˆì¸¡ ì •í™•ë„**: ê°ì • ê¸°ë°˜ + 5% í–¥ìƒ

## ğŸ—ï¸ **ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

### ğŸ“ **ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ êµ¬ì¡°**
```
sentiment-analysis/
â”œâ”€â”€ data-collectors/               # ë°ì´í„° ìˆ˜ì§‘ê¸°
â”‚   â”œâ”€â”€ news/                     # ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
â”‚   â”œâ”€â”€ social-media/             # ì†Œì…œë¯¸ë””ì–´ ìˆ˜ì§‘
â”‚   â”œâ”€â”€ blogs/                    # ë¸”ë¡œê·¸ ìˆ˜ì§‘
â”‚   â”œâ”€â”€ forums/                   # í¬ëŸ¼ ìˆ˜ì§‘
â”‚   â””â”€â”€ real-time/                # ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
â”œâ”€â”€ preprocessors/                 # ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ text-cleaning/            # í…ìŠ¤íŠ¸ ì •ì œ
â”‚   â”œâ”€â”€ language-detection/       # ì–¸ì–´ ê°ì§€
â”‚   â”œâ”€â”€ entity-extraction/        # ê°œì²´ ì¶”ì¶œ
â”‚   â””â”€â”€ context-enrichment/       # ì»¨í…ìŠ¤íŠ¸ ë³´ê°•
â”œâ”€â”€ models/                       # ê°ì • ë¶„ì„ ëª¨ë¸
â”‚   â”œâ”€â”€ transformer/              # Transformer ëª¨ë¸
â”‚   â”œâ”€â”€ lstm/                     # LSTM ëª¨ë¸
â”‚   â”œâ”€â”€ bert/                     # BERT ëª¨ë¸
â”‚   â”œâ”€â”€ multilingual/             # ë‹¤êµ­ì–´ ëª¨ë¸
â”‚   â””â”€â”€ ensemble/                 # ì•™ìƒë¸” ëª¨ë¸
â”œâ”€â”€ analyzers/                    # ë¶„ì„ê¸°
â”‚   â”œâ”€â”€ sentiment-scorer/         # ê°ì • ìŠ¤ì½”ì–´ë§
â”‚   â”œâ”€â”€ topic-extractor/          # ì£¼ì œ ì¶”ì¶œ
â”‚   â”œâ”€â”€ influence-analyzer/       # ì˜í–¥ë ¥ ë¶„ì„
â”‚   â””â”€â”€ trend-detector/           # íŠ¸ë Œë“œ ê°ì§€
â”œâ”€â”€ aggregators/                  # ì§‘ê³„ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ real-time/                # ì‹¤ì‹œê°„ ì§‘ê³„
â”‚   â”œâ”€â”€ historical/               # íˆìŠ¤í† ë¦¬ ì§‘ê³„
â”‚   â””â”€â”€ market-correlation/       # ì‹œì¥ ìƒê´€ê´€ê³„
â””â”€â”€ api/                          # API ì„œë¹„ìŠ¤
    â”œâ”€â”€ sentiment-api/            # ê°ì • ë¶„ì„ API
    â”œâ”€â”€ trend-api/                # íŠ¸ë Œë“œ API
    â””â”€â”€ alert-api/                # ì•Œë¦¼ API
```

## ğŸ”§ **ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ**

### ğŸ“¦ **ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ê¸°**

```python
# sentiment-analysis/data-collectors/multi_source_collector.py
import asyncio
import aiohttp
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
import json
import re
from bs4 import BeautifulSoup
import tweepy
import praw
import feedparser

logger = logging.getLogger(__name__)

@dataclass
class DataSource:
    """ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •"""
    source_id: str
    source_type: str  # 'news', 'social', 'blog', 'forum'
    url: str
    api_key: Optional[str] = None
    rate_limit: int = 100  # requests per minute
    priority: int = 1  # 1-5, ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„

@dataclass
class CollectedData:
    """ìˆ˜ì§‘ëœ ë°ì´í„°"""
    source_id: str
    source_type: str
    content: str
    title: Optional[str] = None
    author: Optional[str] = None
    timestamp: datetime = None
    url: Optional[str] = None
    language: Optional[str] = None
    metadata: Dict[str, Any] = None

class MultiSourceDataCollector:
    """ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, sources: List[DataSource]):
        self.sources = sources
        self.session = None
        self.collected_data = []
        self.rate_limiters = {}
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self._initialize_api_clients()
        
        logger.info(f"Multi-source data collector initialized with {len(sources)} sources")
    
    def _initialize_api_clients(self):
        """API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # Twitter API (ì˜ˆì‹œ)
            # self.twitter_client = tweepy.Client(bearer_token="YOUR_BEARER_TOKEN")
            
            # Reddit API (ì˜ˆì‹œ)
            # self.reddit_client = praw.Reddit(
            #     client_id="YOUR_CLIENT_ID",
            #     client_secret="YOUR_CLIENT_SECRET",
            #     user_agent="TradingSentimentBot/1.0"
            # )
            
            logger.info("API clients initialized")
            
        except Exception as e:
            logger.error(f"API client initialization failed: {e}")
    
    async def start_collection(self, collection_interval: int = 60):
        """ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"""
        try:
            self.session = aiohttp.ClientSession()
            
            while True:
                logger.info("Starting data collection cycle")
                
                # ë³‘ë ¬ë¡œ ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
                tasks = []
                for source in self.sources:
                    if self._can_collect_from_source(source):
                        task = asyncio.create_task(self._collect_from_source(source))
                        tasks.append(task)
                
                if tasks:
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    # ê²°ê³¼ ì²˜ë¦¬
                    for result in results:
                        if isinstance(result, Exception):
                            logger.error(f"Collection error: {result}")
                        elif result:
                            self.collected_data.extend(result)
                
                # ìˆ˜ì§‘ëœ ë°ì´í„° ì²˜ë¦¬
                if self.collected_data:
                    await self._process_collected_data()
                
                # ëŒ€ê¸°
                await asyncio.sleep(collection_interval)
                
        except Exception as e:
            logger.error(f"Data collection failed: {e}")
        finally:
            if self.session:
                await self.session.close()
    
    def _can_collect_from_source(self, source: DataSource) -> bool:
        """ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘ ê°€ëŠ¥í•œì§€ í™•ì¸"""
        current_time = datetime.now()
        
        if source.source_id not in self.rate_limiters:
            self.rate_limiters[source.source_id] = {
                'last_request': current_time - timedelta(minutes=1),
                'request_count': 0
            }
        
        limiter = self.rate_limiters[source.source_id]
        time_diff = (current_time - limiter['last_request']).total_seconds()
        
        # 1ë¶„ë§ˆë‹¤ ìš”ì²­ ìˆ˜ ë¦¬ì…‹
        if time_diff >= 60:
            limiter['request_count'] = 0
            limiter['last_request'] = current_time
        
        return limiter['request_count'] < source.rate_limit
    
    async def _collect_from_source(self, source: DataSource) -> List[CollectedData]:
        """ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            collected = []
            
            if source.source_type == 'news':
                collected = await self._collect_news(source)
            elif source.source_type == 'social':
                collected = await self._collect_social_media(source)
            elif source.source_type == 'blog':
                collected = await self._collect_blogs(source)
            elif source.source_type == 'forum':
                collected = await self._collect_forums(source)
            
            # ìš”ì²­ ìˆ˜ ì¦ê°€
            self.rate_limiters[source.source_id]['request_count'] += 1
            
            logger.info(f"Collected {len(collected)} items from {source.source_id}")
            return collected
            
        except Exception as e:
            logger.error(f"Collection from {source.source_id} failed: {e}")
            return []
    
    async def _collect_news(self, source: DataSource) -> List[CollectedData]:
        """ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            collected = []
            
            # RSS í”¼ë“œ ìˆ˜ì§‘
            if source.url.endswith('.xml') or 'rss' in source.url:
                feed = feedparser.parse(source.url)
                
                for entry in feed.entries[:10]:  # ìµœê·¼ 10ê°œ
                    content = entry.get('summary', '')
                    if not content and hasattr(entry, 'content'):
                        content = entry.content[0].value
                    
                    collected.append(CollectedData(
                        source_id=source.source_id,
                        source_type='news',
                        content=content,
                        title=entry.get('title', ''),
                        author=entry.get('author', ''),
                        timestamp=datetime(*entry.published_parsed[:6]),
                        url=entry.get('link', ''),
                        metadata={'feed_title': feed.feed.get('title', '')}
                    ))
            
            # ì›¹ ìŠ¤í¬ë˜í•‘
            else:
                async with self.session.get(source.url) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ë‰´ìŠ¤ ê¸°ì‚¬ ì¶”ì¶œ (ì˜ˆì‹œ)
                        articles = soup.find_all('article') or soup.find_all('div', class_='article')
                        
                        for article in articles[:5]:
                            title_elem = article.find('h1') or article.find('h2') or article.find('h3')
                            content_elem = article.find('p') or article.find('div', class_='content')
                            
                            if title_elem and content_elem:
                                collected.append(CollectedData(
                                    source_id=source.source_id,
                                    source_type='news',
                                    content=content_elem.get_text().strip(),
                                    title=title_elem.get_text().strip(),
                                    timestamp=datetime.now(),
                                    url=source.url
                                ))
            
            return collected
            
        except Exception as e:
            logger.error(f"News collection failed: {e}")
            return []
    
    async def _collect_social_media(self, source: DataSource) -> List[CollectedData]:
        """ì†Œì…œë¯¸ë””ì–´ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            collected = []
            
            # Twitter ìˆ˜ì§‘ (ì˜ˆì‹œ)
            if 'twitter' in source.url or 'x.com' in source.url:
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Twitter API ì‚¬ìš©
                # tweets = self.twitter_client.search_recent_tweets(
                #     query="bitcoin OR crypto OR trading",
                #     max_results=100
                # )
                
                # ì˜ˆì‹œ ë°ì´í„°
                collected.append(CollectedData(
                    source_id=source.source_id,
                    source_type='social',
                    content="Bitcoin showing strong momentum today! ğŸš€",
                    author="crypto_trader",
                    timestamp=datetime.now(),
                    metadata={'platform': 'twitter', 'followers': 1000}
                ))
            
            # Reddit ìˆ˜ì§‘ (ì˜ˆì‹œ)
            elif 'reddit' in source.url:
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Reddit API ì‚¬ìš©
                # subreddit = self.reddit_client.subreddit("cryptocurrency")
                # posts = subreddit.hot(limit=10)
                
                # ì˜ˆì‹œ ë°ì´í„°
                collected.append(CollectedData(
                    source_id=source.source_id,
                    source_type='social',
                    content="What do you think about the current market trend?",
                    author="reddit_user",
                    timestamp=datetime.now(),
                    metadata={'platform': 'reddit', 'subreddit': 'cryptocurrency'}
                ))
            
            return collected
            
        except Exception as e:
            logger.error(f"Social media collection failed: {e}")
            return []
    
    async def _collect_blogs(self, source: DataSource) -> List[CollectedData]:
        """ë¸”ë¡œê·¸ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            collected = []
            
            async with self.session.get(source.url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì¶”ì¶œ
                    posts = soup.find_all('article') or soup.find_all('div', class_='post')
                    
                    for post in posts[:5]:
                        title_elem = post.find('h1') or post.find('h2')
                        content_elem = post.find('div', class_='content') or post.find('p')
                        
                        if title_elem and content_elem:
                            collected.append(CollectedData(
                                source_id=source.source_id,
                                source_type='blog',
                                content=content_elem.get_text().strip(),
                                title=title_elem.get_text().strip(),
                                timestamp=datetime.now(),
                                url=source.url
                            ))
            
            return collected
            
        except Exception as e:
            logger.error(f"Blog collection failed: {e}")
            return []
    
    async def _collect_forums(self, source: DataSource) -> List[CollectedData]:
        """í¬ëŸ¼ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            collected = []
            
            async with self.session.get(source.url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # í¬ëŸ¼ í¬ìŠ¤íŠ¸ ì¶”ì¶œ
                    posts = soup.find_all('div', class_='post') or soup.find_all('tr', class_='thread')
                    
                    for post in posts[:10]:
                        content_elem = post.find('div', class_='message') or post.find('td', class_='content')
                        author_elem = post.find('span', class_='author') or post.find('td', class_='author')
                        
                        if content_elem:
                            collected.append(CollectedData(
                                source_id=source.source_id,
                                source_type='forum',
                                content=content_elem.get_text().strip(),
                                author=author_elem.get_text().strip() if author_elem else None,
                                timestamp=datetime.now(),
                                url=source.url
                            ))
            
            return collected
            
        except Exception as e:
            logger.error(f"Forum collection failed: {e}")
            return []
    
    async def _process_collected_data(self):
        """ìˆ˜ì§‘ëœ ë°ì´í„° ì²˜ë¦¬"""
        try:
            # ì–¸ì–´ ê°ì§€
            for data in self.collected_data:
                if not data.language:
                    data.language = self._detect_language(data.content)
            
            # ì¤‘ë³µ ì œê±°
            self.collected_data = self._remove_duplicates(self.collected_data)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë˜ëŠ” íì— ì „ì†¡
            await self._store_data(self.collected_data)
            
            # ìˆ˜ì§‘ëœ ë°ì´í„° í´ë¦¬ì–´
            self.collected_data = []
            
            logger.info("Collected data processed successfully")
            
        except Exception as e:
            logger.error(f"Data processing failed: {e}")
    
    def _detect_language(self, text: str) -> str:
        """ì–¸ì–´ ê°ì§€"""
        try:
            # ê°„ë‹¨í•œ ì–¸ì–´ ê°ì§€ (ì‹¤ì œë¡œëŠ” langdetect ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
            korean_chars = len(re.findall(r'[ê°€-í£]', text))
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text))
            
            if korean_chars > 0:
                return 'ko'
            elif chinese_chars > 0:
                return 'zh'
            elif japanese_chars > 0:
                return 'ja'
            else:
                return 'en'
                
        except Exception as e:
            logger.error(f"Language detection failed: {e}")
            return 'en'
    
    def _remove_duplicates(self, data_list: List[CollectedData]) -> List[CollectedData]:
        """ì¤‘ë³µ ì œê±°"""
        try:
            seen = set()
            unique_data = []
            
            for data in data_list:
                # ì œëª©ê³¼ ë‚´ìš©ì˜ í•´ì‹œë¡œ ì¤‘ë³µ í™•ì¸
                content_hash = hash(f"{data.title}:{data.content[:100]}")
                
                if content_hash not in seen:
                    seen.add(content_hash)
                    unique_data.append(data)
            
            return unique_data
            
        except Exception as e:
            logger.error(f"Duplicate removal failed: {e}")
            return data_list
    
    async def _store_data(self, data_list: List[CollectedData]):
        """ë°ì´í„° ì €ì¥"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            # ë˜ëŠ” ë©”ì‹œì§€ íì— ì „ì†¡
            
            for data in data_list:
                # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜ˆì‹œ
                # await self.db.insert_sentiment_data(data)
                
                # ë©”ì‹œì§€ í ì „ì†¡ ì˜ˆì‹œ
                # await self.message_queue.send(data)
                
                pass
            
            logger.info(f"Stored {len(data_list)} data items")
            
        except Exception as e:
            logger.error(f"Data storage failed: {e}")
```

## ğŸ”§ **ê°ì • ë¶„ì„ ëª¨ë¸**

### ğŸ“¦ **Transformer ê¸°ë°˜ ê°ì • ë¶„ì„**

```python
# sentiment-analysis/models/transformer_sentiment_analyzer.py
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, pipeline
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
import pandas as pd
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class SentimentResult:
    """ê°ì • ë¶„ì„ ê²°ê³¼"""
    text: str
    sentiment_score: float  # -1.0 (ë§¤ìš° ë¶€ì •ì ) ~ 1.0 (ë§¤ìš° ê¸ì •ì )
    sentiment_label: str    # 'positive', 'negative', 'neutral'
    confidence: float       # 0.0 ~ 1.0
    topics: List[str]       # ê°ì§€ëœ ì£¼ì œë“¤
    entities: List[str]     # ê°ì§€ëœ ê°œì²´ë“¤
    language: str           # ì–¸ì–´ ì½”ë“œ

class TransformerSentimentAnalyzer:
    """Transformer ê¸°ë°˜ ê°ì • ë¶„ì„ê¸°"""
    
    def __init__(self, model_name: str = "cardiffnlp/twitter-roberta-base-sentiment-latest",
                 device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        self.model_name = model_name
        self.device = device
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        
        # ì–¸ì–´ë³„ ëª¨ë¸
        self.language_models = {
            'en': "cardiffnlp/twitter-roberta-base-sentiment-latest",
            'ko': "klue/roberta-base",  # í•œêµ­ì–´ ëª¨ë¸
            'zh': "hfl/chinese-roberta-wwm-ext",  # ì¤‘êµ­ì–´ ëª¨ë¸
            'ja': "cl-tohoku/bert-base-japanese-v3"  # ì¼ë³¸ì–´ ëª¨ë¸
        }
        
        self._load_models()
        
        logger.info(f"Transformer sentiment analyzer initialized with {model_name}")
    
    def _load_models(self):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            # ê¸°ë³¸ ì˜ì–´ ëª¨ë¸
            self.pipeline = pipeline(
                "sentiment-analysis",
                model=self.language_models['en'],
                device=0 if self.device == "cuda" else -1
            )
            
            # ë‹¤êµ­ì–´ ëª¨ë¸ë“¤ ë¡œë“œ
            self.multilingual_models = {}
            for lang, model_name in self.language_models.items():
                if lang != 'en':
                    try:
                        self.multilingual_models[lang] = pipeline(
                            "sentiment-analysis",
                            model=model_name,
                            device=0 if self.device == "cuda" else -1
                        )
                        logger.info(f"Loaded {lang} model: {model_name}")
                    except Exception as e:
                        logger.warning(f"Failed to load {lang} model: {e}")
            
        except Exception as e:
            logger.error(f"Model loading failed: {e}")
            raise
    
    def analyze_sentiment(self, text: str, language: str = 'en') -> SentimentResult:
        """ê°ì • ë¶„ì„ ìˆ˜í–‰"""
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_text = self._preprocess_text(text)
            
            # ì–¸ì–´ë³„ ëª¨ë¸ ì„ íƒ
            if language in self.multilingual_models:
                model = self.multilingual_models[language]
            else:
                model = self.pipeline
            
            # ê°ì • ë¶„ì„
            result = model(processed_text)
            
            # ê²°ê³¼ íŒŒì‹±
            sentiment_score = self._parse_sentiment_result(result)
            sentiment_label = self._get_sentiment_label(sentiment_score)
            confidence = result[0].get('score', 0.5)
            
            # ì£¼ì œ ë° ê°œì²´ ì¶”ì¶œ
            topics = self._extract_topics(processed_text)
            entities = self._extract_entities(processed_text)
            
            return SentimentResult(
                text=text,
                sentiment_score=sentiment_score,
                sentiment_label=sentiment_label,
                confidence=confidence,
                topics=topics,
                entities=entities,
                language=language
            )
            
        except Exception as e:
            logger.error(f"Sentiment analysis failed: {e}")
            return SentimentResult(
                text=text,
                sentiment_score=0.0,
                sentiment_label='neutral',
                confidence=0.0,
                topics=[],
                entities=[],
                language=language
            )
    
    def analyze_batch(self, texts: List[str], languages: List[str] = None) -> List[SentimentResult]:
        """ë°°ì¹˜ ê°ì • ë¶„ì„"""
        try:
            if languages is None:
                languages = ['en'] * len(texts)
            
            results = []
            
            # ì–¸ì–´ë³„ë¡œ ê·¸ë£¹í™”
            language_groups = {}
            for i, (text, lang) in enumerate(zip(texts, languages)):
                if lang not in language_groups:
                    language_groups[lang] = []
                language_groups[lang].append((i, text))
            
            # ê° ì–¸ì–´ë³„ë¡œ ë°°ì¹˜ ì²˜ë¦¬
            for lang, group in language_groups.items():
                indices, group_texts = zip(*group)
                
                if lang in self.multilingual_models:
                    model = self.multilingual_models[lang]
                else:
                    model = self.pipeline
                
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_results = model(group_texts)
                
                # ê²°ê³¼ ë³€í™˜
                for idx, (text, result) in enumerate(zip(group_texts, batch_results)):
                    sentiment_score = self._parse_sentiment_result([result])
                    sentiment_label = self._get_sentiment_label(sentiment_score)
                    
                    results.append(SentimentResult(
                        text=text,
                        sentiment_score=sentiment_score,
                        sentiment_label=sentiment_label,
                        confidence=result.get('score', 0.5),
                        topics=self._extract_topics(text),
                        entities=self._extract_entities(text),
                        language=lang
                    ))
            
            # ì›ë˜ ìˆœì„œë¡œ ì •ë ¬
            results = [results[i] for i in range(len(results))]
            
            return results
            
        except Exception as e:
            logger.error(f"Batch sentiment analysis failed: {e}")
            return []
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        try:
            # URL ì œê±°
            text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
            
            # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
            text = re.sub(r'[^\w\sê°€-í£\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]', ' ', text)
            
            # ê³µë°± ì •ë¦¬
            text = re.sub(r'\s+', ' ', text).strip()
            
            # ê¸¸ì´ ì œí•œ
            if len(text) > 512:
                text = text[:512]
            
            return text
            
        except Exception as e:
            logger.error(f"Text preprocessing failed: {e}")
            return text
    
    def _parse_sentiment_result(self, result: List[Dict]) -> float:
        """ê°ì • ë¶„ì„ ê²°ê³¼ íŒŒì‹±"""
        try:
            if not result:
                return 0.0
            
            label = result[0].get('label', 'NEU')
            score = result[0].get('score', 0.5)
            
            # ë¼ë²¨ì„ ì ìˆ˜ë¡œ ë³€í™˜
            if label in ['POS', 'positive', 'LABEL_2']:
                return score
            elif label in ['NEG', 'negative', 'LABEL_0']:
                return -score
            else:  # NEU, neutral, LABEL_1
                return 0.0
                
        except Exception as e:
            logger.error(f"Sentiment result parsing failed: {e}")
            return 0.0
    
    def _get_sentiment_label(self, score: float) -> str:
        """ê°ì • ì ìˆ˜ë¥¼ ë¼ë²¨ë¡œ ë³€í™˜"""
        if score > 0.1:
            return 'positive'
        elif score < -0.1:
            return 'negative'
        else:
            return 'neutral'
    
    def _extract_topics(self, text: str) -> List[str]:
        """ì£¼ì œ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì£¼ì œ ì¶”ì¶œ
            topics = []
            
            # ì•”í˜¸í™”í ê´€ë ¨ í‚¤ì›Œë“œ
            crypto_keywords = ['bitcoin', 'btc', 'ethereum', 'eth', 'crypto', 'blockchain']
            for keyword in crypto_keywords:
                if keyword.lower() in text.lower():
                    topics.append(keyword)
            
            # ê±°ë˜ ê´€ë ¨ í‚¤ì›Œë“œ
            trading_keywords = ['trading', 'market', 'price', 'bull', 'bear', 'pump', 'dump']
            for keyword in trading_keywords:
                if keyword.lower() in text.lower():
                    topics.append(keyword)
            
            return list(set(topics))
            
        except Exception as e:
            logger.error(f"Topic extraction failed: {e}")
            return []
    
    def _extract_entities(self, text: str) -> List[str]:
        """ê°œì²´ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ ê°œì²´ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” NER ëª¨ë¸ ì‚¬ìš©)
            entities = []
            
            # ì•”í˜¸í™”í ì‹¬ë³¼ íŒ¨í„´
            crypto_pattern = r'\b[A-Z]{3,10}\b'
            crypto_matches = re.findall(crypto_pattern, text)
            entities.extend(crypto_matches)
            
            # íšŒì‚¬ëª… íŒ¨í„´
            company_pattern = r'\b[A-Z][a-z]+ (Inc|Corp|Ltd|LLC)\b'
            company_matches = re.findall(company_pattern, text)
            entities.extend(company_matches)
            
            return list(set(entities))
            
        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            return []
```

## ğŸ”§ **ì‹¤ì‹œê°„ ê°ì • ì§‘ê³„ ì‹œìŠ¤í…œ**

### ğŸ“¦ **ì‹¤ì‹œê°„ ê°ì • ì§‘ê³„ê¸°**

```python
# sentiment-analysis/aggregators/real_time_aggregator.py
import asyncio
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from collections import defaultdict, deque

logger = logging.getLogger(__name__)

@dataclass
class SentimentAggregate:
    """ê°ì • ì§‘ê³„ ê²°ê³¼"""
    timestamp: datetime
    overall_sentiment: float
    positive_ratio: float
    negative_ratio: float
    neutral_ratio: float
    volume: int
    confidence: float
    topics: Dict[str, float]
    sources: Dict[str, float]
    market_correlation: float

class RealTimeSentimentAggregator:
    """ì‹¤ì‹œê°„ ê°ì • ì§‘ê³„ê¸°"""
    
    def __init__(self, window_size: int = 300,  # 5ë¶„ ìœˆë„ìš°
                 update_interval: int = 30):     # 30ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
        self.window_size = window_size
        self.update_interval = update_interval
        self.sentiment_data = deque(maxlen=10000)  # ìµœëŒ€ 10000ê°œ ë°ì´í„° ì €ì¥
        self.aggregates = deque(maxlen=1000)       # ìµœëŒ€ 1000ê°œ ì§‘ê³„ ê²°ê³¼ ì €ì¥
        
        # ì‹¤ì‹œê°„ í†µê³„
        self.current_stats = {
            'total_sentiments': 0,
            'positive_count': 0,
            'negative_count': 0,
            'neutral_count': 0,
            'total_confidence': 0.0,
            'topic_counts': defaultdict(int),
            'source_counts': defaultdict(int)
        }
        
        logger.info(f"Real-time sentiment aggregator initialized")
    
    async def start_aggregation(self):
        """ì§‘ê³„ ì‹œì‘"""
        try:
            while True:
                # í˜„ì¬ ìœˆë„ìš°ì˜ ë°ì´í„° ì§‘ê³„
                current_time = datetime.now()
                window_start = current_time - timedelta(seconds=self.window_size)
                
                # ìœˆë„ìš° ë‚´ ë°ì´í„° í•„í„°ë§
                window_data = [
                    data for data in self.sentiment_data
                    if data.timestamp >= window_start
                ]
                
                if window_data:
                    # ì§‘ê³„ ìˆ˜í–‰
                    aggregate = self._aggregate_sentiments(window_data)
                    self.aggregates.append(aggregate)
                    
                    logger.info(f"Aggregated {len(window_data)} sentiments: {aggregate.overall_sentiment:.3f}")
                
                # ëŒ€ê¸°
                await asyncio.sleep(self.update_interval)
                
        except Exception as e:
            logger.error(f"Sentiment aggregation failed: {e}")
    
    def add_sentiment_data(self, sentiment_result: 'SentimentResult'):
        """ê°ì • ë°ì´í„° ì¶”ê°€"""
        try:
            # ë°ì´í„° ì¶”ê°€
            self.sentiment_data.append(sentiment_result)
            
            # ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_current_stats(sentiment_result)
            
        except Exception as e:
            logger.error(f"Sentiment data addition failed: {e}")
    
    def _update_current_stats(self, sentiment_result: 'SentimentResult'):
        """ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.current_stats['total_sentiments'] += 1
            
            # ê°ì • ë¼ë²¨ ì¹´ìš´íŠ¸
            if sentiment_result.sentiment_label == 'positive':
                self.current_stats['positive_count'] += 1
            elif sentiment_result.sentiment_label == 'negative':
                self.current_stats['negative_count'] += 1
            else:
                self.current_stats['neutral_count'] += 1
            
            # ì‹ ë¢°ë„ ëˆ„ì 
            self.current_stats['total_confidence'] += sentiment_result.confidence
            
            # ì£¼ì œ ì¹´ìš´íŠ¸
            for topic in sentiment_result.topics:
                self.current_stats['topic_counts'][topic] += 1
            
            # ì†ŒìŠ¤ ì¹´ìš´íŠ¸
            source = sentiment_result.language  # ë˜ëŠ” ì‹¤ì œ ì†ŒìŠ¤ ì •ë³´
            self.current_stats['source_counts'][source] += 1
            
        except Exception as e:
            logger.error(f"Stats update failed: {e}")
    
    def _aggregate_sentiments(self, sentiment_data: List['SentimentResult']) -> SentimentAggregate:
        """ê°ì • ë°ì´í„° ì§‘ê³„"""
        try:
            if not sentiment_data:
                return SentimentAggregate(
                    timestamp=datetime.now(),
                    overall_sentiment=0.0,
                    positive_ratio=0.0,
                    negative_ratio=0.0,
                    neutral_ratio=0.0,
                    volume=0,
                    confidence=0.0,
                    topics={},
                    sources={},
                    market_correlation=0.0
                )
            
            # ê¸°ë³¸ í†µê³„
            total_count = len(sentiment_data)
            sentiment_scores = [data.sentiment_score for data in sentiment_data]
            confidences = [data.confidence for data in sentiment_data]
            
            # ê°ì • ë¹„ìœ¨
            positive_count = sum(1 for data in sentiment_data if data.sentiment_label == 'positive')
            negative_count = sum(1 for data in sentiment_data if data.sentiment_label == 'negative')
            neutral_count = sum(1 for data in sentiment_data if data.sentiment_label == 'neutral')
            
            positive_ratio = positive_count / total_count
            negative_ratio = negative_count / total_count
            neutral_ratio = neutral_count / total_count
            
            # ê°€ì¤‘ í‰ê·  ê°ì • ì ìˆ˜
            weighted_sentiment = np.average(sentiment_scores, weights=confidences)
            
            # í‰ê·  ì‹ ë¢°ë„
            avg_confidence = np.mean(confidences)
            
            # ì£¼ì œë³„ ê°ì • ì ìˆ˜
            topic_sentiments = defaultdict(list)
            for data in sentiment_data:
                for topic in data.topics:
                    topic_sentiments[topic].append(data.sentiment_score)
            
            topic_aggregates = {}
            for topic, scores in topic_sentiments.items():
                topic_aggregates[topic] = np.mean(scores)
            
            # ì†ŒìŠ¤ë³„ ê°ì • ì ìˆ˜
            source_sentiments = defaultdict(list)
            for data in sentiment_data:
                source = data.language  # ë˜ëŠ” ì‹¤ì œ ì†ŒìŠ¤ ì •ë³´
                source_sentiments[source].append(data.sentiment_score)
            
            source_aggregates = {}
            for source, scores in source_sentiments.items():
                source_aggregates[source] = np.mean(scores)
            
            # ì‹œì¥ ìƒê´€ê´€ê³„ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹œì¥ ë°ì´í„°ì™€ ë¹„êµ)
            market_correlation = self._calculate_market_correlation(sentiment_scores)
            
            return SentimentAggregate(
                timestamp=datetime.now(),
                overall_sentiment=weighted_sentiment,
                positive_ratio=positive_ratio,
                negative_ratio=negative_ratio,
                neutral_ratio=neutral_ratio,
                volume=total_count,
                confidence=avg_confidence,
                topics=topic_aggregates,
                sources=source_aggregates,
                market_correlation=market_correlation
            )
            
        except Exception as e:
            logger.error(f"Sentiment aggregation failed: {e}")
            return SentimentAggregate(
                timestamp=datetime.now(),
                overall_sentiment=0.0,
                positive_ratio=0.0,
                negative_ratio=0.0,
                neutral_ratio=0.0,
                volume=0,
                confidence=0.0,
                topics={},
                sources={},
                market_correlation=0.0
            )
    
    def _calculate_market_correlation(self, sentiment_scores: List[float]) -> float:
        """ì‹œì¥ ìƒê´€ê´€ê³„ ê³„ì‚°"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹œì¥ ê°€ê²© ë°ì´í„°ì™€ ìƒê´€ê´€ê³„ ê³„ì‚°
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ëœë¤ ê°’ ë°˜í™˜
            return np.random.uniform(-0.5, 0.5)
            
        except Exception as e:
            logger.error(f"Market correlation calculation failed: {e}")
            return 0.0
    
    def get_current_sentiment(self) -> Optional[SentimentAggregate]:
        """í˜„ì¬ ê°ì • ìƒíƒœ ë°˜í™˜"""
        try:
            if self.aggregates:
                return self.aggregates[-1]
            return None
            
        except Exception as e:
            logger.error(f"Current sentiment retrieval failed: {e}")
            return None
    
    def get_sentiment_trend(self, hours: int = 24) -> List[SentimentAggregate]:
        """ê°ì • íŠ¸ë Œë“œ ë°˜í™˜"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            trend_data = [
                agg for agg in self.aggregates
                if agg.timestamp >= cutoff_time
            ]
            
            return trend_data
            
        except Exception as e:
            logger.error(f"Sentiment trend retrieval failed: {e}")
            return []
    
    def get_topic_sentiment(self, topic: str, hours: int = 24) -> float:
        """ì£¼ì œë³„ ê°ì • ì ìˆ˜"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            topic_sentiments = []
            for agg in self.aggregates:
                if agg.timestamp >= cutoff_time and topic in agg.topics:
                    topic_sentiments.append(agg.topics[topic])
            
            if topic_sentiments:
                return np.mean(topic_sentiments)
            return 0.0
            
        except Exception as e:
            logger.error(f"Topic sentiment retrieval failed: {e}")
            return 0.0
```

## ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„**

### ğŸ“‹ **ì™„ë£Œëœ ì‘ì—…**
- âœ… ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ê¸°
- âœ… Transformer ê¸°ë°˜ ê°ì • ë¶„ì„
- âœ… ì‹¤ì‹œê°„ ê°ì • ì§‘ê³„ ì‹œìŠ¤í…œ
- âœ… ë‹¤êµ­ì–´ ê°ì • ë¶„ì„
- âœ… ì£¼ì œ ë° ê°œì²´ ì¶”ì¶œ

### ğŸ”„ **ì§„í–‰ ì¤‘ì¸ ì‘ì—…**
- ğŸ”„ ê°ì • ê¸°ë°˜ ê±°ë˜ ì‹ í˜¸ ìƒì„±
- ğŸ”„ ì‹œì¥ ìƒê´€ê´€ê³„ ë¶„ì„
- ğŸ”„ ê°ì • ì•Œë¦¼ ì‹œìŠ¤í…œ

### â³ **ë‹¤ìŒ ë‹¨ê³„**
1. **Phase 3.4 í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”** ë¬¸ì„œ ìƒì„±
2. **Phase 3.5 ë¦¬ìŠ¤í¬ ê´€ë¦¬ AI** ë¬¸ì„œ ìƒì„±
3. **Phase 3.6 Web3 ì§€ê°‘ í†µí•©** ë¬¸ì„œ ìƒì„±

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-01-31
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: 2024-02-01 (Phase 3.4 í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”)
**ê°ì • ë¶„ì„ ëª©í‘œ**: > 85% ì •í™•ë„, < 500ms ì²˜ë¦¬ ì†ë„, > 60% ê±°ë˜ ê²°ì • ê°œì„ 
**ê°ì • ë¶„ì„ ì„±ëŠ¥**: ì‹¤ì‹œê°„ ì²˜ë¦¬, ë‹¤êµ­ì–´ ì§€ì›, ì»¨í…ìŠ¤íŠ¸ ì¸ì‹, ì‹œì¥ ìƒê´€ê´€ê³„ ë¶„ì„ 