# 📊 Phase 3.3: 실시간 감정 분석 시스템

## 📋 **개요**

### 🎯 **목표**
- **다중 소스 감정 분석**: 뉴스, 소셜미디어, 블로그, 포럼 통합 분석
- **실시간 처리**: < 1초 응답 시간, 실시간 감정 스코어 업데이트
- **다국어 지원**: 영어, 한국어, 중국어, 일본어 감정 분석
- **컨텍스트 인식**: 시장 상황, 뉴스 중요도, 영향력 분석
- **감정 기반 거래**: 감정 신호를 거래 결정에 통합

### 📊 **성능 목표**
- **감정 분석 정확도**: > 85%
- **실시간 처리 속도**: < 500ms
- **다국어 정확도**: > 80%
- **감정 신호 영향력**: > 60% 거래 결정 개선
- **시장 예측 정확도**: 감정 기반 + 5% 향상

## 🏗️ **감정 분석 시스템 아키텍처**

### 📁 **감정 분석 시스템 구조**
```
sentiment-analysis/
├── data-collectors/               # 데이터 수집기
│   ├── news/                     # 뉴스 데이터 수집
│   ├── social-media/             # 소셜미디어 수집
│   ├── blogs/                    # 블로그 수집
│   ├── forums/                   # 포럼 수집
│   └── real-time/                # 실시간 데이터 수집
├── preprocessors/                 # 전처리 시스템
│   ├── text-cleaning/            # 텍스트 정제
│   ├── language-detection/       # 언어 감지
│   ├── entity-extraction/        # 개체 추출
│   └── context-enrichment/       # 컨텍스트 보강
├── models/                       # 감정 분석 모델
│   ├── transformer/              # Transformer 모델
│   ├── lstm/                     # LSTM 모델
│   ├── bert/                     # BERT 모델
│   ├── multilingual/             # 다국어 모델
│   └── ensemble/                 # 앙상블 모델
├── analyzers/                    # 분석기
│   ├── sentiment-scorer/         # 감정 스코어링
│   ├── topic-extractor/          # 주제 추출
│   ├── influence-analyzer/       # 영향력 분석
│   └── trend-detector/           # 트렌드 감지
├── aggregators/                  # 집계 시스템
│   ├── real-time/                # 실시간 집계
│   ├── historical/               # 히스토리 집계
│   └── market-correlation/       # 시장 상관관계
└── api/                          # API 서비스
    ├── sentiment-api/            # 감정 분석 API
    ├── trend-api/                # 트렌드 API
    └── alert-api/                # 알림 API
```

## 🔧 **실시간 데이터 수집 시스템**

### 📦 **다중 소스 데이터 수집기**

```python
# sentiment-analysis/data-collectors/multi_source_collector.py
import asyncio
import aiohttp
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
import json
import re
from bs4 import BeautifulSoup
import tweepy
import praw
import feedparser

logger = logging.getLogger(__name__)

@dataclass
class DataSource:
    """데이터 소스 설정"""
    source_id: str
    source_type: str  # 'news', 'social', 'blog', 'forum'
    url: str
    api_key: Optional[str] = None
    rate_limit: int = 100  # requests per minute
    priority: int = 1  # 1-5, 높을수록 우선순위

@dataclass
class CollectedData:
    """수집된 데이터"""
    source_id: str
    source_type: str
    content: str
    title: Optional[str] = None
    author: Optional[str] = None
    timestamp: datetime = None
    url: Optional[str] = None
    language: Optional[str] = None
    metadata: Dict[str, Any] = None

class MultiSourceDataCollector:
    """다중 소스 데이터 수집기"""
    
    def __init__(self, sources: List[DataSource]):
        self.sources = sources
        self.session = None
        self.collected_data = []
        self.rate_limiters = {}
        
        # API 클라이언트 초기화
        self._initialize_api_clients()
        
        logger.info(f"Multi-source data collector initialized with {len(sources)} sources")
    
    def _initialize_api_clients(self):
        """API 클라이언트 초기화"""
        try:
            # Twitter API (예시)
            # self.twitter_client = tweepy.Client(bearer_token="YOUR_BEARER_TOKEN")
            
            # Reddit API (예시)
            # self.reddit_client = praw.Reddit(
            #     client_id="YOUR_CLIENT_ID",
            #     client_secret="YOUR_CLIENT_SECRET",
            #     user_agent="TradingSentimentBot/1.0"
            # )
            
            logger.info("API clients initialized")
            
        except Exception as e:
            logger.error(f"API client initialization failed: {e}")
    
    async def start_collection(self, collection_interval: int = 60):
        """데이터 수집 시작"""
        try:
            self.session = aiohttp.ClientSession()
            
            while True:
                logger.info("Starting data collection cycle")
                
                # 병렬로 모든 소스에서 데이터 수집
                tasks = []
                for source in self.sources:
                    if self._can_collect_from_source(source):
                        task = asyncio.create_task(self._collect_from_source(source))
                        tasks.append(task)
                
                if tasks:
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    # 결과 처리
                    for result in results:
                        if isinstance(result, Exception):
                            logger.error(f"Collection error: {result}")
                        elif result:
                            self.collected_data.extend(result)
                
                # 수집된 데이터 처리
                if self.collected_data:
                    await self._process_collected_data()
                
                # 대기
                await asyncio.sleep(collection_interval)
                
        except Exception as e:
            logger.error(f"Data collection failed: {e}")
        finally:
            if self.session:
                await self.session.close()
    
    def _can_collect_from_source(self, source: DataSource) -> bool:
        """소스에서 수집 가능한지 확인"""
        current_time = datetime.now()
        
        if source.source_id not in self.rate_limiters:
            self.rate_limiters[source.source_id] = {
                'last_request': current_time - timedelta(minutes=1),
                'request_count': 0
            }
        
        limiter = self.rate_limiters[source.source_id]
        time_diff = (current_time - limiter['last_request']).total_seconds()
        
        # 1분마다 요청 수 리셋
        if time_diff >= 60:
            limiter['request_count'] = 0
            limiter['last_request'] = current_time
        
        return limiter['request_count'] < source.rate_limit
    
    async def _collect_from_source(self, source: DataSource) -> List[CollectedData]:
        """소스에서 데이터 수집"""
        try:
            collected = []
            
            if source.source_type == 'news':
                collected = await self._collect_news(source)
            elif source.source_type == 'social':
                collected = await self._collect_social_media(source)
            elif source.source_type == 'blog':
                collected = await self._collect_blogs(source)
            elif source.source_type == 'forum':
                collected = await self._collect_forums(source)
            
            # 요청 수 증가
            self.rate_limiters[source.source_id]['request_count'] += 1
            
            logger.info(f"Collected {len(collected)} items from {source.source_id}")
            return collected
            
        except Exception as e:
            logger.error(f"Collection from {source.source_id} failed: {e}")
            return []
    
    async def _collect_news(self, source: DataSource) -> List[CollectedData]:
        """뉴스 데이터 수집"""
        try:
            collected = []
            
            # RSS 피드 수집
            if source.url.endswith('.xml') or 'rss' in source.url:
                feed = feedparser.parse(source.url)
                
                for entry in feed.entries[:10]:  # 최근 10개
                    content = entry.get('summary', '')
                    if not content and hasattr(entry, 'content'):
                        content = entry.content[0].value
                    
                    collected.append(CollectedData(
                        source_id=source.source_id,
                        source_type='news',
                        content=content,
                        title=entry.get('title', ''),
                        author=entry.get('author', ''),
                        timestamp=datetime(*entry.published_parsed[:6]),
                        url=entry.get('link', ''),
                        metadata={'feed_title': feed.feed.get('title', '')}
                    ))
            
            # 웹 스크래핑
            else:
                async with self.session.get(source.url) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # 뉴스 기사 추출 (예시)
                        articles = soup.find_all('article') or soup.find_all('div', class_='article')
                        
                        for article in articles[:5]:
                            title_elem = article.find('h1') or article.find('h2') or article.find('h3')
                            content_elem = article.find('p') or article.find('div', class_='content')
                            
                            if title_elem and content_elem:
                                collected.append(CollectedData(
                                    source_id=source.source_id,
                                    source_type='news',
                                    content=content_elem.get_text().strip(),
                                    title=title_elem.get_text().strip(),
                                    timestamp=datetime.now(),
                                    url=source.url
                                ))
            
            return collected
            
        except Exception as e:
            logger.error(f"News collection failed: {e}")
            return []
    
    async def _collect_social_media(self, source: DataSource) -> List[CollectedData]:
        """소셜미디어 데이터 수집"""
        try:
            collected = []
            
            # Twitter 수집 (예시)
            if 'twitter' in source.url or 'x.com' in source.url:
                # 실제 구현에서는 Twitter API 사용
                # tweets = self.twitter_client.search_recent_tweets(
                #     query="bitcoin OR crypto OR trading",
                #     max_results=100
                # )
                
                # 예시 데이터
                collected.append(CollectedData(
                    source_id=source.source_id,
                    source_type='social',
                    content="Bitcoin showing strong momentum today! 🚀",
                    author="crypto_trader",
                    timestamp=datetime.now(),
                    metadata={'platform': 'twitter', 'followers': 1000}
                ))
            
            # Reddit 수집 (예시)
            elif 'reddit' in source.url:
                # 실제 구현에서는 Reddit API 사용
                # subreddit = self.reddit_client.subreddit("cryptocurrency")
                # posts = subreddit.hot(limit=10)
                
                # 예시 데이터
                collected.append(CollectedData(
                    source_id=source.source_id,
                    source_type='social',
                    content="What do you think about the current market trend?",
                    author="reddit_user",
                    timestamp=datetime.now(),
                    metadata={'platform': 'reddit', 'subreddit': 'cryptocurrency'}
                ))
            
            return collected
            
        except Exception as e:
            logger.error(f"Social media collection failed: {e}")
            return []
    
    async def _collect_blogs(self, source: DataSource) -> List[CollectedData]:
        """블로그 데이터 수집"""
        try:
            collected = []
            
            async with self.session.get(source.url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # 블로그 포스트 추출
                    posts = soup.find_all('article') or soup.find_all('div', class_='post')
                    
                    for post in posts[:5]:
                        title_elem = post.find('h1') or post.find('h2')
                        content_elem = post.find('div', class_='content') or post.find('p')
                        
                        if title_elem and content_elem:
                            collected.append(CollectedData(
                                source_id=source.source_id,
                                source_type='blog',
                                content=content_elem.get_text().strip(),
                                title=title_elem.get_text().strip(),
                                timestamp=datetime.now(),
                                url=source.url
                            ))
            
            return collected
            
        except Exception as e:
            logger.error(f"Blog collection failed: {e}")
            return []
    
    async def _collect_forums(self, source: DataSource) -> List[CollectedData]:
        """포럼 데이터 수집"""
        try:
            collected = []
            
            async with self.session.get(source.url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # 포럼 포스트 추출
                    posts = soup.find_all('div', class_='post') or soup.find_all('tr', class_='thread')
                    
                    for post in posts[:10]:
                        content_elem = post.find('div', class_='message') or post.find('td', class_='content')
                        author_elem = post.find('span', class_='author') or post.find('td', class_='author')
                        
                        if content_elem:
                            collected.append(CollectedData(
                                source_id=source.source_id,
                                source_type='forum',
                                content=content_elem.get_text().strip(),
                                author=author_elem.get_text().strip() if author_elem else None,
                                timestamp=datetime.now(),
                                url=source.url
                            ))
            
            return collected
            
        except Exception as e:
            logger.error(f"Forum collection failed: {e}")
            return []
    
    async def _process_collected_data(self):
        """수집된 데이터 처리"""
        try:
            # 언어 감지
            for data in self.collected_data:
                if not data.language:
                    data.language = self._detect_language(data.content)
            
            # 중복 제거
            self.collected_data = self._remove_duplicates(self.collected_data)
            
            # 데이터베이스 저장 또는 큐에 전송
            await self._store_data(self.collected_data)
            
            # 수집된 데이터 클리어
            self.collected_data = []
            
            logger.info("Collected data processed successfully")
            
        except Exception as e:
            logger.error(f"Data processing failed: {e}")
    
    def _detect_language(self, text: str) -> str:
        """언어 감지"""
        try:
            # 간단한 언어 감지 (실제로는 langdetect 라이브러리 사용)
            korean_chars = len(re.findall(r'[가-힣]', text))
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text))
            
            if korean_chars > 0:
                return 'ko'
            elif chinese_chars > 0:
                return 'zh'
            elif japanese_chars > 0:
                return 'ja'
            else:
                return 'en'
                
        except Exception as e:
            logger.error(f"Language detection failed: {e}")
            return 'en'
    
    def _remove_duplicates(self, data_list: List[CollectedData]) -> List[CollectedData]:
        """중복 제거"""
        try:
            seen = set()
            unique_data = []
            
            for data in data_list:
                # 제목과 내용의 해시로 중복 확인
                content_hash = hash(f"{data.title}:{data.content[:100]}")
                
                if content_hash not in seen:
                    seen.add(content_hash)
                    unique_data.append(data)
            
            return unique_data
            
        except Exception as e:
            logger.error(f"Duplicate removal failed: {e}")
            return data_list
    
    async def _store_data(self, data_list: List[CollectedData]):
        """데이터 저장"""
        try:
            # 실제 구현에서는 데이터베이스에 저장
            # 또는 메시지 큐에 전송
            
            for data in data_list:
                # 데이터베이스 저장 예시
                # await self.db.insert_sentiment_data(data)
                
                # 메시지 큐 전송 예시
                # await self.message_queue.send(data)
                
                pass
            
            logger.info(f"Stored {len(data_list)} data items")
            
        except Exception as e:
            logger.error(f"Data storage failed: {e}")
```

## 🔧 **감정 분석 모델**

### 📦 **Transformer 기반 감정 분석**

```python
# sentiment-analysis/models/transformer_sentiment_analyzer.py
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, pipeline
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
import pandas as pd
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class SentimentResult:
    """감정 분석 결과"""
    text: str
    sentiment_score: float  # -1.0 (매우 부정적) ~ 1.0 (매우 긍정적)
    sentiment_label: str    # 'positive', 'negative', 'neutral'
    confidence: float       # 0.0 ~ 1.0
    topics: List[str]       # 감지된 주제들
    entities: List[str]     # 감지된 개체들
    language: str           # 언어 코드

class TransformerSentimentAnalyzer:
    """Transformer 기반 감정 분석기"""
    
    def __init__(self, model_name: str = "cardiffnlp/twitter-roberta-base-sentiment-latest",
                 device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        self.model_name = model_name
        self.device = device
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        
        # 언어별 모델
        self.language_models = {
            'en': "cardiffnlp/twitter-roberta-base-sentiment-latest",
            'ko': "klue/roberta-base",  # 한국어 모델
            'zh': "hfl/chinese-roberta-wwm-ext",  # 중국어 모델
            'ja': "cl-tohoku/bert-base-japanese-v3"  # 일본어 모델
        }
        
        self._load_models()
        
        logger.info(f"Transformer sentiment analyzer initialized with {model_name}")
    
    def _load_models(self):
        """모델 로드"""
        try:
            # 기본 영어 모델
            self.pipeline = pipeline(
                "sentiment-analysis",
                model=self.language_models['en'],
                device=0 if self.device == "cuda" else -1
            )
            
            # 다국어 모델들 로드
            self.multilingual_models = {}
            for lang, model_name in self.language_models.items():
                if lang != 'en':
                    try:
                        self.multilingual_models[lang] = pipeline(
                            "sentiment-analysis",
                            model=model_name,
                            device=0 if self.device == "cuda" else -1
                        )
                        logger.info(f"Loaded {lang} model: {model_name}")
                    except Exception as e:
                        logger.warning(f"Failed to load {lang} model: {e}")
            
        except Exception as e:
            logger.error(f"Model loading failed: {e}")
            raise
    
    def analyze_sentiment(self, text: str, language: str = 'en') -> SentimentResult:
        """감정 분석 수행"""
        try:
            # 텍스트 전처리
            processed_text = self._preprocess_text(text)
            
            # 언어별 모델 선택
            if language in self.multilingual_models:
                model = self.multilingual_models[language]
            else:
                model = self.pipeline
            
            # 감정 분석
            result = model(processed_text)
            
            # 결과 파싱
            sentiment_score = self._parse_sentiment_result(result)
            sentiment_label = self._get_sentiment_label(sentiment_score)
            confidence = result[0].get('score', 0.5)
            
            # 주제 및 개체 추출
            topics = self._extract_topics(processed_text)
            entities = self._extract_entities(processed_text)
            
            return SentimentResult(
                text=text,
                sentiment_score=sentiment_score,
                sentiment_label=sentiment_label,
                confidence=confidence,
                topics=topics,
                entities=entities,
                language=language
            )
            
        except Exception as e:
            logger.error(f"Sentiment analysis failed: {e}")
            return SentimentResult(
                text=text,
                sentiment_score=0.0,
                sentiment_label='neutral',
                confidence=0.0,
                topics=[],
                entities=[],
                language=language
            )
    
    def analyze_batch(self, texts: List[str], languages: List[str] = None) -> List[SentimentResult]:
        """배치 감정 분석"""
        try:
            if languages is None:
                languages = ['en'] * len(texts)
            
            results = []
            
            # 언어별로 그룹화
            language_groups = {}
            for i, (text, lang) in enumerate(zip(texts, languages)):
                if lang not in language_groups:
                    language_groups[lang] = []
                language_groups[lang].append((i, text))
            
            # 각 언어별로 배치 처리
            for lang, group in language_groups.items():
                indices, group_texts = zip(*group)
                
                if lang in self.multilingual_models:
                    model = self.multilingual_models[lang]
                else:
                    model = self.pipeline
                
                # 배치 처리
                batch_results = model(group_texts)
                
                # 결과 변환
                for idx, (text, result) in enumerate(zip(group_texts, batch_results)):
                    sentiment_score = self._parse_sentiment_result([result])
                    sentiment_label = self._get_sentiment_label(sentiment_score)
                    
                    results.append(SentimentResult(
                        text=text,
                        sentiment_score=sentiment_score,
                        sentiment_label=sentiment_label,
                        confidence=result.get('score', 0.5),
                        topics=self._extract_topics(text),
                        entities=self._extract_entities(text),
                        language=lang
                    ))
            
            # 원래 순서로 정렬
            results = [results[i] for i in range(len(results))]
            
            return results
            
        except Exception as e:
            logger.error(f"Batch sentiment analysis failed: {e}")
            return []
    
    def _preprocess_text(self, text: str) -> str:
        """텍스트 전처리"""
        try:
            # URL 제거
            text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
            
            # 특수문자 정리
            text = re.sub(r'[^\w\s가-힣\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]', ' ', text)
            
            # 공백 정리
            text = re.sub(r'\s+', ' ', text).strip()
            
            # 길이 제한
            if len(text) > 512:
                text = text[:512]
            
            return text
            
        except Exception as e:
            logger.error(f"Text preprocessing failed: {e}")
            return text
    
    def _parse_sentiment_result(self, result: List[Dict]) -> float:
        """감정 분석 결과 파싱"""
        try:
            if not result:
                return 0.0
            
            label = result[0].get('label', 'NEU')
            score = result[0].get('score', 0.5)
            
            # 라벨을 점수로 변환
            if label in ['POS', 'positive', 'LABEL_2']:
                return score
            elif label in ['NEG', 'negative', 'LABEL_0']:
                return -score
            else:  # NEU, neutral, LABEL_1
                return 0.0
                
        except Exception as e:
            logger.error(f"Sentiment result parsing failed: {e}")
            return 0.0
    
    def _get_sentiment_label(self, score: float) -> str:
        """감정 점수를 라벨로 변환"""
        if score > 0.1:
            return 'positive'
        elif score < -0.1:
            return 'negative'
        else:
            return 'neutral'
    
    def _extract_topics(self, text: str) -> List[str]:
        """주제 추출"""
        try:
            # 간단한 키워드 기반 주제 추출
            topics = []
            
            # 암호화폐 관련 키워드
            crypto_keywords = ['bitcoin', 'btc', 'ethereum', 'eth', 'crypto', 'blockchain']
            for keyword in crypto_keywords:
                if keyword.lower() in text.lower():
                    topics.append(keyword)
            
            # 거래 관련 키워드
            trading_keywords = ['trading', 'market', 'price', 'bull', 'bear', 'pump', 'dump']
            for keyword in trading_keywords:
                if keyword.lower() in text.lower():
                    topics.append(keyword)
            
            return list(set(topics))
            
        except Exception as e:
            logger.error(f"Topic extraction failed: {e}")
            return []
    
    def _extract_entities(self, text: str) -> List[str]:
        """개체 추출"""
        try:
            # 간단한 개체 추출 (실제로는 NER 모델 사용)
            entities = []
            
            # 암호화폐 심볼 패턴
            crypto_pattern = r'\b[A-Z]{3,10}\b'
            crypto_matches = re.findall(crypto_pattern, text)
            entities.extend(crypto_matches)
            
            # 회사명 패턴
            company_pattern = r'\b[A-Z][a-z]+ (Inc|Corp|Ltd|LLC)\b'
            company_matches = re.findall(company_pattern, text)
            entities.extend(company_matches)
            
            return list(set(entities))
            
        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            return []
```

## 🔧 **실시간 감정 집계 시스템**

### 📦 **실시간 감정 집계기**

```python
# sentiment-analysis/aggregators/real_time_aggregator.py
import asyncio
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from collections import defaultdict, deque

logger = logging.getLogger(__name__)

@dataclass
class SentimentAggregate:
    """감정 집계 결과"""
    timestamp: datetime
    overall_sentiment: float
    positive_ratio: float
    negative_ratio: float
    neutral_ratio: float
    volume: int
    confidence: float
    topics: Dict[str, float]
    sources: Dict[str, float]
    market_correlation: float

class RealTimeSentimentAggregator:
    """실시간 감정 집계기"""
    
    def __init__(self, window_size: int = 300,  # 5분 윈도우
                 update_interval: int = 30):     # 30초마다 업데이트
        self.window_size = window_size
        self.update_interval = update_interval
        self.sentiment_data = deque(maxlen=10000)  # 최대 10000개 데이터 저장
        self.aggregates = deque(maxlen=1000)       # 최대 1000개 집계 결과 저장
        
        # 실시간 통계
        self.current_stats = {
            'total_sentiments': 0,
            'positive_count': 0,
            'negative_count': 0,
            'neutral_count': 0,
            'total_confidence': 0.0,
            'topic_counts': defaultdict(int),
            'source_counts': defaultdict(int)
        }
        
        logger.info(f"Real-time sentiment aggregator initialized")
    
    async def start_aggregation(self):
        """집계 시작"""
        try:
            while True:
                # 현재 윈도우의 데이터 집계
                current_time = datetime.now()
                window_start = current_time - timedelta(seconds=self.window_size)
                
                # 윈도우 내 데이터 필터링
                window_data = [
                    data for data in self.sentiment_data
                    if data.timestamp >= window_start
                ]
                
                if window_data:
                    # 집계 수행
                    aggregate = self._aggregate_sentiments(window_data)
                    self.aggregates.append(aggregate)
                    
                    logger.info(f"Aggregated {len(window_data)} sentiments: {aggregate.overall_sentiment:.3f}")
                
                # 대기
                await asyncio.sleep(self.update_interval)
                
        except Exception as e:
            logger.error(f"Sentiment aggregation failed: {e}")
    
    def add_sentiment_data(self, sentiment_result: 'SentimentResult'):
        """감정 데이터 추가"""
        try:
            # 데이터 추가
            self.sentiment_data.append(sentiment_result)
            
            # 실시간 통계 업데이트
            self._update_current_stats(sentiment_result)
            
        except Exception as e:
            logger.error(f"Sentiment data addition failed: {e}")
    
    def _update_current_stats(self, sentiment_result: 'SentimentResult'):
        """실시간 통계 업데이트"""
        try:
            self.current_stats['total_sentiments'] += 1
            
            # 감정 라벨 카운트
            if sentiment_result.sentiment_label == 'positive':
                self.current_stats['positive_count'] += 1
            elif sentiment_result.sentiment_label == 'negative':
                self.current_stats['negative_count'] += 1
            else:
                self.current_stats['neutral_count'] += 1
            
            # 신뢰도 누적
            self.current_stats['total_confidence'] += sentiment_result.confidence
            
            # 주제 카운트
            for topic in sentiment_result.topics:
                self.current_stats['topic_counts'][topic] += 1
            
            # 소스 카운트
            source = sentiment_result.language  # 또는 실제 소스 정보
            self.current_stats['source_counts'][source] += 1
            
        except Exception as e:
            logger.error(f"Stats update failed: {e}")
    
    def _aggregate_sentiments(self, sentiment_data: List['SentimentResult']) -> SentimentAggregate:
        """감정 데이터 집계"""
        try:
            if not sentiment_data:
                return SentimentAggregate(
                    timestamp=datetime.now(),
                    overall_sentiment=0.0,
                    positive_ratio=0.0,
                    negative_ratio=0.0,
                    neutral_ratio=0.0,
                    volume=0,
                    confidence=0.0,
                    topics={},
                    sources={},
                    market_correlation=0.0
                )
            
            # 기본 통계
            total_count = len(sentiment_data)
            sentiment_scores = [data.sentiment_score for data in sentiment_data]
            confidences = [data.confidence for data in sentiment_data]
            
            # 감정 비율
            positive_count = sum(1 for data in sentiment_data if data.sentiment_label == 'positive')
            negative_count = sum(1 for data in sentiment_data if data.sentiment_label == 'negative')
            neutral_count = sum(1 for data in sentiment_data if data.sentiment_label == 'neutral')
            
            positive_ratio = positive_count / total_count
            negative_ratio = negative_count / total_count
            neutral_ratio = neutral_count / total_count
            
            # 가중 평균 감정 점수
            weighted_sentiment = np.average(sentiment_scores, weights=confidences)
            
            # 평균 신뢰도
            avg_confidence = np.mean(confidences)
            
            # 주제별 감정 점수
            topic_sentiments = defaultdict(list)
            for data in sentiment_data:
                for topic in data.topics:
                    topic_sentiments[topic].append(data.sentiment_score)
            
            topic_aggregates = {}
            for topic, scores in topic_sentiments.items():
                topic_aggregates[topic] = np.mean(scores)
            
            # 소스별 감정 점수
            source_sentiments = defaultdict(list)
            for data in sentiment_data:
                source = data.language  # 또는 실제 소스 정보
                source_sentiments[source].append(data.sentiment_score)
            
            source_aggregates = {}
            for source, scores in source_sentiments.items():
                source_aggregates[source] = np.mean(scores)
            
            # 시장 상관관계 (실제 구현에서는 시장 데이터와 비교)
            market_correlation = self._calculate_market_correlation(sentiment_scores)
            
            return SentimentAggregate(
                timestamp=datetime.now(),
                overall_sentiment=weighted_sentiment,
                positive_ratio=positive_ratio,
                negative_ratio=negative_ratio,
                neutral_ratio=neutral_ratio,
                volume=total_count,
                confidence=avg_confidence,
                topics=topic_aggregates,
                sources=source_aggregates,
                market_correlation=market_correlation
            )
            
        except Exception as e:
            logger.error(f"Sentiment aggregation failed: {e}")
            return SentimentAggregate(
                timestamp=datetime.now(),
                overall_sentiment=0.0,
                positive_ratio=0.0,
                negative_ratio=0.0,
                neutral_ratio=0.0,
                volume=0,
                confidence=0.0,
                topics={},
                sources={},
                market_correlation=0.0
            )
    
    def _calculate_market_correlation(self, sentiment_scores: List[float]) -> float:
        """시장 상관관계 계산"""
        try:
            # 실제 구현에서는 시장 가격 데이터와 상관관계 계산
            # 여기서는 예시로 랜덤 값 반환
            return np.random.uniform(-0.5, 0.5)
            
        except Exception as e:
            logger.error(f"Market correlation calculation failed: {e}")
            return 0.0
    
    def get_current_sentiment(self) -> Optional[SentimentAggregate]:
        """현재 감정 상태 반환"""
        try:
            if self.aggregates:
                return self.aggregates[-1]
            return None
            
        except Exception as e:
            logger.error(f"Current sentiment retrieval failed: {e}")
            return None
    
    def get_sentiment_trend(self, hours: int = 24) -> List[SentimentAggregate]:
        """감정 트렌드 반환"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            trend_data = [
                agg for agg in self.aggregates
                if agg.timestamp >= cutoff_time
            ]
            
            return trend_data
            
        except Exception as e:
            logger.error(f"Sentiment trend retrieval failed: {e}")
            return []
    
    def get_topic_sentiment(self, topic: str, hours: int = 24) -> float:
        """주제별 감정 점수"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            topic_sentiments = []
            for agg in self.aggregates:
                if agg.timestamp >= cutoff_time and topic in agg.topics:
                    topic_sentiments.append(agg.topics[topic])
            
            if topic_sentiments:
                return np.mean(topic_sentiments)
            return 0.0
            
        except Exception as e:
            logger.error(f"Topic sentiment retrieval failed: {e}")
            return 0.0
```

## 🎯 **다음 단계**

### 📋 **완료된 작업**
- ✅ 다중 소스 데이터 수집기
- ✅ Transformer 기반 감정 분석
- ✅ 실시간 감정 집계 시스템
- ✅ 다국어 감정 분석
- ✅ 주제 및 개체 추출

### 🔄 **진행 중인 작업**
- 🔄 감정 기반 거래 신호 생성
- 🔄 시장 상관관계 분석
- 🔄 감정 알림 시스템

### ⏳ **다음 단계**
1. **Phase 3.4 포트폴리오 최적화** 문서 생성
2. **Phase 3.5 리스크 관리 AI** 문서 생성
3. **Phase 3.6 Web3 지갑 통합** 문서 생성

---

**마지막 업데이트**: 2024-01-31
**다음 업데이트**: 2024-02-01 (Phase 3.4 포트폴리오 최적화)
**감정 분석 목표**: > 85% 정확도, < 500ms 처리 속도, > 60% 거래 결정 개선
**감정 분석 성능**: 실시간 처리, 다국어 지원, 컨텍스트 인식, 시장 상관관계 분석 